{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "from fastText import load_model\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext embeddings trained on train and val sets\n",
    "# ./fasttext skipgram -input input_text_file -output output_model -dim 128 (fastText-0.1.0)\n",
    "fasttext_model = load_model('word_vectors/fasttext_model.bin')\n",
    "num_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab contains frequent words apperaing in the text along with their frequencies\n",
    "# minimum frequency = 6\n",
    "vocab_file = open('finished_files/vocab')\n",
    "# Store appearing words\n",
    "vocab_words = {}\n",
    "for line in vocab_file:\n",
    "    li = line.split()\n",
    "    if len(li) == 2:\n",
    "        word, freq = li\n",
    "        vocab_words[word] = freq\n",
    "# Final word to id dictionary    \n",
    "word2id = {}\n",
    "tokens = ['<pad>', '<unk>', '<eos>']\n",
    "for token in tokens:\n",
    "    word2id[token] = len(word2id)\n",
    "# Retrieve words from fasttext model and keep only those which are also present in 'vocab'\n",
    "fasttext_words = fasttext_model.get_words()\n",
    "for word in fasttext_words:\n",
    "    if word in vocab_words:\n",
    "        word2id[word] = len(word2id)        \n",
    "vocab_size = len(word2id)\n",
    "# Reverse dictionary\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "# Embeddings\n",
    "embeddings = np.zeros((vocab_size, num_dims))\n",
    "# <pad> token vector contains all zeros. Rest sampled from a normal distribution\n",
    "mu, sigma = 0, 0.05\n",
    "for i in range(1, len(tokens)):\n",
    "    embeddings[i] = np.random.normal(mu, sigma, num_dims)\n",
    "# Get word vectors from fasttext model and store in embeddings matrix\n",
    "for i in range(len(tokens), vocab_size):\n",
    "    embeddings[i] = fasttext_model.get_word_vector(id2word[i])\n",
    "    \n",
    "del fasttext_model, vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "for i in range(10000):\n",
    "    temp[i] = id2word[i]\n",
    "id2word = temp\n",
    "embeddings = embeddings[:10000]\n",
    "word2id = dict(zip(id2word.values(), id2word.keys()))\n",
    "\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_points = 500\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "words = [id2word[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_article_size = 50 #400\n",
    "max_abstract_size = 15 #100\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self):\n",
    "        self.abstract = (None, None)\n",
    "        self.article = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator: \n",
    "    \n",
    "    def __init__(self, batch_size, dataframe):\n",
    "        self.batch_size = batch_size\n",
    "        # train, valid, or test dataframe imported from csv\n",
    "        self.df = dataframe\n",
    "        self.generator = self.row_generator()\n",
    "        \n",
    "        \n",
    "    def row_generator(self):\n",
    "        for row in self.df.itertuples(index=False):\n",
    "            yield row\n",
    "            \n",
    "    def build_batch(self, rows):\n",
    "        # If number of rows less than batch size, get extra rows from the beginning of the dataframe\n",
    "        if len(rows) < self.batch_size:\n",
    "            temp_generator = self.row_generator()\n",
    "            for i in range(self.batch_size - len(rows)):\n",
    "                rows.append(self.get_row(temp_generator))\n",
    "                \n",
    "        # Get lengths of all the sequences in the batch upto max number of tokens\n",
    "        # + 1 is for the <eos> token\n",
    "        abstract_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.abstract.split()[:max_abstract_size]) for row in rows]) + 1\n",
    "        article_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.article.split()[:max_article_size]) for row in rows]) + 1 \n",
    "        abs_len = torch.max(abstract_lengths)\n",
    "        art_len = torch.max(article_lengths) \n",
    "        \n",
    "        # Variables containing abstracts and articles of the batch\n",
    "        abstracts = torch.cuda.LongTensor(abs_len, self.batch_size).fill_(0) # zero padding\n",
    "        articles = torch.cuda.LongTensor(art_len, self.batch_size).fill_(0) # zero padding\n",
    "        \n",
    "        # Sort rows in descending order of sequence (article) lengths\n",
    "        article_lengths, indices = torch.sort(article_lengths, descending=True)\n",
    "        rows = [rows[i] for i in indices]\n",
    "        abstract_lengths = torch.cuda.LongTensor([abstract_lengths[i] for i in indices])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Tokenize abstract and take max_abstract_size number of tokens\n",
    "            tokens = rows[i].abstract.split()[:max_abstract_size]\n",
    "            tokens.append('<eos>')\n",
    "            # Convert each token to word index\n",
    "            # <unk> token index for unknown words\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            # Store as column in abstracts variable with zero padding\n",
    "            abstracts[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "            # Same for articles\n",
    "            tokens = rows[i].article.split()[:max_article_size]\n",
    "            tokens.append('<eos>')\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            articles[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "        batch = Batch()\n",
    "        batch.article = (Variable(articles), article_lengths)\n",
    "        batch.abstract = (Variable(abstracts), abstract_lengths)\n",
    "        return batch\n",
    "            \n",
    "    def get_row(self, generator):\n",
    "        row = generator.__next__()\n",
    "        while not isinstance(row.article, str):\n",
    "            row = generator.__next__()\n",
    "        return row\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        rows = []\n",
    "        for b in range(self.batch_size):\n",
    "            try: rows.append(self.get_row(self.generator))\n",
    "            except StopIteration: break\n",
    "        if rows: return self.build_batch(rows)\n",
    "        else: raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embed): Embedding(10000, 128)\n",
       "  (lstm): LSTM(128, 512, bidirectional=True)\n",
       "  (linear_transform): Linear(in_features=1024, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Hidden states of the encoder\n",
    "        self.hidden_states = None\n",
    "        self.sequence_lengths = None\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm module\n",
    "        self.lstm = nn.LSTM(num_dims, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.lstm.cuda()\n",
    "        \n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(2*hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, articles, article_lengths):\n",
    "        # Embedding lookup\n",
    "        input = self.embed(articles) # (T,B,N)\n",
    "        # batch is sorted in descending order of sequence lengths\n",
    "        packed_input = pack_padded_sequence(input, list(article_lengths))\n",
    "        packed_output, last_hidden = self.lstm(packed_input)\n",
    "        self.hidden_states, self.sequence_lengths = pad_packed_sequence(packed_output) # hidden_states (T,B,2H)\n",
    "        \n",
    "        # Sum hidden states for all time steps for bidirectional lstm\n",
    "        #self.hidden_states = unpacked[:,:,:hidden_size] + unpacked[:,:,hidden_size:] for summing hidden states\n",
    "        \n",
    "        # Concatenate hidden and cell states of last time step for bidirectional lstm\n",
    "        h_n = torch.cat((last_hidden[0][0], last_hidden[0][1]), dim=1) #(B,2H)\n",
    "        c_n = torch.cat((last_hidden[1][0], last_hidden[1][1]), dim=1) #(B,2H)\n",
    "        \n",
    "        hiddenT = (h_n, c_n)\n",
    "        output = self.linear_transform(h_n)\n",
    "        \n",
    "        # Final hidden state\n",
    "        return hiddenT, output\n",
    "    \n",
    "encoder = Encoder()\n",
    "encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embed): Embedding(10000, 128)\n",
       "  (lstm_cell): LSTMCell(128, 1024)\n",
       "  (output_lt): Linear(in_features=1024, out_features=10000, bias=True)\n",
       "  (context_lt): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "    \n",
    "        self.hidden = None\n",
    "        self.lstm_cell = nn.LSTMCell(num_dims, 2*hidden_size).cuda()\n",
    "\n",
    "        # Linear transformations \n",
    "        self.output_lt = nn.Linear(2*hidden_size, vocab_size)\n",
    "        self.context_lt = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        \n",
    "        #self.W_att = nn.Parameter()\n",
    "            \n",
    "    \n",
    "    def attention(self, h_t, batch_size):\n",
    "        score = self.score_function(h_t, batch_size)\n",
    "        #for b in range(batch_size):\n",
    "            #score[:,b][encoder.sequence_lengths[b]:] = float('-inf')\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        a_t = softmax(score).unsqueeze(2) # (T,B,1)\n",
    "        c_t = encoder.hidden_states * a_t # c_t (T,B,2H)\n",
    "        context = torch.sum(c_t, dim=0) # (B,2H)\n",
    "        return context\n",
    "        \n",
    "    def score_function(self, h_t, batch_size):\n",
    "        seq_len = encoder.hidden_states.shape[0]\n",
    "        score = Variable(torch.cuda.FloatTensor(seq_len, batch_size))\n",
    "        for t in range(seq_len):\n",
    "            score[t] = torch.diag(torch.mm(encoder.hidden_states[t], torch.transpose(h_t, 1, 0)))\n",
    "        return score #(T,B)\n",
    "    \n",
    "\n",
    "    def forward(self, input, batch_size):\n",
    "        # input is a LongTensor of size B\n",
    "        input = self.embed(input) #(B,N)\n",
    "\n",
    "        # Attention mechanism\n",
    "        context = self.attention(self.hidden[0], batch_size) #(B,2H)\n",
    "        h_0 = self.context_lt(context) + self.hidden[0] #(B,2H)\n",
    "        self.hidden = (h_0, self.hidden[1])\n",
    "        \n",
    "        self.hidden = self.lstm_cell(input, self.hidden)\n",
    "\n",
    "        output = self.output_lt(self.hidden[0]) #(B,V)\n",
    "        return output\n",
    "    \n",
    "decoder = Decoder()\n",
    "decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, param in decoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "   55\n",
      "   72\n",
      " 4210\n",
      "   55\n",
      "  227\n",
      " 1571\n",
      "   73\n",
      "    4\n",
      "  246\n",
      "    1\n",
      "  561\n",
      "   10\n",
      "    4\n",
      "  104\n",
      "  124\n",
      "   15\n",
      " 9749\n",
      "  114\n",
      " 6874\n",
      "    5\n",
      " 1866\n",
      "    5\n",
      "   25\n",
      "    4\n",
      " 1771\n",
      "    9\n",
      "  114\n",
      "   86\n",
      " 5242\n",
      "   25\n",
      "  283\n",
      "    1\n",
      "    3\n",
      "  102\n",
      "  164\n",
      "    1\n",
      "    1\n",
      " 3674\n",
      "    6\n",
      " 8563\n",
      " 1064\n",
      "    1\n",
      "  676\n",
      "   18\n",
      "   17\n",
      " 2874\n",
      " 7401\n",
      "   15\n",
      "   63\n",
      "   17\n",
      "    2\n",
      "[torch.cuda.LongTensor of size 51x1 (GPU 0)]\n",
      " \n",
      " 51\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def beam_search_decoder():\n",
    "    beam_size = 3\n",
    "    \n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "    batch_size = 1\n",
    "    df = pd.read_csv('datasets/val.csv')\n",
    "    sample = np.random.randint(0,10000)\n",
    "    generator = BatchGenerator(batch_size, df[sample:])\n",
    "    batch = generator.get_batch()\n",
    "        \n",
    "    # article, abstract are LongTensor vairables of shape (max_sequence_length, 1)\n",
    "    # containig word indices from the respective vocabs\n",
    "    article, article_length = batch.article\n",
    "    abstract, abstract_length = batch.abstract\n",
    "    hiddenT, output = encoder(article, article_length)\n",
    "    \n",
    "    # Initialize decoder hidden state\n",
    "    \n",
    "    decoder.hidden = hiddenT\n",
    "    \n",
    "    # First input to the decoder is the predicted word from the last state of encoder\n",
    "    input = log_softmax(output, batch_size)\n",
    "    value, input = torch.topk(output, 1, dim=1)\n",
    "    inputs = [input]*beam_size\n",
    "    values = torch.cuda.FloatTensor(beam_size, vocab_size).fill_(value)\n",
    "    \n",
    "    while True:\n",
    "        for b in range(beam_size):\n",
    "            output = decoder(inputs[b], batch_size)\n",
    "            output = log_softmax(output)\n",
    "            values[b] = output.data + values[b]\n",
    "        values = values.view(-1)\n",
    "        topk, indices = torch.topk(values, beam_size)\n",
    "        indices = (indices + 1) % vocab_size\n",
    "        for b in range(beam_size):\n",
    "            if indices[b] == 0: indices[b] = vocab_size\n",
    "            values[b] = topk[b]\n",
    "        indices = indices - 1\n",
    "        \n",
    "        \n",
    "\n",
    "beam_search_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.7\n",
    "\n",
    "# Filter parameters that do not require gradients\n",
    "encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "# Optimizers\n",
    "encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "# Loss function\n",
    "# Way to accumulate loss on sequences with variable lengths in batches :\n",
    "# size_average: By default, the losses are averaged over observations for each minibatch.\n",
    "# However, if the field size_average is set to False, the losses are instead summed for each minibatch. \n",
    "# Ignored if reduce is False.\n",
    "# Set size_average to False and divide the loss by the number of non-padding tokens.\n",
    "# ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient. \n",
    "# When size_average is True, the loss is averaged over non-ignored targets.\n",
    "# Set ignore_index to the padding value\n",
    "loss_function = nn.NLLLoss(size_average=False, ignore_index=0).cuda() # 0 is the index of <pad>\n",
    "\n",
    "def train_model(batch):\n",
    "    loss = 0\n",
    "    # Clear optimizer gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # articles, abstracts are LongTensor vairables of shape (max_sequence_length, B)\n",
    "    # containig word indices from the respective vocabs\n",
    "    # lengths are LongTensors of shape batch_size containing\n",
    "    # lengths of all the sequences in the batch\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    hiddenT, output = encoder(articles, article_lengths)\n",
    "    \n",
    "    # Initialize decoder hidden state\n",
    "    \n",
    "    decoder.hidden = hiddenT\n",
    "    \n",
    "    # First input to the decoder is the predicted word from the last state of encoder\n",
    "    input = most_likely(output, batch_size)\n",
    "    # Looping over all the sequences\n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input, batch_size)\n",
    "        input = most_likely(output, batch_size)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "        \n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    loss.backward()\n",
    "    \n",
    "    #nn.utils.clip_grad_norm(encoder.parameters(), 0.5)\n",
    "    #nn.utils.clip_grad_norm(decoder.parameters(), 0.5)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    # Initialize hidden_list for next batch of inputs\n",
    "    decoder.hidden_list = []\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(df):\n",
    "    batch_size = 1\n",
    "    generator = BatchGenerator(batch_size, df)\n",
    "    loss = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch = generator.get_batch()\n",
    "            step += 1\n",
    "        except StopIteration: break\n",
    "        loss += calc_loss(batch, batch_size)\n",
    "    loss = loss/step\n",
    "    return loss\n",
    "\n",
    "def calc_loss(batch, batch_size):\n",
    "    loss = 0\n",
    "\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    hiddenT, output = encoder(articles, article_lengths)\n",
    "    \n",
    "    # Initialize decoder hidden state\n",
    "    \n",
    "    decoder.hidden = hiddenT\n",
    "    \n",
    "    # First input to the decoder is the predicted word from the last state of encoder\n",
    "    input = most_likely(output, batch_size)\n",
    "    # Looping over all the sequences\n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input, batch_size)\n",
    "        input = most_likely(output, batch_size)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "        \n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely(output, batch_size):\n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "    output = log_softmax(output)\n",
    "    input = torch.topk(output, 1, dim=1)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 0: 9.217\n",
      "Validation loss: 9.197\n",
      "Average minibatch loss at step 30: 6.990\n",
      "Average minibatch loss at step 60: 6.851\n",
      "Average minibatch loss at step 90: 6.838\n",
      "Average minibatch loss at step 120: 6.785\n",
      "Average minibatch loss at step 150: 6.653\n",
      "Average minibatch loss at step 180: 6.423\n",
      "Average minibatch loss at step 210: 6.429\n",
      "Average minibatch loss at step 240: 6.603\n",
      "Average minibatch loss at step 270: 6.450\n",
      "Average minibatch loss at step 300: 6.527\n",
      "Average minibatch loss at step 330: 6.361\n",
      "Average minibatch loss at step 360: 6.308\n",
      "Average minibatch loss at step 390: 6.276\n",
      "Average minibatch loss at step 420: 6.259\n",
      "Average minibatch loss at step 450: 6.158\n",
      "Average minibatch loss at step 480: 6.333\n",
      "Validation loss: 6.359\n",
      "Average minibatch loss at step 510: 6.161\n",
      "Average minibatch loss at step 540: 6.227\n",
      "Average minibatch loss at step 570: 6.399\n",
      "Average minibatch loss at step 600: 6.191\n",
      "Average minibatch loss at step 630: 6.290\n",
      "Average minibatch loss at step 660: 6.105\n",
      "Average minibatch loss at step 690: 6.016\n",
      "Average minibatch loss at step 720: 6.180\n",
      "Average minibatch loss at step 750: 6.084\n",
      "Average minibatch loss at step 780: 6.185\n",
      "Average minibatch loss at step 810: 6.133\n",
      "Average minibatch loss at step 840: 6.130\n",
      "Average minibatch loss at step 870: 6.274\n",
      "Average minibatch loss at step 900: 6.335\n",
      "Average minibatch loss at step 930: 6.132\n",
      "Average minibatch loss at step 960: 6.108\n",
      "Average minibatch loss at step 990: 6.105\n",
      "Validation loss: 6.206\n",
      "Average minibatch loss at step 1020: 6.038\n",
      "Average minibatch loss at step 1050: 6.038\n",
      "Average minibatch loss at step 1080: 6.240\n",
      "Average minibatch loss at step 1110: 6.080\n",
      "Average minibatch loss at step 1140: 6.523\n",
      "Average minibatch loss at step 1170: 6.184\n",
      "Average minibatch loss at step 1200: 6.101\n",
      "Average minibatch loss at step 1230: 5.980\n",
      "Average minibatch loss at step 1260: 6.047\n",
      "Average minibatch loss at step 1290: 6.027\n",
      "Average minibatch loss at step 1320: 6.067\n",
      "Average minibatch loss at step 1350: 6.116\n",
      "Average minibatch loss at step 1380: 6.057\n",
      "Average minibatch loss at step 1410: 6.030\n",
      "Average minibatch loss at step 1440: 5.893\n",
      "Average minibatch loss at step 1470: 5.859\n",
      "Average minibatch loss at step 1500: 6.069\n",
      "Validation loss: 6.198\n",
      "Average minibatch loss at step 1530: 5.982\n",
      "Average minibatch loss at step 1560: 6.077\n",
      "Average minibatch loss at step 1590: 5.956\n",
      "Average minibatch loss at step 1620: 5.974\n",
      "Average minibatch loss at step 1650: 6.146\n",
      "Average minibatch loss at step 1680: 6.183\n",
      "Average minibatch loss at step 1710: 6.002\n",
      "Average minibatch loss at step 1740: 5.966\n",
      "Average minibatch loss at step 1770: 5.960\n",
      "Average minibatch loss at step 1800: 5.928\n",
      "Average minibatch loss at step 1830: 5.990\n",
      "Average minibatch loss at step 1860: 6.149\n",
      "Average minibatch loss at step 1890: 6.008\n",
      "Average minibatch loss at step 1920: 6.030\n",
      "Average minibatch loss at step 1950: 6.000\n",
      "Average minibatch loss at step 1980: 6.028\n",
      "Validation loss: 6.171\n",
      "Average minibatch loss at step 2010: 5.825\n",
      "Average minibatch loss at step 2040: 5.976\n",
      "Average minibatch loss at step 2070: 5.905\n",
      "Average minibatch loss at step 2100: 6.032\n",
      "Average minibatch loss at step 2130: 6.039\n",
      "Average minibatch loss at step 2160: 6.018\n",
      "Average minibatch loss at step 2190: 5.981\n",
      "Average minibatch loss at step 2220: 5.876\n",
      "Average minibatch loss at step 2250: 5.826\n",
      "Average minibatch loss at step 2280: 6.028\n",
      "Average minibatch loss at step 2310: 5.918\n",
      "Average minibatch loss at step 2340: 6.052\n",
      "Average minibatch loss at step 2370: 5.930\n",
      "Average minibatch loss at step 2400: 5.893\n",
      "Average minibatch loss at step 2430: 6.089\n",
      "Average minibatch loss at step 2460: 7.746\n",
      "Average minibatch loss at step 2490: 5.989\n",
      "Validation loss: 6.204\n",
      "Average minibatch loss at step 2520: 5.911\n",
      "Average minibatch loss at step 2550: 5.925\n",
      "Average minibatch loss at step 2580: 5.907\n",
      "Average minibatch loss at step 2610: 5.972\n",
      "Average minibatch loss at step 2640: 6.139\n",
      "Average minibatch loss at step 2670: 5.980\n",
      "Average minibatch loss at step 2700: 5.989\n",
      "Average minibatch loss at step 2730: 5.967\n",
      "Average minibatch loss at step 2760: 6.031\n",
      "Average minibatch loss at step 2790: 5.803\n",
      "Average minibatch loss at step 2820: 5.948\n",
      "Average minibatch loss at step 2850: 5.881\n",
      "Average minibatch loss at step 2880: 5.983\n",
      "Average minibatch loss at step 2910: 6.020\n",
      "Average minibatch loss at step 2940: 5.979\n",
      "Average minibatch loss at step 2970: 5.959\n",
      "Average minibatch loss at step 3000: 5.844\n",
      "Validation loss: 6.221\n",
      "Average minibatch loss at step 3030: 5.792\n",
      "Average minibatch loss at step 3060: 5.986\n",
      "Average minibatch loss at step 3090: 5.892\n",
      "Average minibatch loss at step 3120: 5.998\n",
      "Average minibatch loss at step 3150: 5.896\n",
      "Average minibatch loss at step 3180: 5.900\n",
      "Average minibatch loss at step 3210: 6.059\n",
      "Average minibatch loss at step 3240: 6.086\n",
      "Average minibatch loss at step 3270: 5.960\n",
      "Average minibatch loss at step 3300: 5.887\n",
      "Average minibatch loss at step 3330: 5.896\n",
      "Average minibatch loss at step 3360: 5.881\n",
      "Average minibatch loss at step 3390: 5.954\n",
      "Average minibatch loss at step 3420: 6.089\n",
      "Average minibatch loss at step 3450: 5.941\n",
      "Average minibatch loss at step 3480: 5.969\n",
      "Validation loss: 6.196\n",
      "Average minibatch loss at step 3510: 5.934\n",
      "Average minibatch loss at step 3540: 5.983\n",
      "Average minibatch loss at step 3570: 5.784\n",
      "Average minibatch loss at step 3600: 5.924\n",
      "Average minibatch loss at step 3630: 5.848\n",
      "Average minibatch loss at step 3660: 5.901\n",
      "Average minibatch loss at step 3690: 5.974\n",
      "Average minibatch loss at step 3720: 5.964\n",
      "Average minibatch loss at step 3750: 5.918\n",
      "Average minibatch loss at step 3780: 5.796\n",
      "Average minibatch loss at step 3810: 5.746\n",
      "Average minibatch loss at step 3840: 5.941\n",
      "Average minibatch loss at step 3870: 5.837\n",
      "Average minibatch loss at step 3900: 5.937\n",
      "Average minibatch loss at step 3930: 5.826\n",
      "Average minibatch loss at step 3960: 5.797\n",
      "Average minibatch loss at step 3990: 5.986\n",
      "Validation loss: 6.167\n",
      "Average minibatch loss at step 4020: 5.982\n",
      "Average minibatch loss at step 4050: 5.873\n",
      "Average minibatch loss at step 4080: 5.808\n",
      "Average minibatch loss at step 4110: 5.833\n",
      "Average minibatch loss at step 4140: 5.782\n",
      "Average minibatch loss at step 4170: 5.839\n",
      "Average minibatch loss at step 4200: 6.014\n",
      "Average minibatch loss at step 4230: 5.847\n",
      "Average minibatch loss at step 4260: 5.898\n",
      "Average minibatch loss at step 4290: 5.863\n",
      "Average minibatch loss at step 4320: 5.882\n",
      "Average minibatch loss at step 4350: 5.693\n",
      "Average minibatch loss at step 4380: 5.852\n",
      "Average minibatch loss at step 4410: 5.771\n",
      "Average minibatch loss at step 4440: 5.811\n",
      "Average minibatch loss at step 4470: 5.870\n",
      "Average minibatch loss at step 4500: 5.862\n",
      "Validation loss: 6.156\n",
      "Average minibatch loss at step 4530: 5.854\n",
      "Average minibatch loss at step 4560: 5.715\n",
      "Average minibatch loss at step 4590: 5.676\n",
      "Average minibatch loss at step 4620: 5.847\n",
      "Average minibatch loss at step 4650: 5.728\n",
      "Average minibatch loss at step 4680: 5.861\n",
      "Average minibatch loss at step 4710: 5.765\n",
      "Average minibatch loss at step 4740: 5.739\n",
      "Average minibatch loss at step 4770: 5.918\n",
      "Average minibatch loss at step 4800: 5.943\n",
      "Average minibatch loss at step 4830: 5.837\n",
      "Average minibatch loss at step 4860: 5.743\n",
      "Average minibatch loss at step 4890: 5.778\n",
      "Average minibatch loss at step 4920: 5.715\n",
      "Average minibatch loss at step 4950: 5.785\n",
      "Average minibatch loss at step 4980: 5.971\n",
      "Validation loss: 6.149\n",
      "Average minibatch loss at step 5010: 5.795\n",
      "Average minibatch loss at step 5040: 5.849\n",
      "Average minibatch loss at step 5070: 5.819\n",
      "Average minibatch loss at step 5100: 5.833\n",
      "Average minibatch loss at step 5130: 5.660\n",
      "Average minibatch loss at step 5160: 5.808\n",
      "Average minibatch loss at step 5190: 5.714\n",
      "Average minibatch loss at step 5220: 5.760\n",
      "Average minibatch loss at step 5250: 5.819\n",
      "Average minibatch loss at step 5280: 5.815\n",
      "Average minibatch loss at step 5310: 5.813\n",
      "Average minibatch loss at step 5340: 5.661\n",
      "Average minibatch loss at step 5370: 5.629\n",
      "Average minibatch loss at step 5400: 5.806\n",
      "Average minibatch loss at step 5430: 5.673\n",
      "Average minibatch loss at step 5460: 5.802\n",
      "Average minibatch loss at step 5490: 5.716\n",
      "Validation loss: 6.189\n",
      "Average minibatch loss at step 5520: 5.688\n",
      "Average minibatch loss at step 5550: 5.861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 5580: 5.883\n",
      "Average minibatch loss at step 5610: 5.797\n",
      "Average minibatch loss at step 5640: 5.704\n",
      "Average minibatch loss at step 5670: 5.735\n",
      "Average minibatch loss at step 5700: 5.676\n",
      "Average minibatch loss at step 5730: 5.717\n",
      "Average minibatch loss at step 5760: 5.916\n",
      "Average minibatch loss at step 5790: 5.738\n",
      "Average minibatch loss at step 5820: 5.795\n",
      "Average minibatch loss at step 5850: 5.750\n",
      "Average minibatch loss at step 5880: 5.783\n",
      "Average minibatch loss at step 5910: 5.622\n",
      "Average minibatch loss at step 5940: 5.756\n",
      "Average minibatch loss at step 5970: 5.660\n",
      "Average minibatch loss at step 6000: 5.711\n",
      "Validation loss: 6.165\n",
      "Average minibatch loss at step 6030: 5.762\n",
      "Average minibatch loss at step 6060: 5.770\n",
      "Average minibatch loss at step 6090: 5.753\n",
      "Average minibatch loss at step 6120: 5.618\n",
      "Average minibatch loss at step 6150: 5.580\n",
      "Average minibatch loss at step 6180: 5.767\n",
      "Average minibatch loss at step 6210: 5.640\n",
      "Average minibatch loss at step 6240: 5.713\n",
      "Average minibatch loss at step 6270: 5.640\n",
      "Average minibatch loss at step 6300: 5.618\n",
      "Average minibatch loss at step 6330: 5.807\n",
      "Average minibatch loss at step 6360: 5.835\n",
      "Average minibatch loss at step 6390: 5.736\n",
      "Average minibatch loss at step 6420: 5.631\n",
      "Average minibatch loss at step 6450: 5.662\n",
      "Average minibatch loss at step 6480: 5.611\n",
      "Validation loss: 6.142\n",
      "Average minibatch loss at step 6510: 5.649\n",
      "Average minibatch loss at step 6540: 5.842\n",
      "Average minibatch loss at step 6570: 5.688\n",
      "Average minibatch loss at step 6600: 5.746\n",
      "Average minibatch loss at step 6630: 5.680\n",
      "Average minibatch loss at step 6660: 5.789\n",
      "Average minibatch loss at step 6690: 5.572\n",
      "Average minibatch loss at step 6720: 5.693\n",
      "Average minibatch loss at step 6750: 5.622\n",
      "Average minibatch loss at step 6780: 5.654\n",
      "Average minibatch loss at step 6810: 5.708\n",
      "Average minibatch loss at step 6840: 5.699\n",
      "Average minibatch loss at step 6870: 5.683\n",
      "Average minibatch loss at step 6900: 5.565\n",
      "Average minibatch loss at step 6930: 5.518\n",
      "Average minibatch loss at step 6960: 5.669\n",
      "Average minibatch loss at step 6990: 5.588\n",
      "Validation loss: 6.135\n",
      "Average minibatch loss at step 7020: 5.639\n",
      "Average minibatch loss at step 7050: 5.560\n",
      "Average minibatch loss at step 7080: 5.553\n",
      "Average minibatch loss at step 7110: 5.760\n",
      "Average minibatch loss at step 7140: 5.802\n",
      "Average minibatch loss at step 7170: 5.675\n",
      "Average minibatch loss at step 7200: 5.550\n",
      "Average minibatch loss at step 7230: 5.613\n",
      "Average minibatch loss at step 7260: 5.542\n",
      "Average minibatch loss at step 7290: 5.587\n",
      "Average minibatch loss at step 7320: 5.767\n",
      "Average minibatch loss at step 7350: 5.615\n",
      "Average minibatch loss at step 7380: 5.649\n",
      "Average minibatch loss at step 7410: 5.627\n",
      "Average minibatch loss at step 7440: 5.668\n",
      "Average minibatch loss at step 7470: 5.500\n",
      "Average minibatch loss at step 7500: 5.597\n",
      "Validation loss: 6.152\n",
      "Average minibatch loss at step 7530: 5.549\n",
      "Average minibatch loss at step 7560: 5.583\n",
      "Average minibatch loss at step 7590: 5.649\n",
      "Average minibatch loss at step 7620: 5.619\n",
      "Average minibatch loss at step 7650: 5.607\n",
      "Average minibatch loss at step 7680: 5.488\n",
      "Average minibatch loss at step 7710: 5.450\n",
      "Average minibatch loss at step 7740: 5.622\n",
      "Average minibatch loss at step 7770: 5.554\n",
      "Average minibatch loss at step 7800: 5.897\n",
      "Average minibatch loss at step 7830: 5.753\n",
      "Average minibatch loss at step 7860: 5.625\n",
      "Average minibatch loss at step 7890: 5.758\n",
      "Average minibatch loss at step 7920: 5.702\n",
      "Average minibatch loss at step 7950: 5.634\n",
      "Average minibatch loss at step 7980: 5.544\n",
      "Validation loss: 6.138\n",
      "Average minibatch loss at step 8010: 5.564\n",
      "Average minibatch loss at step 8040: 5.479\n",
      "Average minibatch loss at step 8070: 5.502\n",
      "Average minibatch loss at step 8100: 5.723\n",
      "Average minibatch loss at step 8130: 5.545\n",
      "Average minibatch loss at step 8160: 5.585\n",
      "Average minibatch loss at step 8190: 5.568\n",
      "Average minibatch loss at step 8220: 5.581\n",
      "Average minibatch loss at step 8250: 5.431\n",
      "Average minibatch loss at step 8280: 5.536\n",
      "Average minibatch loss at step 8310: 5.459\n",
      "Average minibatch loss at step 8340: 5.535\n",
      "Average minibatch loss at step 8370: 5.562\n",
      "Average minibatch loss at step 8400: 5.544\n",
      "Average minibatch loss at step 8430: 5.559\n",
      "Average minibatch loss at step 8460: 5.429\n",
      "Average minibatch loss at step 8490: 5.389\n",
      "Validation loss: 6.136\n",
      "Average minibatch loss at step 8520: 5.536\n",
      "Average minibatch loss at step 8550: 5.442\n",
      "Average minibatch loss at step 8580: 5.512\n",
      "Average minibatch loss at step 8610: 5.445\n",
      "Average minibatch loss at step 8640: 5.414\n",
      "Average minibatch loss at step 8670: 5.633\n",
      "Average minibatch loss at step 8700: 5.596\n",
      "Average minibatch loss at step 8730: 5.551\n",
      "Average minibatch loss at step 8760: 5.445\n",
      "Average minibatch loss at step 8790: 5.479\n",
      "Average minibatch loss at step 8820: 5.413\n",
      "Average minibatch loss at step 8850: 5.452\n",
      "Average minibatch loss at step 8880: 5.664\n",
      "Average minibatch loss at step 8910: 5.488\n",
      "Average minibatch loss at step 8940: 5.532\n",
      "Average minibatch loss at step 8970: 5.522\n",
      "Average minibatch loss at step 9000: 5.537\n",
      "Validation loss: 6.132\n",
      "Average minibatch loss at step 9030: 5.390\n",
      "Average minibatch loss at step 9060: 5.474\n",
      "Average minibatch loss at step 9090: 5.406\n",
      "Average minibatch loss at step 9120: 5.481\n",
      "Average minibatch loss at step 9150: 5.523\n",
      "Average minibatch loss at step 9180: 5.500\n",
      "Average minibatch loss at step 9210: 5.517\n",
      "Average minibatch loss at step 9240: 5.350\n",
      "Average minibatch loss at step 9270: 5.350\n",
      "Average minibatch loss at step 9300: 5.508\n",
      "Average minibatch loss at step 9330: 5.417\n",
      "Average minibatch loss at step 9360: 5.462\n",
      "Average minibatch loss at step 9390: 5.403\n",
      "Average minibatch loss at step 9420: 5.371\n",
      "Average minibatch loss at step 9450: 5.579\n",
      "Average minibatch loss at step 9480: 5.555\n",
      "Validation loss: 6.145\n",
      "Average minibatch loss at step 9510: 5.513\n",
      "Average minibatch loss at step 9540: 5.406\n",
      "Average minibatch loss at step 9570: 5.448\n",
      "Average minibatch loss at step 9600: 5.368\n",
      "Average minibatch loss at step 9630: 5.407\n",
      "Average minibatch loss at step 9660: 5.629\n",
      "Average minibatch loss at step 9690: 5.438\n",
      "Average minibatch loss at step 9720: 5.481\n",
      "Average minibatch loss at step 9750: 5.473\n",
      "Average minibatch loss at step 9780: 5.513\n",
      "Average minibatch loss at step 9810: 5.346\n",
      "Average minibatch loss at step 9840: 5.437\n",
      "Average minibatch loss at step 9870: 5.369\n",
      "Average minibatch loss at step 9900: 5.453\n",
      "Average minibatch loss at step 9930: 5.467\n",
      "Average minibatch loss at step 9960: 5.466\n",
      "Average minibatch loss at step 9990: 5.475\n",
      "Validation loss: 6.153\n",
      "Average minibatch loss at step 10020: 5.320\n",
      "Average minibatch loss at step 10050: 5.315\n",
      "Average minibatch loss at step 10080: 5.466\n",
      "Average minibatch loss at step 10110: 5.374\n",
      "Average minibatch loss at step 10140: 5.412\n",
      "Average minibatch loss at step 10170: 5.362\n",
      "Average minibatch loss at step 10200: 5.330\n",
      "Average minibatch loss at step 10230: 5.555\n",
      "Average minibatch loss at step 10260: 5.529\n",
      "Average minibatch loss at step 10290: 5.485\n",
      "Average minibatch loss at step 10320: 5.368\n",
      "Average minibatch loss at step 10350: 5.423\n",
      "Average minibatch loss at step 10380: 5.349\n",
      "Average minibatch loss at step 10410: 5.392\n",
      "Average minibatch loss at step 10440: 5.598\n",
      "Average minibatch loss at step 10470: 5.411\n",
      "Average minibatch loss at step 10500: 5.438\n",
      "Validation loss: 6.153\n",
      "Average minibatch loss at step 10530: 5.432\n",
      "Average minibatch loss at step 10560: 5.463\n",
      "Average minibatch loss at step 10590: 5.329\n",
      "Average minibatch loss at step 10620: 5.394\n",
      "Average minibatch loss at step 10650: 5.321\n",
      "Average minibatch loss at step 10680: 5.414\n",
      "Average minibatch loss at step 10710: 5.444\n",
      "Average minibatch loss at step 10740: 5.427\n",
      "Average minibatch loss at step 10770: 5.437\n",
      "Average minibatch loss at step 10800: 5.285\n",
      "Average minibatch loss at step 10830: 5.280\n",
      "Average minibatch loss at step 10860: 5.412\n",
      "Average minibatch loss at step 10890: 5.330\n",
      "Average minibatch loss at step 10920: 5.376\n",
      "Average minibatch loss at step 10950: 5.318\n",
      "Average minibatch loss at step 10980: 5.286\n",
      "Validation loss: 6.178\n",
      "Average minibatch loss at step 11010: 5.502\n",
      "Average minibatch loss at step 11040: 5.479\n",
      "Average minibatch loss at step 11070: 5.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 11100: 5.333\n",
      "Average minibatch loss at step 11130: 5.382\n",
      "Average minibatch loss at step 11160: 5.306\n",
      "Average minibatch loss at step 11190: 5.333\n",
      "Average minibatch loss at step 11220: 5.557\n",
      "Average minibatch loss at step 11250: 5.361\n",
      "Average minibatch loss at step 11280: 5.394\n",
      "Average minibatch loss at step 11310: 5.419\n",
      "Average minibatch loss at step 11340: 5.445\n",
      "Average minibatch loss at step 11370: 5.275\n",
      "Average minibatch loss at step 11400: 5.360\n",
      "Average minibatch loss at step 11430: 5.285\n",
      "Average minibatch loss at step 11460: 5.375\n",
      "Average minibatch loss at step 11490: 5.395\n",
      "Validation loss: 6.161\n",
      "Average minibatch loss at step 11520: 5.386\n",
      "Average minibatch loss at step 11550: 5.411\n",
      "Average minibatch loss at step 11580: 5.238\n",
      "Average minibatch loss at step 11610: 5.263\n",
      "Average minibatch loss at step 11640: 5.392\n",
      "Average minibatch loss at step 11670: 5.311\n",
      "Average minibatch loss at step 11700: 5.327\n",
      "Average minibatch loss at step 11730: 5.270\n",
      "Average minibatch loss at step 11760: 5.234\n",
      "Average minibatch loss at step 11790: 5.459\n",
      "Average minibatch loss at step 11820: 5.396\n",
      "Average minibatch loss at step 11850: 5.388\n",
      "Average minibatch loss at step 11880: 5.278\n",
      "Average minibatch loss at step 11910: 5.329\n",
      "Average minibatch loss at step 11940: 5.262\n",
      "Average minibatch loss at step 11970: 5.279\n",
      "Average minibatch loss at step 12000: 5.512\n",
      "Validation loss: 6.150\n",
      "Average minibatch loss at step 12030: 5.318\n",
      "Average minibatch loss at step 12060: 5.345\n",
      "Average minibatch loss at step 12090: 5.354\n",
      "Average minibatch loss at step 12120: 5.384\n",
      "Average minibatch loss at step 12150: 5.216\n",
      "Average minibatch loss at step 12180: 5.293\n",
      "Average minibatch loss at step 12210: 5.257\n",
      "Average minibatch loss at step 12240: 5.338\n",
      "Average minibatch loss at step 12270: 5.347\n",
      "Average minibatch loss at step 12300: 5.337\n",
      "Average minibatch loss at step 12330: 5.363\n",
      "Average minibatch loss at step 12360: 5.195\n",
      "Average minibatch loss at step 12390: 5.201\n",
      "Average minibatch loss at step 12420: 5.320\n",
      "Average minibatch loss at step 12450: 5.241\n",
      "Average minibatch loss at step 12480: 5.292\n",
      "Validation loss: 6.161\n",
      "Average minibatch loss at step 12510: 5.239\n",
      "Average minibatch loss at step 12540: 5.205\n",
      "Average minibatch loss at step 12570: 5.430\n",
      "Average minibatch loss at step 12600: 5.379\n",
      "Average minibatch loss at step 12630: 5.371\n",
      "Average minibatch loss at step 12660: 5.250\n",
      "Average minibatch loss at step 12690: 5.305\n",
      "Average minibatch loss at step 12720: 5.243\n",
      "Average minibatch loss at step 12750: 5.256\n",
      "Average minibatch loss at step 12780: 5.492\n",
      "Average minibatch loss at step 12810: 5.291\n",
      "Average minibatch loss at step 12840: 5.318\n",
      "Average minibatch loss at step 12870: 5.328\n",
      "Average minibatch loss at step 12900: 5.364\n",
      "Average minibatch loss at step 12930: 5.193\n",
      "Average minibatch loss at step 12960: 5.264\n",
      "Average minibatch loss at step 12990: 5.231\n",
      "Validation loss: 6.173\n",
      "Average minibatch loss at step 13020: 5.315\n",
      "Average minibatch loss at step 13050: 5.320\n",
      "Average minibatch loss at step 13080: 5.312\n",
      "Average minibatch loss at step 13110: 5.340\n",
      "Average minibatch loss at step 13140: 5.178\n",
      "Average minibatch loss at step 13170: 5.181\n",
      "Average minibatch loss at step 13200: 5.295\n",
      "Average minibatch loss at step 13230: 5.223\n",
      "Average minibatch loss at step 13260: 5.264\n",
      "Average minibatch loss at step 13290: 5.213\n",
      "Average minibatch loss at step 13320: 5.181\n",
      "Average minibatch loss at step 13350: 5.408\n",
      "Average minibatch loss at step 13380: 5.363\n",
      "Average minibatch loss at step 13410: 5.352\n",
      "Average minibatch loss at step 13440: 5.230\n",
      "Average minibatch loss at step 13470: 5.286\n",
      "Average minibatch loss at step 13500: 5.213\n",
      "Validation loss: 6.182\n",
      "Average minibatch loss at step 13530: 5.234\n",
      "Average minibatch loss at step 13560: 5.470\n",
      "Average minibatch loss at step 13590: 5.274\n",
      "Average minibatch loss at step 13620: 5.293\n",
      "Average minibatch loss at step 13650: 5.307\n",
      "Average minibatch loss at step 13680: 5.344\n",
      "Average minibatch loss at step 13710: 5.175\n",
      "Average minibatch loss at step 13740: 5.238\n",
      "Average minibatch loss at step 13770: 5.203\n",
      "Average minibatch loss at step 13800: 5.295\n",
      "Average minibatch loss at step 13830: 5.297\n",
      "Average minibatch loss at step 13860: 5.290\n",
      "Average minibatch loss at step 13890: 5.322\n",
      "Average minibatch loss at step 13920: 5.149\n",
      "Average minibatch loss at step 13950: 5.165\n",
      "Average minibatch loss at step 13980: 5.275\n",
      "Validation loss: 6.165\n",
      "Average minibatch loss at step 14010: 5.203\n",
      "Average minibatch loss at step 14040: 5.245\n",
      "Average minibatch loss at step 14070: 5.192\n",
      "Average minibatch loss at step 14100: 5.163\n",
      "Average minibatch loss at step 14130: 5.387\n",
      "Average minibatch loss at step 14160: 5.331\n",
      "Average minibatch loss at step 14190: 5.337\n",
      "Average minibatch loss at step 14220: 5.205\n",
      "Average minibatch loss at step 14250: 5.266\n",
      "Average minibatch loss at step 14280: 5.200\n",
      "Average minibatch loss at step 14310: 5.210\n",
      "Average minibatch loss at step 14340: 5.453\n",
      "Average minibatch loss at step 14370: 5.244\n",
      "Average minibatch loss at step 14400: 5.267\n",
      "Average minibatch loss at step 14430: 5.282\n",
      "Average minibatch loss at step 14460: 5.326\n",
      "Average minibatch loss at step 14490: 5.157\n",
      "Validation loss: 6.172\n",
      "Average minibatch loss at step 14520: 5.209\n",
      "Average minibatch loss at step 14550: 5.177\n",
      "Average minibatch loss at step 14580: 5.271\n",
      "Average minibatch loss at step 14610: 5.277\n",
      "Average minibatch loss at step 14640: 5.273\n",
      "Average minibatch loss at step 14670: 5.312\n",
      "Average minibatch loss at step 14700: 5.133\n",
      "Average minibatch loss at step 14730: 5.147\n",
      "Average minibatch loss at step 14760: 5.258\n",
      "Average minibatch loss at step 14790: 5.185\n",
      "Average minibatch loss at step 14820: 5.215\n",
      "Average minibatch loss at step 14850: 5.173\n",
      "Average minibatch loss at step 14880: 5.139\n",
      "Average minibatch loss at step 14910: 5.364\n",
      "Average minibatch loss at step 14940: 5.313\n",
      "Average minibatch loss at step 14970: 5.312\n",
      "Average minibatch loss at step 15000: 5.182\n",
      "Validation loss: 6.168\n",
      "Average minibatch loss at step 15030: 5.248\n",
      "Average minibatch loss at step 15060: 5.178\n",
      "Average minibatch loss at step 15090: 5.191\n",
      "Average minibatch loss at step 15120: 5.435\n",
      "Average minibatch loss at step 15150: 5.223\n",
      "Average minibatch loss at step 15180: 5.253\n",
      "Average minibatch loss at step 15210: 5.260\n",
      "Average minibatch loss at step 15240: 5.305\n",
      "Average minibatch loss at step 15270: 5.134\n",
      "Average minibatch loss at step 15300: 5.185\n",
      "Average minibatch loss at step 15330: 5.151\n",
      "Average minibatch loss at step 15360: 5.251\n",
      "Average minibatch loss at step 15390: 5.259\n",
      "Average minibatch loss at step 15420: 5.255\n",
      "Average minibatch loss at step 15450: 5.282\n",
      "Average minibatch loss at step 15480: 5.106\n",
      "Validation loss: 6.189\n",
      "Average minibatch loss at step 15510: 5.120\n",
      "Average minibatch loss at step 15540: 5.236\n",
      "Average minibatch loss at step 15570: 5.167\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "epochs = 100 \n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "val_df = pd.read_csv('datasets/val.csv')\n",
    "iteration = 0\n",
    "\n",
    "en_scheduler = StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n",
    "de_scheduler = StepLR(decoder_optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    en_scheduler.step()\n",
    "    de_scheduler.step()\n",
    "    \n",
    "    generator = BatchGenerator(batch_size, train_df[:10000])\n",
    "    while True:\n",
    "        try: \n",
    "            batch = generator.get_batch()\n",
    "        except StopIteration: break\n",
    "        loss = train_model(batch)\n",
    "        \n",
    "        if iteration % 30 == 0:\n",
    "            print('Average minibatch loss at step %d: %.3f' % (iteration, loss))\n",
    "            writer.add_scalar('train_loss', loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "            \n",
    "        if iteration % 500 == 0:\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            val_loss = validation_loss(val_df[:100]) # truncating validation dataframe\n",
    "            print('Validation loss: %.3f' % val_loss)\n",
    "\n",
    "            writer.add_scalar('valid_loss', val_loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            \n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "epochs = 7000 #7000\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "val_df = pd.read_csv('datasets/val.csv')\n",
    "iteration = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch % 1500 == 0: #500, 275\n",
    "        learning_rate = learning_rate / 2 #2\n",
    "        # Filter parameters that do not require gradients\n",
    "        encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "        # Optimizers\n",
    "        encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "        print('')\n",
    "        print('learning rate: %f' % learning_rate)\n",
    "        print('')\n",
    "        \n",
    "    generator = BatchGenerator(batch_size, train_df[:64]) #64\n",
    "\n",
    "    while True:\n",
    "        try: \n",
    "            batch = generator.get_batch()\n",
    "        except StopIteration: break\n",
    "        loss = train_model(batch)\n",
    "        \n",
    "        if iteration % 2 == 0:\n",
    "            print('Average minibatch loss at step %d: %.3f' % (iteration, loss))\n",
    "            writer.add_scalar('train_loss', loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "        \n",
    "        \"\"\"if iteration % 8 == 0:    \n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            val_loss = validation_loss(val_df[:8]) # truncating validation dataframe\n",
    "            print('Validation loss: %.3f' % val_loss)\n",
    "            \n",
    "            writer.add_scalar('valid_loss', val_loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\"\"\"\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder')\n",
    "torch.save(decoder.state_dict(), 'decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
