{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "from fastText import load_model\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext embeddings trained on train and val sets\n",
    "# ./fasttext skipgram -input input_text_file -output output_model -dim 128 (fastText-0.1.0)\n",
    "fasttext_model = load_model('word_vectors/fasttext_model.bin')\n",
    "num_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab contains frequent words apperaing in the text along with their frequencies\n",
    "# minimum frequency = 6\n",
    "vocab_file = open('finished_files/vocab')\n",
    "# Store appearing words\n",
    "vocab_words = {}\n",
    "for line in vocab_file:\n",
    "    li = line.split()\n",
    "    if len(li) == 2:\n",
    "        word, freq = li\n",
    "        vocab_words[word] = freq\n",
    "# Final word to id dictionary    \n",
    "word2id = {}\n",
    "tokens = ['<pad>', '<unk>', '<eos>']\n",
    "for token in tokens:\n",
    "    word2id[token] = len(word2id)\n",
    "# Retrieve words from fasttext model and keep only those which are also present in 'vocab'\n",
    "fasttext_words = fasttext_model.get_words()\n",
    "for word in fasttext_words:\n",
    "    if word in vocab_words:\n",
    "        word2id[word] = len(word2id)        \n",
    "vocab_size = len(word2id)\n",
    "# Reverse dictionary\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "# Embeddings\n",
    "embeddings = np.zeros((vocab_size, num_dims))\n",
    "# <pad> token vector contains all zeros. Rest sampled from a normal distribution\n",
    "mu, sigma = 0, 0.05\n",
    "for i in range(1, len(tokens)):\n",
    "    embeddings[i] = np.random.normal(mu, sigma, num_dims)\n",
    "# Get word vectors from fasttext model and store in embeddings matrix\n",
    "for i in range(len(tokens), vocab_size):\n",
    "    embeddings[i] = fasttext_model.get_word_vector(id2word[i])\n",
    "    \n",
    "del fasttext_model, vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "for i in range(10000):\n",
    "    temp[i] = id2word[i]\n",
    "id2word = temp\n",
    "embeddings = embeddings[:10000]\n",
    "word2id = dict(zip(id2word.values(), id2word.keys()))\n",
    "\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_points = 500\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "words = [id2word[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_article_size = 50 #400\n",
    "max_abstract_size = 15 #100\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self):\n",
    "        self.abstract = (None, None)\n",
    "        self.article = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator: \n",
    "    \n",
    "    def __init__(self, batch_size, dataframe):\n",
    "        self.batch_size = batch_size\n",
    "        # train, valid, or test dataframe imported from csv\n",
    "        self.df = dataframe\n",
    "        self.generator = self.row_generator()\n",
    "        \n",
    "        \n",
    "    def row_generator(self):\n",
    "        for row in self.df.itertuples(index=False):\n",
    "            yield row\n",
    "            \n",
    "    def build_batch(self, rows):\n",
    "        # If number of rows less than batch size, get extra rows from the beginning of the dataframe\n",
    "        if len(rows) < self.batch_size:\n",
    "            temp_generator = self.row_generator()\n",
    "            for i in range(self.batch_size - len(rows)):\n",
    "                rows.append(self.get_row(temp_generator))\n",
    "                \n",
    "        # Get lengths of all the sequences in the batch upto max number of tokens\n",
    "        # + 1 is for the <eos> token\n",
    "        abstract_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.abstract.split()[:max_abstract_size]) for row in rows]) + 1\n",
    "        article_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.article.split()[:max_article_size]) for row in rows]) + 1 \n",
    "        abs_len = torch.max(abstract_lengths)\n",
    "        art_len = torch.max(article_lengths) \n",
    "        \n",
    "        # Variables containing abstracts and articles of the batch\n",
    "        abstracts = torch.cuda.LongTensor(abs_len, self.batch_size).fill_(0) # zero padding\n",
    "        articles = torch.cuda.LongTensor(art_len, self.batch_size).fill_(0) # zero padding\n",
    "        \n",
    "        # Sort rows in descending order of sequence (article) lengths\n",
    "        article_lengths, indices = torch.sort(article_lengths, descending=True)\n",
    "        rows = [rows[i] for i in indices]\n",
    "        abstract_lengths = torch.cuda.LongTensor([abstract_lengths[i] for i in indices])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Tokenize abstract and take max_abstract_size number of tokens\n",
    "            tokens = rows[i].abstract.split()[:max_abstract_size]\n",
    "            tokens.append('<eos>')\n",
    "            # Convert each token to word index\n",
    "            # <unk> token index for unknown words\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            # Store as column in abstracts variable with zero padding\n",
    "            abstracts[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "            # Same for articles\n",
    "            tokens = rows[i].article.split()[:max_article_size]\n",
    "            tokens.append('<eos>')\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            articles[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "        batch = Batch()\n",
    "        batch.article = (Variable(articles), article_lengths)\n",
    "        batch.abstract = (Variable(abstracts), abstract_lengths)\n",
    "        return batch\n",
    "            \n",
    "    def get_row(self, generator):\n",
    "        row = generator.__next__()\n",
    "        while not isinstance(row.article, str):\n",
    "            row = generator.__next__()\n",
    "        return row\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        rows = []\n",
    "        for b in range(self.batch_size):\n",
    "            try: rows.append(self.get_row(self.generator))\n",
    "            except StopIteration: break\n",
    "        if rows: return self.build_batch(rows)\n",
    "        else: raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embed): Embedding(10000, 128)\n",
       "  (lstm): LSTM(128, 512, bidirectional=True)\n",
       "  (linear_transform): Linear(in_features=1024, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Hidden states of the encoder\n",
    "        self.hidden_states = None\n",
    "        self.sequence_lengths = None\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm module\n",
    "        self.lstm = nn.LSTM(num_dims, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.lstm.cuda()\n",
    "        \n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(2*hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, articles, article_lengths):\n",
    "        # Embedding lookup\n",
    "        input = self.embed(articles) # (T,B,N)\n",
    "        # batch is sorted in descending order of sequence lengths\n",
    "        packed_input = pack_padded_sequence(input, list(article_lengths))\n",
    "        packed_output, last_hidden = self.lstm(packed_input)\n",
    "        self.hidden_states, self.sequence_lengths = pad_packed_sequence(packed_output) # hidden_states (T,B,2H)\n",
    "        \n",
    "        # Sum hidden states for all time steps for bidirectional lstm\n",
    "        #self.hidden_states = unpacked[:,:,:hidden_size] + unpacked[:,:,hidden_size:] for summing hidden states\n",
    "        \n",
    "        # Concatenate hidden and cell states of last time step for bidirectional lstm\n",
    "        h_n = torch.cat((last_hidden[0][0], last_hidden[0][1]), dim=1) #(B,2H)\n",
    "        c_n = torch.cat((last_hidden[1][0], last_hidden[1][1]), dim=1) #(B,2H)\n",
    "        \n",
    "        hiddenT = (h_n, c_n)\n",
    "        output = self.linear_transform(h_n)\n",
    "        \n",
    "        # Final hidden state\n",
    "        return hiddenT, output\n",
    "    \n",
    "encoder = Encoder(batch_size)\n",
    "encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embed): Embedding(10000, 128)\n",
       "  (lstm_cell): LSTMCell(128, 1024)\n",
       "  (output_lt): Linear(in_features=1024, out_features=10000, bias=True)\n",
       "  (context_lt): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "    \n",
    "        self.hidden = None\n",
    "        self.lstm_cell = nn.LSTMCell(num_dims, 2*hidden_size).cuda()\n",
    "\n",
    "        # Linear transformations \n",
    "        self.output_lt = nn.Linear(2*hidden_size, vocab_size)\n",
    "        self.context_lt = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        \n",
    "        #self.W_att = nn.Parameter()\n",
    "            \n",
    "    \n",
    "    def attention(self, h_t, batch_size):\n",
    "        score = self.score_function(h_t, batch_size)\n",
    "        #for b in range(batch_size):\n",
    "            #score[:,b][encoder.sequence_lengths[b]:] = float('-inf')\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        a_t = softmax(score).unsqueeze(2) # (T,B,1)\n",
    "        c_t = encoder.hidden_states * a_t # c_t (T,B,2H)\n",
    "        context = torch.sum(c_t, dim=0) # (B,2H)\n",
    "        return context\n",
    "        \n",
    "    def score_function(self, h_t, batch_size):\n",
    "        seq_len = encoder.hidden_states.shape[0]\n",
    "        score = Variable(torch.cuda.FloatTensor(seq_len, batch_size))\n",
    "        for t in range(seq_len):\n",
    "            score[t] = torch.diag(torch.mm(encoder.hidden_states[t], torch.transpose(h_t, 1, 0)))\n",
    "        return score #(T,B)\n",
    "    \n",
    "\n",
    "    def forward(self, input, batch_size):\n",
    "        # input is a LongTensor of size B\n",
    "        input = self.embed(input) #(B,N)\n",
    "\n",
    "        # Attention mechanism\n",
    "        context = self.attention(self.hidden[0], batch_size) #(B,2H)\n",
    "        h_0 = self.context_lt(context) + self.hidden[0] #(B,2H)\n",
    "        self.hidden = (h_0, self.hidden[1])\n",
    "        \n",
    "        self.hidden = self.lstm_cell(input, self.hidden)\n",
    "\n",
    "        output = self.output_lt(self.hidden[0]) #(B,V)\n",
    "        return output\n",
    "    \n",
    "decoder = Decoder()\n",
    "decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, param in decoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5 #3.0, 3.5\n",
    "\n",
    "# Filter parameters that do not require gradients\n",
    "encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "# Optimizers\n",
    "encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "# Loss function\n",
    "# Way to accumulate loss on sequences with variable lengths in batches :\n",
    "# size_average: By default, the losses are averaged over observations for each minibatch.\n",
    "# However, if the field size_average is set to False, the losses are instead summed for each minibatch. \n",
    "# Ignored if reduce is False.\n",
    "# Set size_average to False and divide the loss by the number of non-padding tokens.\n",
    "# ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient. \n",
    "# When size_average is True, the loss is averaged over non-ignored targets.\n",
    "# Set ignore_index to the padding value\n",
    "loss_function = nn.NLLLoss(size_average=False, ignore_index=0).cuda() # 0 is the index of <pad>\n",
    "\n",
    "def train_model(batch):\n",
    "    loss = 0\n",
    "    # Clear optimizer gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # articles, abstracts are LongTensor vairables of shape (max_sequence_length, B)\n",
    "    # containig word indices from the respective vocabs\n",
    "    # lengths are LongTensor varibles of shape batch_size containing\n",
    "    # lengths of all the sequences in the batch\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    hiddenT, output = encoder(articles, article_lengths)\n",
    "    \n",
    "    # Initialize decoder hidden state\n",
    "    \n",
    "    decoder.hidden = hiddenT\n",
    "    \n",
    "    # First input to the decoder is the predicted word from the last state of encoder\n",
    "    output = softmax(output, batch_size)\n",
    "    _, input = torch.topk(output, 1, dim=1)\n",
    "    # Looping over all the sequences\n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input, batch_size)\n",
    "        output = softmax(output, batch_size)\n",
    "        _, input = torch.topk(output, 1, dim=1)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "        \n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    loss.backward()\n",
    "    \n",
    "    #nn.utils.clip_grad_norm(encoder.parameters(), 0.5)\n",
    "    #nn.utils.clip_grad_norm(decoder.parameters(), 0.5)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    # Initialize hidden_list for next batch of inputs\n",
    "    decoder.hidden_list = []\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(df):\n",
    "    batch_size = 1\n",
    "    generator = BatchGenerator(batch_size, df)\n",
    "    loss = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch = generator.get_batch()\n",
    "            step += 1\n",
    "        except StopIteration: break\n",
    "        loss += calc_loss(batch, batch_size)\n",
    "    loss = loss/step\n",
    "    return loss\n",
    "\n",
    "def calc_loss(batch, batch_size):\n",
    "    loss = 0\n",
    "    encoder.hidden = encoder.init_hidden(batch_size, volatile=True)\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    \n",
    "    articles.volatile = True\n",
    "    abstracts.volatile = True\n",
    "        \n",
    "    hiddenT, output = encoder(articles, article_lengths) ###\n",
    "    for layer in range(hidden_layers):\n",
    "        decoder.hidden_list.append((hiddenT[0][layer], hiddenT[1][layer])) \n",
    "    #input = Variable(torch.cuda.LongTensor(batch_size).fill_(2), volatile=True)\n",
    "    input = most_likely(output, batch_size)\n",
    "    \n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input)\n",
    "        input = most_likely(output, batch_size)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    decoder.hidden_list = []\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(output, batch_size):\n",
    "    if batch_size > 1:\n",
    "        log_softmax = nn.LogSoftmax(dim=1)\n",
    "        output = log_softmax(output)\n",
    "    else: \n",
    "        log_softmax = nn.LogSoftmax(dim=0)\n",
    "        output = log_softmax(output)\n",
    "        _, next_input = torch.topk(output, 1)##\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate: 0.250000\n",
      "\n",
      "Average minibatch loss at step 2: 9.199\n",
      "Average minibatch loss at step 4: 9.188\n",
      "Average minibatch loss at step 6: 9.175\n",
      "Average minibatch loss at step 8: 9.162\n",
      "Average minibatch loss at step 10: 9.149\n",
      "Average minibatch loss at step 12: 9.135\n",
      "Average minibatch loss at step 14: 9.121\n",
      "Average minibatch loss at step 16: 9.106\n",
      "Average minibatch loss at step 18: 9.091\n",
      "Average minibatch loss at step 20: 9.074\n",
      "Average minibatch loss at step 22: 9.056\n",
      "Average minibatch loss at step 24: 9.036\n",
      "Average minibatch loss at step 26: 9.013\n",
      "Average minibatch loss at step 28: 8.987\n",
      "Average minibatch loss at step 30: 8.956\n",
      "Average minibatch loss at step 32: 8.918\n",
      "Average minibatch loss at step 34: 8.868\n",
      "Average minibatch loss at step 36: 8.800\n",
      "Average minibatch loss at step 38: 8.697\n",
      "Average minibatch loss at step 40: 8.524\n",
      "Average minibatch loss at step 42: 8.215\n",
      "Average minibatch loss at step 44: 7.827\n",
      "Average minibatch loss at step 46: 7.676\n",
      "Average minibatch loss at step 48: 7.554\n",
      "Average minibatch loss at step 50: 7.420\n",
      "Average minibatch loss at step 52: 7.288\n",
      "Average minibatch loss at step 54: 7.172\n",
      "Average minibatch loss at step 56: 7.068\n",
      "Average minibatch loss at step 58: 6.974\n",
      "Average minibatch loss at step 60: 6.886\n",
      "Average minibatch loss at step 62: 6.804\n",
      "Average minibatch loss at step 64: 6.726\n",
      "Average minibatch loss at step 66: 6.650\n",
      "Average minibatch loss at step 68: 6.574\n",
      "Average minibatch loss at step 70: 6.501\n",
      "Average minibatch loss at step 72: 6.525\n",
      "Average minibatch loss at step 74: 6.658\n",
      "Average minibatch loss at step 76: 6.404\n",
      "Average minibatch loss at step 78: 6.362\n",
      "Average minibatch loss at step 80: 6.736\n",
      "Average minibatch loss at step 82: 6.189\n",
      "Average minibatch loss at step 84: 6.972\n",
      "Average minibatch loss at step 86: 6.628\n",
      "Average minibatch loss at step 88: 6.092\n",
      "Average minibatch loss at step 90: 6.121\n",
      "Average minibatch loss at step 92: 7.370\n",
      "Average minibatch loss at step 94: 6.557\n",
      "Average minibatch loss at step 96: 5.934\n",
      "Average minibatch loss at step 98: 6.063\n",
      "Average minibatch loss at step 100: 6.761\n",
      "Average minibatch loss at step 102: 6.037\n",
      "Average minibatch loss at step 104: 6.915\n",
      "Average minibatch loss at step 106: 6.068\n",
      "Average minibatch loss at step 108: 5.919\n",
      "Average minibatch loss at step 110: 6.511\n",
      "Average minibatch loss at step 112: 5.945\n",
      "Average minibatch loss at step 114: 6.417\n",
      "Average minibatch loss at step 116: 5.842\n",
      "Average minibatch loss at step 118: 6.128\n",
      "Average minibatch loss at step 120: 5.734\n",
      "Average minibatch loss at step 122: 6.933\n",
      "Average minibatch loss at step 124: 5.588\n",
      "Average minibatch loss at step 126: 6.406\n",
      "Average minibatch loss at step 128: 5.733\n",
      "Average minibatch loss at step 130: 6.255\n",
      "Average minibatch loss at step 132: 5.774\n",
      "Average minibatch loss at step 134: 5.636\n",
      "Average minibatch loss at step 136: 6.923\n",
      "Average minibatch loss at step 138: 6.157\n",
      "Average minibatch loss at step 140: 5.662\n",
      "Average minibatch loss at step 142: 5.708\n",
      "Average minibatch loss at step 144: 6.076\n",
      "Average minibatch loss at step 146: 5.549\n",
      "Average minibatch loss at step 148: 5.618\n",
      "Average minibatch loss at step 150: 5.551\n",
      "Average minibatch loss at step 152: 6.268\n",
      "Average minibatch loss at step 154: 6.291\n",
      "Average minibatch loss at step 156: 5.961\n",
      "Average minibatch loss at step 158: 5.386\n",
      "Average minibatch loss at step 160: 5.651\n",
      "Average minibatch loss at step 162: 7.461\n",
      "Average minibatch loss at step 164: 6.447\n",
      "Average minibatch loss at step 166: 5.530\n",
      "Average minibatch loss at step 168: 5.898\n",
      "Average minibatch loss at step 170: 5.660\n",
      "Average minibatch loss at step 172: 5.893\n",
      "Average minibatch loss at step 174: 5.382\n",
      "Average minibatch loss at step 176: 5.844\n",
      "Average minibatch loss at step 178: 5.350\n",
      "Average minibatch loss at step 180: 5.547\n",
      "Average minibatch loss at step 182: 5.475\n",
      "Average minibatch loss at step 184: 5.770\n",
      "Average minibatch loss at step 186: 5.508\n",
      "Average minibatch loss at step 188: 5.389\n",
      "Average minibatch loss at step 190: 6.054\n",
      "Average minibatch loss at step 192: 5.651\n",
      "Average minibatch loss at step 194: 5.771\n",
      "Average minibatch loss at step 196: 5.438\n",
      "Average minibatch loss at step 198: 6.246\n",
      "Average minibatch loss at step 200: 6.120\n",
      "Average minibatch loss at step 202: 5.676\n",
      "Average minibatch loss at step 204: 5.305\n",
      "Average minibatch loss at step 206: 5.386\n",
      "Average minibatch loss at step 208: 5.482\n",
      "Average minibatch loss at step 210: 6.216\n",
      "Average minibatch loss at step 212: 5.579\n",
      "Average minibatch loss at step 214: 5.560\n",
      "Average minibatch loss at step 216: 5.572\n",
      "Average minibatch loss at step 218: 5.476\n",
      "Average minibatch loss at step 220: 5.449\n",
      "Average minibatch loss at step 222: 5.385\n",
      "Average minibatch loss at step 224: 5.380\n",
      "Average minibatch loss at step 226: 5.487\n",
      "Average minibatch loss at step 228: 5.892\n",
      "Average minibatch loss at step 230: 6.221\n",
      "Average minibatch loss at step 232: 6.011\n",
      "Average minibatch loss at step 234: 5.671\n",
      "Average minibatch loss at step 236: 5.206\n",
      "Average minibatch loss at step 238: 5.118\n",
      "Average minibatch loss at step 240: 5.240\n",
      "Average minibatch loss at step 242: 5.363\n",
      "Average minibatch loss at step 244: 5.099\n",
      "Average minibatch loss at step 246: 5.345\n",
      "Average minibatch loss at step 248: 5.203\n",
      "Average minibatch loss at step 250: 5.048\n",
      "Average minibatch loss at step 252: 5.296\n",
      "Average minibatch loss at step 254: 5.509\n",
      "Average minibatch loss at step 256: 5.169\n",
      "Average minibatch loss at step 258: 5.053\n",
      "Average minibatch loss at step 260: 5.695\n",
      "Average minibatch loss at step 262: 5.286\n",
      "Average minibatch loss at step 264: 5.171\n",
      "Average minibatch loss at step 266: 5.083\n",
      "Average minibatch loss at step 268: 5.345\n",
      "Average minibatch loss at step 270: 5.364\n",
      "Average minibatch loss at step 272: 5.036\n",
      "Average minibatch loss at step 274: 5.032\n",
      "Average minibatch loss at step 276: 5.689\n",
      "Average minibatch loss at step 278: 5.024\n",
      "Average minibatch loss at step 280: 5.175\n",
      "Average minibatch loss at step 282: 5.244\n",
      "Average minibatch loss at step 284: 4.974\n",
      "Average minibatch loss at step 286: 4.930\n",
      "Average minibatch loss at step 288: 5.155\n",
      "Average minibatch loss at step 290: 5.140\n",
      "Average minibatch loss at step 292: 5.075\n",
      "Average minibatch loss at step 294: 4.873\n",
      "Average minibatch loss at step 296: 5.814\n",
      "Average minibatch loss at step 298: 5.091\n",
      "Average minibatch loss at step 300: 5.076\n",
      "Average minibatch loss at step 302: 4.827\n",
      "Average minibatch loss at step 304: 4.951\n",
      "Average minibatch loss at step 306: 5.462\n",
      "Average minibatch loss at step 308: 4.944\n",
      "Average minibatch loss at step 310: 5.209\n",
      "Average minibatch loss at step 312: 4.798\n",
      "Average minibatch loss at step 314: 4.948\n",
      "Average minibatch loss at step 316: 4.766\n",
      "Average minibatch loss at step 318: 4.889\n",
      "Average minibatch loss at step 320: 5.473\n",
      "Average minibatch loss at step 322: 4.812\n",
      "Average minibatch loss at step 324: 4.798\n",
      "Average minibatch loss at step 326: 5.269\n",
      "Average minibatch loss at step 328: 5.026\n",
      "Average minibatch loss at step 330: 4.891\n",
      "Average minibatch loss at step 332: 4.859\n",
      "Average minibatch loss at step 334: 4.786\n",
      "Average minibatch loss at step 336: 5.121\n",
      "Average minibatch loss at step 338: 4.813\n",
      "Average minibatch loss at step 340: 4.889\n",
      "Average minibatch loss at step 342: 4.761\n",
      "Average minibatch loss at step 344: 4.882\n",
      "Average minibatch loss at step 346: 4.720\n",
      "Average minibatch loss at step 348: 5.013\n",
      "Average minibatch loss at step 350: 4.801\n",
      "Average minibatch loss at step 352: 4.662\n",
      "Average minibatch loss at step 354: 4.771\n",
      "Average minibatch loss at step 356: 4.840\n",
      "Average minibatch loss at step 358: 4.661\n",
      "Average minibatch loss at step 360: 5.009\n",
      "Average minibatch loss at step 362: 4.604\n",
      "Average minibatch loss at step 364: 4.844\n",
      "Average minibatch loss at step 366: 5.505\n",
      "Average minibatch loss at step 368: 5.116\n",
      "Average minibatch loss at step 370: 4.892\n",
      "Average minibatch loss at step 372: 4.631\n",
      "Average minibatch loss at step 374: 4.639\n",
      "Average minibatch loss at step 376: 4.771\n",
      "Average minibatch loss at step 378: 4.744\n",
      "Average minibatch loss at step 380: 4.613\n",
      "Average minibatch loss at step 382: 5.252\n",
      "Average minibatch loss at step 384: 4.784\n",
      "Average minibatch loss at step 386: 4.780\n",
      "Average minibatch loss at step 388: 4.611\n",
      "Average minibatch loss at step 390: 4.683\n",
      "Average minibatch loss at step 392: 4.729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 394: 5.160\n",
      "Average minibatch loss at step 396: 4.645\n",
      "Average minibatch loss at step 398: 4.744\n",
      "Average minibatch loss at step 400: 4.859\n",
      "Average minibatch loss at step 402: 4.640\n",
      "Average minibatch loss at step 404: 4.758\n",
      "Average minibatch loss at step 406: 4.665\n",
      "Average minibatch loss at step 408: 4.836\n",
      "Average minibatch loss at step 410: 4.702\n",
      "Average minibatch loss at step 412: 4.802\n",
      "Average minibatch loss at step 414: 4.468\n",
      "Average minibatch loss at step 416: 4.470\n",
      "Average minibatch loss at step 418: 4.954\n",
      "Average minibatch loss at step 420: 4.490\n",
      "Average minibatch loss at step 422: 4.441\n",
      "Average minibatch loss at step 424: 4.749\n",
      "Average minibatch loss at step 426: 4.939\n",
      "Average minibatch loss at step 428: 4.448\n",
      "Average minibatch loss at step 430: 4.611\n",
      "Average minibatch loss at step 432: 4.967\n",
      "Average minibatch loss at step 434: 4.815\n",
      "Average minibatch loss at step 436: 4.550\n",
      "Average minibatch loss at step 438: 4.521\n",
      "Average minibatch loss at step 440: 4.420\n",
      "Average minibatch loss at step 442: 4.542\n",
      "Average minibatch loss at step 444: 5.168\n",
      "Average minibatch loss at step 446: 4.775\n",
      "Average minibatch loss at step 448: 4.837\n",
      "Average minibatch loss at step 450: 4.462\n",
      "Average minibatch loss at step 452: 4.613\n",
      "Average minibatch loss at step 454: 4.463\n",
      "Average minibatch loss at step 456: 4.500\n",
      "Average minibatch loss at step 458: 4.748\n",
      "Average minibatch loss at step 460: 4.454\n",
      "Average minibatch loss at step 462: 4.573\n",
      "Average minibatch loss at step 464: 4.465\n",
      "Average minibatch loss at step 466: 4.483\n",
      "Average minibatch loss at step 468: 4.628\n",
      "Average minibatch loss at step 470: 4.488\n",
      "Average minibatch loss at step 472: 4.569\n",
      "Average minibatch loss at step 474: 4.701\n",
      "Average minibatch loss at step 476: 4.370\n",
      "Average minibatch loss at step 478: 4.333\n",
      "Average minibatch loss at step 480: 4.456\n",
      "Average minibatch loss at step 482: 4.609\n",
      "Average minibatch loss at step 484: 5.754\n",
      "Average minibatch loss at step 486: 5.210\n",
      "Average minibatch loss at step 488: 4.791\n",
      "Average minibatch loss at step 490: 4.678\n",
      "Average minibatch loss at step 492: 4.432\n",
      "Average minibatch loss at step 494: 4.389\n",
      "Average minibatch loss at step 496: 4.526\n",
      "Average minibatch loss at step 498: 4.609\n",
      "Average minibatch loss at step 500: 4.556\n",
      "Average minibatch loss at step 502: 4.371\n",
      "Average minibatch loss at step 504: 4.582\n",
      "Average minibatch loss at step 506: 4.528\n",
      "Average minibatch loss at step 508: 4.404\n",
      "Average minibatch loss at step 510: 4.620\n",
      "Average minibatch loss at step 512: 4.719\n",
      "Average minibatch loss at step 514: 4.579\n",
      "Average minibatch loss at step 516: 4.366\n",
      "Average minibatch loss at step 518: 4.453\n",
      "Average minibatch loss at step 520: 4.399\n",
      "Average minibatch loss at step 522: 4.335\n",
      "Average minibatch loss at step 524: 4.505\n",
      "Average minibatch loss at step 526: 4.417\n",
      "Average minibatch loss at step 528: 4.306\n",
      "Average minibatch loss at step 530: 4.379\n",
      "Average minibatch loss at step 532: 4.643\n",
      "Average minibatch loss at step 534: 4.312\n",
      "Average minibatch loss at step 536: 4.377\n",
      "Average minibatch loss at step 538: 4.425\n",
      "Average minibatch loss at step 540: 4.327\n",
      "Average minibatch loss at step 542: 4.305\n",
      "Average minibatch loss at step 544: 4.568\n",
      "Average minibatch loss at step 546: 4.338\n",
      "Average minibatch loss at step 548: 4.234\n",
      "Average minibatch loss at step 550: 4.268\n",
      "Average minibatch loss at step 552: 4.299\n",
      "Average minibatch loss at step 554: 4.984\n",
      "Average minibatch loss at step 556: 4.252\n",
      "Average minibatch loss at step 558: 4.353\n",
      "Average minibatch loss at step 560: 4.491\n",
      "Average minibatch loss at step 562: 4.447\n",
      "Average minibatch loss at step 564: 4.225\n",
      "Average minibatch loss at step 566: 4.311\n",
      "Average minibatch loss at step 568: 4.675\n",
      "Average minibatch loss at step 570: 4.218\n",
      "Average minibatch loss at step 572: 4.229\n",
      "Average minibatch loss at step 574: 4.530\n",
      "Average minibatch loss at step 576: 4.363\n",
      "Average minibatch loss at step 578: 4.288\n",
      "Average minibatch loss at step 580: 4.425\n",
      "Average minibatch loss at step 582: 4.260\n",
      "Average minibatch loss at step 584: 4.374\n",
      "Average minibatch loss at step 586: 4.300\n",
      "Average minibatch loss at step 588: 4.250\n",
      "Average minibatch loss at step 590: 4.386\n",
      "Average minibatch loss at step 592: 4.278\n",
      "Average minibatch loss at step 594: 4.285\n",
      "Average minibatch loss at step 596: 4.268\n",
      "Average minibatch loss at step 598: 4.272\n",
      "Average minibatch loss at step 600: 4.391\n",
      "Average minibatch loss at step 602: 4.238\n",
      "Average minibatch loss at step 604: 4.294\n",
      "Average minibatch loss at step 606: 4.300\n",
      "Average minibatch loss at step 608: 4.294\n",
      "Average minibatch loss at step 610: 4.240\n",
      "Average minibatch loss at step 612: 4.441\n",
      "Average minibatch loss at step 614: 4.223\n",
      "Average minibatch loss at step 616: 4.212\n",
      "Average minibatch loss at step 618: 4.258\n",
      "Average minibatch loss at step 620: 4.238\n",
      "Average minibatch loss at step 622: 4.326\n",
      "Average minibatch loss at step 624: 4.233\n",
      "Average minibatch loss at step 626: 4.312\n",
      "Average minibatch loss at step 628: 4.231\n",
      "Average minibatch loss at step 630: 4.267\n",
      "Average minibatch loss at step 632: 4.353\n",
      "Average minibatch loss at step 634: 4.165\n",
      "Average minibatch loss at step 636: 4.322\n",
      "Average minibatch loss at step 638: 4.186\n",
      "Average minibatch loss at step 640: 4.353\n",
      "Average minibatch loss at step 642: 4.185\n",
      "Average minibatch loss at step 644: 4.135\n",
      "Average minibatch loss at step 646: 4.170\n",
      "Average minibatch loss at step 648: 4.338\n",
      "Average minibatch loss at step 650: 4.198\n",
      "Average minibatch loss at step 652: 4.119\n",
      "Average minibatch loss at step 654: 4.356\n",
      "Average minibatch loss at step 656: 4.288\n",
      "Average minibatch loss at step 658: 4.137\n",
      "Average minibatch loss at step 660: 4.094\n",
      "Average minibatch loss at step 662: 4.182\n",
      "Average minibatch loss at step 664: 4.345\n",
      "Average minibatch loss at step 666: 4.242\n",
      "Average minibatch loss at step 668: 4.155\n",
      "Average minibatch loss at step 670: 4.348\n",
      "Average minibatch loss at step 672: 4.218\n",
      "Average minibatch loss at step 674: 4.132\n",
      "Average minibatch loss at step 676: 4.104\n",
      "Average minibatch loss at step 678: 4.167\n",
      "Average minibatch loss at step 680: 4.262\n",
      "Average minibatch loss at step 682: 4.145\n",
      "Average minibatch loss at step 684: 4.186\n",
      "Average minibatch loss at step 686: 4.244\n",
      "Average minibatch loss at step 688: 4.304\n",
      "Average minibatch loss at step 690: 4.102\n",
      "Average minibatch loss at step 692: 4.111\n",
      "Average minibatch loss at step 694: 4.198\n",
      "Average minibatch loss at step 696: 4.188\n",
      "Average minibatch loss at step 698: 4.198\n",
      "Average minibatch loss at step 700: 4.163\n",
      "Average minibatch loss at step 702: 4.279\n",
      "Average minibatch loss at step 704: 4.106\n",
      "Average minibatch loss at step 706: 4.105\n",
      "Average minibatch loss at step 708: 4.162\n",
      "Average minibatch loss at step 710: 4.144\n",
      "Average minibatch loss at step 712: 4.132\n",
      "Average minibatch loss at step 714: 4.211\n",
      "Average minibatch loss at step 716: 4.166\n",
      "Average minibatch loss at step 718: 4.221\n",
      "Average minibatch loss at step 720: 4.081\n",
      "Average minibatch loss at step 722: 4.127\n",
      "Average minibatch loss at step 724: 4.119\n",
      "Average minibatch loss at step 726: 4.071\n",
      "Average minibatch loss at step 728: 4.058\n",
      "Average minibatch loss at step 730: 4.183\n",
      "Average minibatch loss at step 732: 4.205\n",
      "Average minibatch loss at step 734: 4.196\n",
      "Average minibatch loss at step 736: 4.046\n",
      "Average minibatch loss at step 738: 4.078\n",
      "Average minibatch loss at step 740: 4.112\n",
      "Average minibatch loss at step 742: 4.104\n",
      "Average minibatch loss at step 744: 4.101\n",
      "Average minibatch loss at step 746: 4.060\n",
      "Average minibatch loss at step 748: 4.091\n",
      "Average minibatch loss at step 750: 4.234\n",
      "Average minibatch loss at step 752: 4.097\n",
      "Average minibatch loss at step 754: 4.037\n",
      "Average minibatch loss at step 756: 4.199\n",
      "Average minibatch loss at step 758: 4.093\n",
      "Average minibatch loss at step 760: 4.073\n",
      "Average minibatch loss at step 762: 4.070\n",
      "Average minibatch loss at step 764: 4.032\n",
      "Average minibatch loss at step 766: 4.104\n",
      "Average minibatch loss at step 768: 4.197\n",
      "Average minibatch loss at step 770: 4.210\n",
      "Average minibatch loss at step 772: 4.013\n",
      "Average minibatch loss at step 774: 4.029\n",
      "Average minibatch loss at step 776: 4.009\n",
      "Average minibatch loss at step 778: 3.998\n",
      "Average minibatch loss at step 780: 4.021\n",
      "Average minibatch loss at step 782: 4.148\n",
      "Average minibatch loss at step 784: 4.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 786: 4.068\n",
      "Average minibatch loss at step 788: 4.011\n",
      "Average minibatch loss at step 790: 4.173\n",
      "Average minibatch loss at step 792: 4.049\n",
      "Average minibatch loss at step 794: 4.028\n",
      "Average minibatch loss at step 796: 4.039\n",
      "Average minibatch loss at step 798: 4.005\n",
      "Average minibatch loss at step 800: 4.068\n",
      "Average minibatch loss at step 802: 4.122\n",
      "Average minibatch loss at step 804: 4.239\n",
      "Average minibatch loss at step 806: 3.976\n",
      "Average minibatch loss at step 808: 3.973\n",
      "Average minibatch loss at step 810: 3.941\n",
      "Average minibatch loss at step 812: 3.929\n",
      "Average minibatch loss at step 814: 3.961\n",
      "Average minibatch loss at step 816: 3.980\n",
      "Average minibatch loss at step 818: 3.989\n",
      "Average minibatch loss at step 820: 4.302\n",
      "Average minibatch loss at step 822: 4.221\n",
      "Average minibatch loss at step 824: 4.048\n",
      "Average minibatch loss at step 826: 3.899\n",
      "Average minibatch loss at step 828: 3.930\n",
      "Average minibatch loss at step 830: 4.062\n",
      "Average minibatch loss at step 832: 4.213\n",
      "Average minibatch loss at step 834: 6.895\n",
      "Average minibatch loss at step 836: 7.150\n",
      "Average minibatch loss at step 838: 7.460\n",
      "Average minibatch loss at step 840: 6.149\n",
      "Average minibatch loss at step 842: 5.885\n",
      "Average minibatch loss at step 844: 5.667\n",
      "Average minibatch loss at step 846: 6.430\n",
      "Average minibatch loss at step 848: 5.547\n",
      "Average minibatch loss at step 850: 5.906\n",
      "Average minibatch loss at step 852: 5.496\n",
      "Average minibatch loss at step 854: 5.153\n",
      "Average minibatch loss at step 856: 5.712\n",
      "Average minibatch loss at step 858: 5.423\n",
      "Average minibatch loss at step 860: 5.382\n",
      "Average minibatch loss at step 862: 4.761\n",
      "Average minibatch loss at step 864: 4.689\n",
      "Average minibatch loss at step 866: 6.115\n",
      "Average minibatch loss at step 868: 5.512\n",
      "Average minibatch loss at step 870: 4.854\n",
      "Average minibatch loss at step 872: 4.605\n",
      "Average minibatch loss at step 874: 4.904\n",
      "Average minibatch loss at step 876: 4.953\n",
      "Average minibatch loss at step 878: 4.570\n",
      "Average minibatch loss at step 880: 4.912\n",
      "Average minibatch loss at step 882: 4.932\n",
      "Average minibatch loss at step 884: 4.552\n",
      "Average minibatch loss at step 886: 4.797\n",
      "Average minibatch loss at step 888: 4.544\n",
      "Average minibatch loss at step 890: 4.805\n",
      "Average minibatch loss at step 892: 4.606\n",
      "Average minibatch loss at step 894: 4.648\n",
      "Average minibatch loss at step 896: 4.394\n",
      "Average minibatch loss at step 898: 4.626\n",
      "Average minibatch loss at step 900: 4.740\n",
      "Average minibatch loss at step 902: 4.485\n",
      "Average minibatch loss at step 904: 4.998\n",
      "Average minibatch loss at step 906: 4.334\n",
      "Average minibatch loss at step 908: 4.337\n",
      "Average minibatch loss at step 910: 4.774\n",
      "Average minibatch loss at step 912: 4.952\n",
      "Average minibatch loss at step 914: 4.424\n",
      "Average minibatch loss at step 916: 4.436\n",
      "Average minibatch loss at step 918: 4.753\n",
      "Average minibatch loss at step 920: 4.284\n",
      "Average minibatch loss at step 922: 4.231\n",
      "Average minibatch loss at step 924: 4.286\n",
      "Average minibatch loss at step 926: 5.082\n",
      "Average minibatch loss at step 928: 4.507\n",
      "Average minibatch loss at step 930: 4.410\n",
      "Average minibatch loss at step 932: 4.455\n",
      "Average minibatch loss at step 934: 4.297\n",
      "Average minibatch loss at step 936: 4.326\n",
      "Average minibatch loss at step 938: 4.594\n",
      "Average minibatch loss at step 940: 4.273\n",
      "Average minibatch loss at step 942: 4.243\n",
      "Average minibatch loss at step 944: 4.331\n",
      "Average minibatch loss at step 946: 4.884\n",
      "Average minibatch loss at step 948: 4.159\n",
      "Average minibatch loss at step 950: 4.251\n",
      "Average minibatch loss at step 952: 4.598\n",
      "Average minibatch loss at step 954: 4.155\n",
      "Average minibatch loss at step 956: 4.267\n",
      "Average minibatch loss at step 958: 4.606\n",
      "Average minibatch loss at step 960: 4.181\n",
      "Average minibatch loss at step 962: 4.261\n",
      "Average minibatch loss at step 964: 4.106\n",
      "Average minibatch loss at step 966: 4.550\n",
      "Average minibatch loss at step 968: 4.413\n",
      "Average minibatch loss at step 970: 4.146\n",
      "Average minibatch loss at step 972: 4.456\n",
      "Average minibatch loss at step 974: 4.351\n",
      "Average minibatch loss at step 976: 4.122\n",
      "Average minibatch loss at step 978: 4.371\n",
      "Average minibatch loss at step 980: 4.469\n",
      "Average minibatch loss at step 982: 4.090\n",
      "Average minibatch loss at step 984: 4.130\n",
      "Average minibatch loss at step 986: 4.336\n",
      "Average minibatch loss at step 988: 4.500\n",
      "Average minibatch loss at step 990: 4.148\n",
      "Average minibatch loss at step 992: 4.375\n",
      "Average minibatch loss at step 994: 4.434\n",
      "Average minibatch loss at step 996: 4.082\n",
      "Average minibatch loss at step 998: 4.239\n",
      "Average minibatch loss at step 1000: 4.417\n",
      "Average minibatch loss at step 1002: 4.191\n",
      "Average minibatch loss at step 1004: 4.213\n",
      "Average minibatch loss at step 1006: 4.501\n",
      "Average minibatch loss at step 1008: 4.086\n",
      "Average minibatch loss at step 1010: 4.220\n",
      "Average minibatch loss at step 1012: 4.321\n",
      "Average minibatch loss at step 1014: 4.122\n",
      "Average minibatch loss at step 1016: 4.175\n",
      "Average minibatch loss at step 1018: 4.276\n",
      "Average minibatch loss at step 1020: 4.164\n",
      "Average minibatch loss at step 1022: 4.122\n",
      "Average minibatch loss at step 1024: 4.215\n",
      "Average minibatch loss at step 1026: 4.194\n",
      "Average minibatch loss at step 1028: 4.123\n",
      "Average minibatch loss at step 1030: 4.121\n",
      "Average minibatch loss at step 1032: 4.177\n",
      "Average minibatch loss at step 1034: 4.155\n",
      "Average minibatch loss at step 1036: 4.132\n",
      "Average minibatch loss at step 1038: 4.146\n",
      "Average minibatch loss at step 1040: 4.136\n",
      "Average minibatch loss at step 1042: 4.094\n",
      "Average minibatch loss at step 1044: 4.087\n",
      "Average minibatch loss at step 1046: 4.070\n",
      "Average minibatch loss at step 1048: 4.108\n",
      "Average minibatch loss at step 1050: 4.171\n",
      "Average minibatch loss at step 1052: 4.116\n",
      "Average minibatch loss at step 1054: 4.070\n",
      "Average minibatch loss at step 1056: 4.035\n",
      "Average minibatch loss at step 1058: 4.051\n",
      "Average minibatch loss at step 1060: 4.047\n",
      "Average minibatch loss at step 1062: 4.102\n",
      "Average minibatch loss at step 1064: 4.181\n",
      "Average minibatch loss at step 1066: 4.125\n",
      "Average minibatch loss at step 1068: 4.031\n",
      "Average minibatch loss at step 1070: 3.970\n",
      "Average minibatch loss at step 1072: 3.964\n",
      "Average minibatch loss at step 1074: 3.922\n",
      "Average minibatch loss at step 1076: 4.113\n",
      "Average minibatch loss at step 1078: 4.599\n",
      "Average minibatch loss at step 1080: 4.024\n",
      "Average minibatch loss at step 1082: 3.954\n",
      "Average minibatch loss at step 1084: 4.035\n",
      "Average minibatch loss at step 1086: 4.189\n",
      "Average minibatch loss at step 1088: 4.415\n",
      "Average minibatch loss at step 1090: 4.015\n",
      "Average minibatch loss at step 1092: 3.980\n",
      "Average minibatch loss at step 1094: 3.959\n",
      "Average minibatch loss at step 1096: 3.932\n",
      "Average minibatch loss at step 1098: 3.914\n",
      "Average minibatch loss at step 1100: 3.907\n",
      "Average minibatch loss at step 1102: 4.168\n",
      "Average minibatch loss at step 1104: 4.312\n",
      "Average minibatch loss at step 1106: 3.982\n",
      "Average minibatch loss at step 1108: 4.096\n",
      "Average minibatch loss at step 1110: 4.046\n",
      "Average minibatch loss at step 1112: 4.027\n",
      "Average minibatch loss at step 1114: 4.048\n",
      "Average minibatch loss at step 1116: 4.018\n",
      "Average minibatch loss at step 1118: 4.027\n",
      "Average minibatch loss at step 1120: 4.011\n",
      "Average minibatch loss at step 1122: 4.017\n",
      "Average minibatch loss at step 1124: 4.003\n",
      "Average minibatch loss at step 1126: 4.018\n",
      "Average minibatch loss at step 1128: 3.985\n",
      "Average minibatch loss at step 1130: 3.953\n",
      "Average minibatch loss at step 1132: 3.991\n",
      "Average minibatch loss at step 1134: 3.988\n",
      "Average minibatch loss at step 1136: 4.002\n",
      "Average minibatch loss at step 1138: 3.978\n",
      "Average minibatch loss at step 1140: 3.986\n",
      "Average minibatch loss at step 1142: 3.954\n",
      "Average minibatch loss at step 1144: 3.938\n",
      "Average minibatch loss at step 1146: 3.966\n",
      "Average minibatch loss at step 1148: 3.946\n",
      "Average minibatch loss at step 1150: 3.958\n",
      "Average minibatch loss at step 1152: 3.945\n",
      "Average minibatch loss at step 1154: 3.964\n",
      "Average minibatch loss at step 1156: 3.939\n",
      "Average minibatch loss at step 1158: 3.938\n",
      "Average minibatch loss at step 1160: 3.949\n",
      "Average minibatch loss at step 1162: 3.920\n",
      "Average minibatch loss at step 1164: 3.925\n",
      "Average minibatch loss at step 1166: 3.915\n",
      "Average minibatch loss at step 1168: 3.910\n",
      "Average minibatch loss at step 1170: 3.918\n",
      "Average minibatch loss at step 1172: 3.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1174: 3.922\n",
      "Average minibatch loss at step 1176: 3.902\n",
      "Average minibatch loss at step 1178: 3.908\n",
      "Average minibatch loss at step 1180: 3.891\n",
      "Average minibatch loss at step 1182: 3.912\n",
      "Average minibatch loss at step 1184: 3.886\n",
      "Average minibatch loss at step 1186: 3.889\n",
      "Average minibatch loss at step 1188: 3.912\n",
      "Average minibatch loss at step 1190: 3.884\n",
      "Average minibatch loss at step 1192: 3.890\n",
      "Average minibatch loss at step 1194: 3.871\n",
      "Average minibatch loss at step 1196: 3.879\n",
      "Average minibatch loss at step 1198: 3.849\n",
      "Average minibatch loss at step 1200: 3.850\n",
      "Average minibatch loss at step 1202: 3.881\n",
      "Average minibatch loss at step 1204: 3.862\n",
      "Average minibatch loss at step 1206: 3.880\n",
      "Average minibatch loss at step 1208: 3.889\n",
      "Average minibatch loss at step 1210: 3.849\n",
      "Average minibatch loss at step 1212: 3.849\n",
      "Average minibatch loss at step 1214: 3.832\n",
      "Average minibatch loss at step 1216: 3.843\n",
      "Average minibatch loss at step 1218: 3.822\n",
      "Average minibatch loss at step 1220: 3.830\n",
      "Average minibatch loss at step 1222: 3.834\n",
      "Average minibatch loss at step 1224: 3.851\n",
      "Average minibatch loss at step 1226: 3.837\n",
      "Average minibatch loss at step 1228: 3.856\n",
      "Average minibatch loss at step 1230: 3.827\n",
      "Average minibatch loss at step 1232: 3.836\n",
      "Average minibatch loss at step 1234: 3.818\n",
      "Average minibatch loss at step 1236: 3.802\n",
      "Average minibatch loss at step 1238: 3.780\n",
      "Average minibatch loss at step 1240: 3.785\n",
      "Average minibatch loss at step 1242: 3.765\n",
      "Average minibatch loss at step 1244: 3.798\n",
      "Average minibatch loss at step 1246: 3.824\n",
      "Average minibatch loss at step 1248: 3.861\n",
      "Average minibatch loss at step 1250: 3.850\n",
      "Average minibatch loss at step 1252: 3.882\n",
      "Average minibatch loss at step 1254: 3.866\n",
      "Average minibatch loss at step 1256: 3.805\n",
      "Average minibatch loss at step 1258: 3.750\n",
      "Average minibatch loss at step 1260: 3.721\n",
      "Average minibatch loss at step 1262: 3.706\n",
      "Average minibatch loss at step 1264: 3.713\n",
      "Average minibatch loss at step 1266: 3.736\n",
      "Average minibatch loss at step 1268: 3.931\n",
      "Average minibatch loss at step 1270: 4.292\n",
      "Average minibatch loss at step 1272: 5.011\n",
      "Average minibatch loss at step 1274: 3.795\n",
      "Average minibatch loss at step 1276: 3.766\n",
      "Average minibatch loss at step 1278: 3.871\n",
      "Average minibatch loss at step 1280: 3.895\n",
      "Average minibatch loss at step 1282: 3.783\n",
      "Average minibatch loss at step 1284: 3.756\n",
      "Average minibatch loss at step 1286: 3.723\n",
      "Average minibatch loss at step 1288: 3.717\n",
      "Average minibatch loss at step 1290: 3.704\n",
      "Average minibatch loss at step 1292: 3.708\n",
      "Average minibatch loss at step 1294: 3.688\n",
      "Average minibatch loss at step 1296: 3.697\n",
      "Average minibatch loss at step 1298: 3.685\n",
      "Average minibatch loss at step 1300: 3.703\n",
      "Average minibatch loss at step 1302: 3.719\n",
      "Average minibatch loss at step 1304: 3.883\n",
      "Average minibatch loss at step 1306: 4.003\n",
      "Average minibatch loss at step 1308: 3.873\n",
      "Average minibatch loss at step 1310: 3.772\n",
      "Average minibatch loss at step 1312: 3.726\n",
      "Average minibatch loss at step 1314: 3.676\n",
      "Average minibatch loss at step 1316: 3.661\n",
      "Average minibatch loss at step 1318: 3.638\n",
      "Average minibatch loss at step 1320: 3.645\n",
      "Average minibatch loss at step 1322: 3.623\n",
      "Average minibatch loss at step 1324: 3.627\n",
      "Average minibatch loss at step 1326: 3.624\n",
      "Average minibatch loss at step 1328: 3.623\n",
      "Average minibatch loss at step 1330: 3.615\n",
      "Average minibatch loss at step 1332: 3.619\n",
      "Average minibatch loss at step 1334: 3.605\n",
      "Average minibatch loss at step 1336: 3.618\n",
      "Average minibatch loss at step 1338: 3.594\n",
      "Average minibatch loss at step 1340: 3.596\n",
      "Average minibatch loss at step 1342: 3.604\n",
      "Average minibatch loss at step 1344: 3.589\n",
      "Average minibatch loss at step 1346: 3.603\n",
      "Average minibatch loss at step 1348: 3.580\n",
      "Average minibatch loss at step 1350: 3.579\n",
      "Average minibatch loss at step 1352: 3.593\n",
      "Average minibatch loss at step 1354: 3.578\n",
      "Average minibatch loss at step 1356: 3.598\n",
      "Average minibatch loss at step 1358: 3.605\n",
      "Average minibatch loss at step 1360: 3.830\n",
      "Average minibatch loss at step 1362: 4.616\n",
      "Average minibatch loss at step 1364: 4.847\n",
      "Average minibatch loss at step 1366: 3.856\n",
      "Average minibatch loss at step 1368: 3.800\n",
      "Average minibatch loss at step 1370: 3.667\n",
      "Average minibatch loss at step 1372: 3.614\n",
      "Average minibatch loss at step 1374: 3.593\n",
      "Average minibatch loss at step 1376: 3.572\n",
      "Average minibatch loss at step 1378: 3.571\n",
      "Average minibatch loss at step 1380: 3.560\n",
      "Average minibatch loss at step 1382: 3.565\n",
      "Average minibatch loss at step 1384: 3.550\n",
      "Average minibatch loss at step 1386: 3.558\n",
      "Average minibatch loss at step 1388: 3.541\n",
      "Average minibatch loss at step 1390: 3.549\n",
      "Average minibatch loss at step 1392: 3.536\n",
      "Average minibatch loss at step 1394: 3.546\n",
      "Average minibatch loss at step 1396: 3.527\n",
      "Average minibatch loss at step 1398: 3.533\n",
      "Average minibatch loss at step 1400: 3.525\n",
      "Average minibatch loss at step 1402: 3.532\n",
      "Average minibatch loss at step 1404: 3.514\n",
      "Average minibatch loss at step 1406: 3.524\n",
      "Average minibatch loss at step 1408: 3.509\n",
      "Average minibatch loss at step 1410: 3.521\n",
      "Average minibatch loss at step 1412: 3.509\n",
      "Average minibatch loss at step 1414: 3.525\n",
      "Average minibatch loss at step 1416: 3.508\n",
      "Average minibatch loss at step 1418: 3.516\n",
      "Average minibatch loss at step 1420: 3.510\n",
      "Average minibatch loss at step 1422: 3.518\n",
      "Average minibatch loss at step 1424: 3.501\n",
      "Average minibatch loss at step 1426: 3.511\n",
      "Average minibatch loss at step 1428: 3.497\n",
      "Average minibatch loss at step 1430: 3.509\n",
      "Average minibatch loss at step 1432: 3.493\n",
      "Average minibatch loss at step 1434: 3.502\n",
      "Average minibatch loss at step 1436: 3.495\n",
      "Average minibatch loss at step 1438: 3.517\n",
      "Average minibatch loss at step 1440: 3.653\n",
      "Average minibatch loss at step 1442: 4.259\n",
      "Average minibatch loss at step 1444: 3.840\n",
      "Average minibatch loss at step 1446: 4.659\n",
      "Average minibatch loss at step 1448: 3.723\n",
      "Average minibatch loss at step 1450: 3.527\n",
      "Average minibatch loss at step 1452: 3.502\n",
      "Average minibatch loss at step 1454: 3.489\n",
      "Average minibatch loss at step 1456: 3.481\n",
      "Average minibatch loss at step 1458: 3.473\n",
      "Average minibatch loss at step 1460: 3.469\n",
      "Average minibatch loss at step 1462: 3.462\n",
      "Average minibatch loss at step 1464: 3.461\n",
      "Average minibatch loss at step 1466: 3.454\n",
      "Average minibatch loss at step 1468: 3.455\n",
      "Average minibatch loss at step 1470: 3.446\n",
      "Average minibatch loss at step 1472: 3.451\n",
      "Average minibatch loss at step 1474: 3.436\n",
      "Average minibatch loss at step 1476: 3.445\n",
      "Average minibatch loss at step 1478: 3.429\n",
      "Average minibatch loss at step 1480: 3.436\n",
      "Average minibatch loss at step 1482: 3.424\n",
      "Average minibatch loss at step 1484: 3.434\n",
      "Average minibatch loss at step 1486: 3.419\n",
      "Average minibatch loss at step 1488: 3.428\n",
      "Average minibatch loss at step 1490: 3.417\n",
      "Average minibatch loss at step 1492: 3.428\n",
      "Average minibatch loss at step 1494: 3.411\n",
      "Average minibatch loss at step 1496: 3.419\n",
      "Average minibatch loss at step 1498: 3.408\n",
      "Average minibatch loss at step 1500: 3.418\n",
      "\n",
      "learning rate: 0.125000\n",
      "\n",
      "Average minibatch loss at step 1502: 3.391\n",
      "Average minibatch loss at step 1504: 3.372\n",
      "Average minibatch loss at step 1506: 3.370\n",
      "Average minibatch loss at step 1508: 3.368\n",
      "Average minibatch loss at step 1510: 3.367\n",
      "Average minibatch loss at step 1512: 3.365\n",
      "Average minibatch loss at step 1514: 3.363\n",
      "Average minibatch loss at step 1516: 3.361\n",
      "Average minibatch loss at step 1518: 3.359\n",
      "Average minibatch loss at step 1520: 3.358\n",
      "Average minibatch loss at step 1522: 3.356\n",
      "Average minibatch loss at step 1524: 3.354\n",
      "Average minibatch loss at step 1526: 3.352\n",
      "Average minibatch loss at step 1528: 3.351\n",
      "Average minibatch loss at step 1530: 3.349\n",
      "Average minibatch loss at step 1532: 3.348\n",
      "Average minibatch loss at step 1534: 3.346\n",
      "Average minibatch loss at step 1536: 3.344\n",
      "Average minibatch loss at step 1538: 3.342\n",
      "Average minibatch loss at step 1540: 3.340\n",
      "Average minibatch loss at step 1542: 3.339\n",
      "Average minibatch loss at step 1544: 3.337\n",
      "Average minibatch loss at step 1546: 3.335\n",
      "Average minibatch loss at step 1548: 3.334\n",
      "Average minibatch loss at step 1550: 3.332\n",
      "Average minibatch loss at step 1552: 3.330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1554: 3.329\n",
      "Average minibatch loss at step 1556: 3.327\n",
      "Average minibatch loss at step 1558: 3.325\n",
      "Average minibatch loss at step 1560: 3.323\n",
      "Average minibatch loss at step 1562: 3.322\n",
      "Average minibatch loss at step 1564: 3.320\n",
      "Average minibatch loss at step 1566: 3.318\n",
      "Average minibatch loss at step 1568: 3.317\n",
      "Average minibatch loss at step 1570: 3.316\n",
      "Average minibatch loss at step 1572: 3.314\n",
      "Average minibatch loss at step 1574: 3.312\n",
      "Average minibatch loss at step 1576: 3.311\n",
      "Average minibatch loss at step 1578: 3.309\n",
      "Average minibatch loss at step 1580: 3.308\n",
      "Average minibatch loss at step 1582: 3.306\n",
      "Average minibatch loss at step 1584: 3.305\n",
      "Average minibatch loss at step 1586: 3.303\n",
      "Average minibatch loss at step 1588: 3.301\n",
      "Average minibatch loss at step 1590: 3.300\n",
      "Average minibatch loss at step 1592: 3.298\n",
      "Average minibatch loss at step 1594: 3.296\n",
      "Average minibatch loss at step 1596: 3.295\n",
      "Average minibatch loss at step 1598: 3.293\n",
      "Average minibatch loss at step 1600: 3.292\n",
      "Average minibatch loss at step 1602: 3.290\n",
      "Average minibatch loss at step 1604: 3.289\n",
      "Average minibatch loss at step 1606: 3.287\n",
      "Average minibatch loss at step 1608: 3.285\n",
      "Average minibatch loss at step 1610: 3.284\n",
      "Average minibatch loss at step 1612: 3.282\n",
      "Average minibatch loss at step 1614: 3.280\n",
      "Average minibatch loss at step 1616: 3.279\n",
      "Average minibatch loss at step 1618: 3.277\n",
      "Average minibatch loss at step 1620: 3.275\n",
      "Average minibatch loss at step 1622: 3.274\n",
      "Average minibatch loss at step 1624: 3.272\n",
      "Average minibatch loss at step 1626: 3.271\n",
      "Average minibatch loss at step 1628: 3.269\n",
      "Average minibatch loss at step 1630: 3.267\n",
      "Average minibatch loss at step 1632: 3.266\n",
      "Average minibatch loss at step 1634: 3.264\n",
      "Average minibatch loss at step 1636: 3.262\n",
      "Average minibatch loss at step 1638: 3.261\n",
      "Average minibatch loss at step 1640: 3.259\n",
      "Average minibatch loss at step 1642: 3.257\n",
      "Average minibatch loss at step 1644: 3.256\n",
      "Average minibatch loss at step 1646: 3.254\n",
      "Average minibatch loss at step 1648: 3.252\n",
      "Average minibatch loss at step 1650: 3.250\n",
      "Average minibatch loss at step 1652: 3.248\n",
      "Average minibatch loss at step 1654: 3.247\n",
      "Average minibatch loss at step 1656: 3.245\n",
      "Average minibatch loss at step 1658: 3.243\n",
      "Average minibatch loss at step 1660: 3.242\n",
      "Average minibatch loss at step 1662: 3.240\n",
      "Average minibatch loss at step 1664: 3.238\n",
      "Average minibatch loss at step 1666: 3.237\n",
      "Average minibatch loss at step 1668: 3.235\n",
      "Average minibatch loss at step 1670: 3.233\n",
      "Average minibatch loss at step 1672: 3.232\n",
      "Average minibatch loss at step 1674: 3.230\n",
      "Average minibatch loss at step 1676: 3.228\n",
      "Average minibatch loss at step 1678: 3.227\n",
      "Average minibatch loss at step 1680: 3.225\n",
      "Average minibatch loss at step 1682: 3.223\n",
      "Average minibatch loss at step 1684: 3.221\n",
      "Average minibatch loss at step 1686: 3.220\n",
      "Average minibatch loss at step 1688: 3.218\n",
      "Average minibatch loss at step 1690: 3.216\n",
      "Average minibatch loss at step 1692: 3.215\n",
      "Average minibatch loss at step 1694: 3.213\n",
      "Average minibatch loss at step 1696: 3.211\n",
      "Average minibatch loss at step 1698: 3.210\n",
      "Average minibatch loss at step 1700: 3.208\n",
      "Average minibatch loss at step 1702: 3.206\n",
      "Average minibatch loss at step 1704: 3.205\n",
      "Average minibatch loss at step 1706: 3.203\n",
      "Average minibatch loss at step 1708: 3.202\n",
      "Average minibatch loss at step 1710: 3.200\n",
      "Average minibatch loss at step 1712: 3.198\n",
      "Average minibatch loss at step 1714: 3.196\n",
      "Average minibatch loss at step 1716: 3.195\n",
      "Average minibatch loss at step 1718: 3.193\n",
      "Average minibatch loss at step 1720: 3.192\n",
      "Average minibatch loss at step 1722: 3.191\n",
      "Average minibatch loss at step 1724: 3.190\n",
      "Average minibatch loss at step 1726: 3.188\n",
      "Average minibatch loss at step 1728: 3.187\n",
      "Average minibatch loss at step 1730: 3.187\n",
      "Average minibatch loss at step 1732: 3.185\n",
      "Average minibatch loss at step 1734: 3.183\n",
      "Average minibatch loss at step 1736: 3.181\n",
      "Average minibatch loss at step 1738: 3.179\n",
      "Average minibatch loss at step 1740: 3.177\n",
      "Average minibatch loss at step 1742: 3.175\n",
      "Average minibatch loss at step 1744: 3.173\n",
      "Average minibatch loss at step 1746: 3.172\n",
      "Average minibatch loss at step 1748: 3.170\n",
      "Average minibatch loss at step 1750: 3.168\n",
      "Average minibatch loss at step 1752: 3.167\n",
      "Average minibatch loss at step 1754: 3.165\n",
      "Average minibatch loss at step 1756: 3.163\n",
      "Average minibatch loss at step 1758: 3.162\n",
      "Average minibatch loss at step 1760: 3.160\n",
      "Average minibatch loss at step 1762: 3.158\n",
      "Average minibatch loss at step 1764: 3.157\n",
      "Average minibatch loss at step 1766: 3.155\n",
      "Average minibatch loss at step 1768: 3.152\n",
      "Average minibatch loss at step 1770: 3.151\n",
      "Average minibatch loss at step 1772: 3.149\n",
      "Average minibatch loss at step 1774: 3.148\n",
      "Average minibatch loss at step 1776: 3.146\n",
      "Average minibatch loss at step 1778: 3.144\n",
      "Average minibatch loss at step 1780: 3.143\n",
      "Average minibatch loss at step 1782: 3.140\n",
      "Average minibatch loss at step 1784: 3.139\n",
      "Average minibatch loss at step 1786: 3.137\n",
      "Average minibatch loss at step 1788: 3.135\n",
      "Average minibatch loss at step 1790: 3.134\n",
      "Average minibatch loss at step 1792: 3.132\n",
      "Average minibatch loss at step 1794: 3.131\n",
      "Average minibatch loss at step 1796: 3.129\n",
      "Average minibatch loss at step 1798: 3.127\n",
      "Average minibatch loss at step 1800: 3.126\n",
      "Average minibatch loss at step 1802: 3.124\n",
      "Average minibatch loss at step 1804: 3.122\n",
      "Average minibatch loss at step 1806: 3.120\n",
      "Average minibatch loss at step 1808: 3.118\n",
      "Average minibatch loss at step 1810: 3.116\n",
      "Average minibatch loss at step 1812: 3.115\n",
      "Average minibatch loss at step 1814: 3.113\n",
      "Average minibatch loss at step 1816: 3.111\n",
      "Average minibatch loss at step 1818: 3.109\n",
      "Average minibatch loss at step 1820: 3.107\n",
      "Average minibatch loss at step 1822: 3.106\n",
      "Average minibatch loss at step 1824: 3.104\n",
      "Average minibatch loss at step 1826: 3.102\n",
      "Average minibatch loss at step 1828: 3.100\n",
      "Average minibatch loss at step 1830: 3.099\n",
      "Average minibatch loss at step 1832: 3.097\n",
      "Average minibatch loss at step 1834: 3.095\n",
      "Average minibatch loss at step 1836: 3.094\n",
      "Average minibatch loss at step 1838: 3.092\n",
      "Average minibatch loss at step 1840: 3.090\n",
      "Average minibatch loss at step 1842: 3.088\n",
      "Average minibatch loss at step 1844: 3.087\n",
      "Average minibatch loss at step 1846: 3.085\n",
      "Average minibatch loss at step 1848: 3.083\n",
      "Average minibatch loss at step 1850: 3.081\n",
      "Average minibatch loss at step 1852: 3.079\n",
      "Average minibatch loss at step 1854: 3.077\n",
      "Average minibatch loss at step 1856: 3.075\n",
      "Average minibatch loss at step 1858: 3.073\n",
      "Average minibatch loss at step 1860: 3.072\n",
      "Average minibatch loss at step 1862: 3.070\n",
      "Average minibatch loss at step 1864: 3.068\n",
      "Average minibatch loss at step 1866: 3.066\n",
      "Average minibatch loss at step 1868: 3.064\n",
      "Average minibatch loss at step 1870: 3.062\n",
      "Average minibatch loss at step 1872: 3.060\n",
      "Average minibatch loss at step 1874: 3.058\n",
      "Average minibatch loss at step 1876: 3.056\n",
      "Average minibatch loss at step 1878: 3.054\n",
      "Average minibatch loss at step 1880: 3.052\n",
      "Average minibatch loss at step 1882: 3.050\n",
      "Average minibatch loss at step 1884: 3.049\n",
      "Average minibatch loss at step 1886: 3.047\n",
      "Average minibatch loss at step 1888: 3.045\n",
      "Average minibatch loss at step 1890: 3.043\n",
      "Average minibatch loss at step 1892: 3.041\n",
      "Average minibatch loss at step 1894: 3.039\n",
      "Average minibatch loss at step 1896: 3.037\n",
      "Average minibatch loss at step 1898: 3.035\n",
      "Average minibatch loss at step 1900: 3.033\n",
      "Average minibatch loss at step 1902: 3.030\n",
      "Average minibatch loss at step 1904: 3.029\n",
      "Average minibatch loss at step 1906: 3.026\n",
      "Average minibatch loss at step 1908: 3.025\n",
      "Average minibatch loss at step 1910: 3.022\n",
      "Average minibatch loss at step 1912: 3.020\n",
      "Average minibatch loss at step 1914: 3.018\n",
      "Average minibatch loss at step 1916: 3.017\n",
      "Average minibatch loss at step 1918: 3.018\n",
      "Average minibatch loss at step 1920: 3.013\n",
      "Average minibatch loss at step 1922: 3.012\n",
      "Average minibatch loss at step 1924: 3.012\n",
      "Average minibatch loss at step 1926: 3.029\n",
      "Average minibatch loss at step 1928: 3.086\n",
      "Average minibatch loss at step 1930: 3.161\n",
      "Average minibatch loss at step 1932: 3.038\n",
      "Average minibatch loss at step 1934: 3.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1936: 2.996\n",
      "Average minibatch loss at step 1938: 2.995\n",
      "Average minibatch loss at step 1940: 2.994\n",
      "Average minibatch loss at step 1942: 2.992\n",
      "Average minibatch loss at step 1944: 2.992\n",
      "Average minibatch loss at step 1946: 2.994\n",
      "Average minibatch loss at step 1948: 2.991\n",
      "Average minibatch loss at step 1950: 2.984\n",
      "Average minibatch loss at step 1952: 2.984\n",
      "Average minibatch loss at step 1954: 2.983\n",
      "Average minibatch loss at step 1956: 2.983\n",
      "Average minibatch loss at step 1958: 2.979\n",
      "Average minibatch loss at step 1960: 2.975\n",
      "Average minibatch loss at step 1962: 2.974\n",
      "Average minibatch loss at step 1964: 2.972\n",
      "Average minibatch loss at step 1966: 2.970\n",
      "Average minibatch loss at step 1968: 2.966\n",
      "Average minibatch loss at step 1970: 2.965\n",
      "Average minibatch loss at step 1972: 2.962\n",
      "Average minibatch loss at step 1974: 2.961\n",
      "Average minibatch loss at step 1976: 2.960\n",
      "Average minibatch loss at step 1978: 2.956\n",
      "Average minibatch loss at step 1980: 2.953\n",
      "Average minibatch loss at step 1982: 2.953\n",
      "Average minibatch loss at step 1984: 2.953\n",
      "Average minibatch loss at step 1986: 2.951\n",
      "Average minibatch loss at step 1988: 2.946\n",
      "Average minibatch loss at step 1990: 2.945\n",
      "Average minibatch loss at step 1992: 2.948\n",
      "Average minibatch loss at step 1994: 2.947\n",
      "Average minibatch loss at step 1996: 2.949\n",
      "Average minibatch loss at step 1998: 2.947\n",
      "Average minibatch loss at step 2000: 2.949\n",
      "Average minibatch loss at step 2002: 2.944\n",
      "Average minibatch loss at step 2004: 2.939\n",
      "Average minibatch loss at step 2006: 2.936\n",
      "Average minibatch loss at step 2008: 2.929\n",
      "Average minibatch loss at step 2010: 2.936\n",
      "Average minibatch loss at step 2012: 2.937\n",
      "Average minibatch loss at step 2014: 2.931\n",
      "Average minibatch loss at step 2016: 2.924\n",
      "Average minibatch loss at step 2018: 2.920\n",
      "Average minibatch loss at step 2020: 2.915\n",
      "Average minibatch loss at step 2022: 2.915\n",
      "Average minibatch loss at step 2024: 2.912\n",
      "Average minibatch loss at step 2026: 2.908\n",
      "Average minibatch loss at step 2028: 2.908\n",
      "Average minibatch loss at step 2030: 2.905\n",
      "Average minibatch loss at step 2032: 2.904\n",
      "Average minibatch loss at step 2034: 2.900\n",
      "Average minibatch loss at step 2036: 2.901\n",
      "Average minibatch loss at step 2038: 2.898\n",
      "Average minibatch loss at step 2040: 2.897\n",
      "Average minibatch loss at step 2042: 2.898\n",
      "Average minibatch loss at step 2044: 2.899\n",
      "Average minibatch loss at step 2046: 2.900\n",
      "Average minibatch loss at step 2048: 2.901\n",
      "Average minibatch loss at step 2050: 2.906\n",
      "Average minibatch loss at step 2052: 2.938\n",
      "Average minibatch loss at step 2054: 3.064\n",
      "Average minibatch loss at step 2056: 2.946\n",
      "Average minibatch loss at step 2058: 2.880\n",
      "Average minibatch loss at step 2060: 2.866\n",
      "Average minibatch loss at step 2062: 2.865\n",
      "Average minibatch loss at step 2064: 2.872\n",
      "Average minibatch loss at step 2066: 2.864\n",
      "Average minibatch loss at step 2068: 2.868\n",
      "Average minibatch loss at step 2070: 2.865\n",
      "Average minibatch loss at step 2072: 2.861\n",
      "Average minibatch loss at step 2074: 2.857\n",
      "Average minibatch loss at step 2076: 2.863\n",
      "Average minibatch loss at step 2078: 2.854\n",
      "Average minibatch loss at step 2080: 2.856\n",
      "Average minibatch loss at step 2082: 2.849\n",
      "Average minibatch loss at step 2084: 2.849\n",
      "Average minibatch loss at step 2086: 2.839\n",
      "Average minibatch loss at step 2088: 2.840\n",
      "Average minibatch loss at step 2090: 2.834\n",
      "Average minibatch loss at step 2092: 2.836\n",
      "Average minibatch loss at step 2094: 2.826\n",
      "Average minibatch loss at step 2096: 2.831\n",
      "Average minibatch loss at step 2098: 2.833\n",
      "Average minibatch loss at step 2100: 2.828\n",
      "Average minibatch loss at step 2102: 2.833\n",
      "Average minibatch loss at step 2104: 2.828\n",
      "Average minibatch loss at step 2106: 2.831\n",
      "Average minibatch loss at step 2108: 2.820\n",
      "Average minibatch loss at step 2110: 2.823\n",
      "Average minibatch loss at step 2112: 2.816\n",
      "Average minibatch loss at step 2114: 2.823\n",
      "Average minibatch loss at step 2116: 2.831\n",
      "Average minibatch loss at step 2118: 2.901\n",
      "Average minibatch loss at step 2120: 4.144\n",
      "Average minibatch loss at step 2122: 5.992\n",
      "Average minibatch loss at step 2124: 3.234\n",
      "Average minibatch loss at step 2126: 2.875\n",
      "Average minibatch loss at step 2128: 2.844\n",
      "Average minibatch loss at step 2130: 2.829\n",
      "Average minibatch loss at step 2132: 2.817\n",
      "Average minibatch loss at step 2134: 2.812\n",
      "Average minibatch loss at step 2136: 2.802\n",
      "Average minibatch loss at step 2138: 2.797\n",
      "Average minibatch loss at step 2140: 2.795\n",
      "Average minibatch loss at step 2142: 2.793\n",
      "Average minibatch loss at step 2144: 2.792\n",
      "Average minibatch loss at step 2146: 2.786\n",
      "Average minibatch loss at step 2148: 2.784\n",
      "Average minibatch loss at step 2150: 2.778\n",
      "Average minibatch loss at step 2152: 2.777\n",
      "Average minibatch loss at step 2154: 2.773\n",
      "Average minibatch loss at step 2156: 2.770\n",
      "Average minibatch loss at step 2158: 2.771\n",
      "Average minibatch loss at step 2160: 2.769\n",
      "Average minibatch loss at step 2162: 2.772\n",
      "Average minibatch loss at step 2164: 2.782\n",
      "Average minibatch loss at step 2166: 2.818\n",
      "Average minibatch loss at step 2168: 2.855\n",
      "Average minibatch loss at step 2170: 2.840\n",
      "Average minibatch loss at step 2172: 2.774\n",
      "Average minibatch loss at step 2174: 2.763\n",
      "Average minibatch loss at step 2176: 2.748\n",
      "Average minibatch loss at step 2178: 2.750\n",
      "Average minibatch loss at step 2180: 2.750\n",
      "Average minibatch loss at step 2182: 2.748\n",
      "Average minibatch loss at step 2184: 2.747\n",
      "Average minibatch loss at step 2186: 2.747\n",
      "Average minibatch loss at step 2188: 2.739\n",
      "Average minibatch loss at step 2190: 2.738\n",
      "Average minibatch loss at step 2192: 2.731\n",
      "Average minibatch loss at step 2194: 2.729\n",
      "Average minibatch loss at step 2196: 2.725\n",
      "Average minibatch loss at step 2198: 2.724\n",
      "Average minibatch loss at step 2200: 2.721\n",
      "Average minibatch loss at step 2202: 2.722\n",
      "Average minibatch loss at step 2204: 2.726\n",
      "Average minibatch loss at step 2206: 2.732\n",
      "Average minibatch loss at step 2208: 2.746\n",
      "Average minibatch loss at step 2210: 2.778\n",
      "Average minibatch loss at step 2212: 2.756\n",
      "Average minibatch loss at step 2214: 2.711\n",
      "Average minibatch loss at step 2216: 2.695\n",
      "Average minibatch loss at step 2218: 2.695\n",
      "Average minibatch loss at step 2220: 2.712\n",
      "Average minibatch loss at step 2222: 2.737\n",
      "Average minibatch loss at step 2224: 2.750\n",
      "Average minibatch loss at step 2226: 2.842\n",
      "Average minibatch loss at step 2228: 4.901\n",
      "Average minibatch loss at step 2230: 2.924\n",
      "Average minibatch loss at step 2232: 3.785\n",
      "Average minibatch loss at step 2234: 2.724\n",
      "Average minibatch loss at step 2236: 2.701\n",
      "Average minibatch loss at step 2238: 2.691\n",
      "Average minibatch loss at step 2240: 2.684\n",
      "Average minibatch loss at step 2242: 2.679\n",
      "Average minibatch loss at step 2244: 2.675\n",
      "Average minibatch loss at step 2246: 2.672\n",
      "Average minibatch loss at step 2248: 2.671\n",
      "Average minibatch loss at step 2250: 2.667\n",
      "Average minibatch loss at step 2252: 2.667\n",
      "Average minibatch loss at step 2254: 2.666\n",
      "Average minibatch loss at step 2256: 2.661\n",
      "Average minibatch loss at step 2258: 2.656\n",
      "Average minibatch loss at step 2260: 2.653\n",
      "Average minibatch loss at step 2262: 2.648\n",
      "Average minibatch loss at step 2264: 2.645\n",
      "Average minibatch loss at step 2266: 2.643\n",
      "Average minibatch loss at step 2268: 2.645\n",
      "Average minibatch loss at step 2270: 2.646\n",
      "Average minibatch loss at step 2272: 2.649\n",
      "Average minibatch loss at step 2274: 2.651\n",
      "Average minibatch loss at step 2276: 2.650\n",
      "Average minibatch loss at step 2278: 2.650\n",
      "Average minibatch loss at step 2280: 2.644\n",
      "Average minibatch loss at step 2282: 2.658\n",
      "Average minibatch loss at step 2284: 2.665\n",
      "Average minibatch loss at step 2286: 2.660\n",
      "Average minibatch loss at step 2288: 2.629\n",
      "Average minibatch loss at step 2290: 2.628\n",
      "Average minibatch loss at step 2292: 2.613\n",
      "Average minibatch loss at step 2294: 2.617\n",
      "Average minibatch loss at step 2296: 2.617\n",
      "Average minibatch loss at step 2298: 2.610\n",
      "Average minibatch loss at step 2300: 2.609\n",
      "Average minibatch loss at step 2302: 2.601\n",
      "Average minibatch loss at step 2304: 2.603\n",
      "Average minibatch loss at step 2306: 2.601\n",
      "Average minibatch loss at step 2308: 2.611\n",
      "Average minibatch loss at step 2310: 2.636\n",
      "Average minibatch loss at step 2312: 2.686\n",
      "Average minibatch loss at step 2314: 2.737\n",
      "Average minibatch loss at step 2316: 2.663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 2318: 2.596\n",
      "Average minibatch loss at step 2320: 2.586\n",
      "Average minibatch loss at step 2322: 2.582\n",
      "Average minibatch loss at step 2324: 2.574\n",
      "Average minibatch loss at step 2326: 2.571\n",
      "Average minibatch loss at step 2328: 2.567\n",
      "Average minibatch loss at step 2330: 2.566\n",
      "Average minibatch loss at step 2332: 2.561\n",
      "Average minibatch loss at step 2334: 2.560\n",
      "Average minibatch loss at step 2336: 2.558\n",
      "Average minibatch loss at step 2338: 2.552\n",
      "Average minibatch loss at step 2340: 2.552\n",
      "Average minibatch loss at step 2342: 2.550\n",
      "Average minibatch loss at step 2344: 2.547\n",
      "Average minibatch loss at step 2346: 2.543\n",
      "Average minibatch loss at step 2348: 2.541\n",
      "Average minibatch loss at step 2350: 2.542\n",
      "Average minibatch loss at step 2352: 2.542\n",
      "Average minibatch loss at step 2354: 2.542\n",
      "Average minibatch loss at step 2356: 2.540\n",
      "Average minibatch loss at step 2358: 2.542\n",
      "Average minibatch loss at step 2360: 2.569\n",
      "Average minibatch loss at step 2362: 2.601\n",
      "Average minibatch loss at step 2364: 2.645\n",
      "Average minibatch loss at step 2366: 2.780\n",
      "Average minibatch loss at step 2368: 4.261\n",
      "Average minibatch loss at step 2370: 2.879\n",
      "Average minibatch loss at step 2372: 3.745\n",
      "Average minibatch loss at step 2374: 2.569\n",
      "Average minibatch loss at step 2376: 2.531\n",
      "Average minibatch loss at step 2378: 2.521\n",
      "Average minibatch loss at step 2380: 2.531\n",
      "Average minibatch loss at step 2382: 2.511\n",
      "Average minibatch loss at step 2384: 2.516\n",
      "Average minibatch loss at step 2386: 2.505\n",
      "Average minibatch loss at step 2388: 2.509\n",
      "Average minibatch loss at step 2390: 2.520\n",
      "Average minibatch loss at step 2392: 2.545\n",
      "Average minibatch loss at step 2394: 2.507\n",
      "Average minibatch loss at step 2396: 2.499\n",
      "Average minibatch loss at step 2398: 2.496\n",
      "Average minibatch loss at step 2400: 2.493\n",
      "Average minibatch loss at step 2402: 2.483\n",
      "Average minibatch loss at step 2404: 2.484\n",
      "Average minibatch loss at step 2406: 2.475\n",
      "Average minibatch loss at step 2408: 2.478\n",
      "Average minibatch loss at step 2410: 2.474\n",
      "Average minibatch loss at step 2412: 2.464\n",
      "Average minibatch loss at step 2414: 2.465\n",
      "Average minibatch loss at step 2416: 2.459\n",
      "Average minibatch loss at step 2418: 2.460\n",
      "Average minibatch loss at step 2420: 2.456\n",
      "Average minibatch loss at step 2422: 2.456\n",
      "Average minibatch loss at step 2424: 2.466\n",
      "Average minibatch loss at step 2426: 2.498\n",
      "Average minibatch loss at step 2428: 2.531\n",
      "Average minibatch loss at step 2430: 2.496\n",
      "Average minibatch loss at step 2432: 2.483\n",
      "Average minibatch loss at step 2434: 2.478\n",
      "Average minibatch loss at step 2436: 2.453\n",
      "Average minibatch loss at step 2438: 2.432\n",
      "Average minibatch loss at step 2440: 2.425\n",
      "Average minibatch loss at step 2442: 2.426\n",
      "Average minibatch loss at step 2444: 2.426\n",
      "Average minibatch loss at step 2446: 2.420\n",
      "Average minibatch loss at step 2448: 2.411\n",
      "Average minibatch loss at step 2450: 2.405\n",
      "Average minibatch loss at step 2452: 2.403\n",
      "Average minibatch loss at step 2454: 2.405\n",
      "Average minibatch loss at step 2456: 2.412\n",
      "Average minibatch loss at step 2458: 2.419\n",
      "Average minibatch loss at step 2460: 2.427\n",
      "Average minibatch loss at step 2462: 2.400\n",
      "Average minibatch loss at step 2464: 2.384\n",
      "Average minibatch loss at step 2466: 2.397\n",
      "Average minibatch loss at step 2468: 2.472\n",
      "Average minibatch loss at step 2470: 2.449\n",
      "Average minibatch loss at step 2472: 2.397\n",
      "Average minibatch loss at step 2474: 2.395\n",
      "Average minibatch loss at step 2476: 2.406\n",
      "Average minibatch loss at step 2478: 2.417\n",
      "Average minibatch loss at step 2480: 2.429\n",
      "Average minibatch loss at step 2482: 2.421\n",
      "Average minibatch loss at step 2484: 2.384\n",
      "Average minibatch loss at step 2486: 2.382\n",
      "Average minibatch loss at step 2488: 2.424\n",
      "Average minibatch loss at step 2490: 2.421\n",
      "Average minibatch loss at step 2492: 2.377\n",
      "Average minibatch loss at step 2494: 2.368\n",
      "Average minibatch loss at step 2496: 2.381\n",
      "Average minibatch loss at step 2498: 2.367\n",
      "Average minibatch loss at step 2500: 2.348\n",
      "Average minibatch loss at step 2502: 2.348\n",
      "Average minibatch loss at step 2504: 2.349\n",
      "Average minibatch loss at step 2506: 2.381\n",
      "Average minibatch loss at step 2508: 2.414\n",
      "Average minibatch loss at step 2510: 2.389\n",
      "Average minibatch loss at step 2512: 2.345\n",
      "Average minibatch loss at step 2514: 2.327\n",
      "Average minibatch loss at step 2516: 2.330\n",
      "Average minibatch loss at step 2518: 2.359\n",
      "Average minibatch loss at step 2520: 2.377\n",
      "Average minibatch loss at step 2522: 2.383\n",
      "Average minibatch loss at step 2524: 2.544\n",
      "Average minibatch loss at step 2526: 4.600\n",
      "Average minibatch loss at step 2528: 3.664\n",
      "Average minibatch loss at step 2530: 2.601\n",
      "Average minibatch loss at step 2532: 2.402\n",
      "Average minibatch loss at step 2534: 2.374\n",
      "Average minibatch loss at step 2536: 2.347\n",
      "Average minibatch loss at step 2538: 2.342\n",
      "Average minibatch loss at step 2540: 2.342\n",
      "Average minibatch loss at step 2542: 2.340\n",
      "Average minibatch loss at step 2544: 2.339\n",
      "Average minibatch loss at step 2546: 2.337\n",
      "Average minibatch loss at step 2548: 2.328\n",
      "Average minibatch loss at step 2550: 2.324\n",
      "Average minibatch loss at step 2552: 2.314\n",
      "Average minibatch loss at step 2554: 2.305\n",
      "Average minibatch loss at step 2556: 2.297\n",
      "Average minibatch loss at step 2558: 2.291\n",
      "Average minibatch loss at step 2560: 2.290\n",
      "Average minibatch loss at step 2562: 2.291\n",
      "Average minibatch loss at step 2564: 2.295\n",
      "Average minibatch loss at step 2566: 2.288\n",
      "Average minibatch loss at step 2568: 2.282\n",
      "Average minibatch loss at step 2570: 2.304\n",
      "Average minibatch loss at step 2572: 2.320\n",
      "Average minibatch loss at step 2574: 2.287\n",
      "Average minibatch loss at step 2576: 2.263\n",
      "Average minibatch loss at step 2578: 2.253\n",
      "Average minibatch loss at step 2580: 2.253\n",
      "Average minibatch loss at step 2582: 2.255\n",
      "Average minibatch loss at step 2584: 2.258\n",
      "Average minibatch loss at step 2586: 2.266\n",
      "Average minibatch loss at step 2588: 2.272\n",
      "Average minibatch loss at step 2590: 2.247\n",
      "Average minibatch loss at step 2592: 2.229\n",
      "Average minibatch loss at step 2594: 2.253\n",
      "Average minibatch loss at step 2596: 2.283\n",
      "Average minibatch loss at step 2598: 2.261\n",
      "Average minibatch loss at step 2600: 2.239\n",
      "Average minibatch loss at step 2602: 2.251\n",
      "Average minibatch loss at step 2604: 2.245\n",
      "Average minibatch loss at step 2606: 2.234\n",
      "Average minibatch loss at step 2608: 2.218\n",
      "Average minibatch loss at step 2610: 2.213\n",
      "Average minibatch loss at step 2612: 2.212\n",
      "Average minibatch loss at step 2614: 2.239\n",
      "Average minibatch loss at step 2616: 2.299\n",
      "Average minibatch loss at step 2618: 2.216\n",
      "Average minibatch loss at step 2620: 2.213\n",
      "Average minibatch loss at step 2622: 2.222\n",
      "Average minibatch loss at step 2624: 2.208\n",
      "Average minibatch loss at step 2626: 2.179\n",
      "Average minibatch loss at step 2628: 2.165\n",
      "Average minibatch loss at step 2630: 2.167\n",
      "Average minibatch loss at step 2632: 2.195\n",
      "Average minibatch loss at step 2634: 2.208\n",
      "Average minibatch loss at step 2636: 2.199\n",
      "Average minibatch loss at step 2638: 2.186\n",
      "Average minibatch loss at step 2640: 2.181\n",
      "Average minibatch loss at step 2642: 2.218\n",
      "Average minibatch loss at step 2644: 2.207\n",
      "Average minibatch loss at step 2646: 2.155\n",
      "Average minibatch loss at step 2648: 2.162\n",
      "Average minibatch loss at step 2650: 2.165\n",
      "Average minibatch loss at step 2652: 2.144\n",
      "Average minibatch loss at step 2654: 2.127\n",
      "Average minibatch loss at step 2656: 2.133\n",
      "Average minibatch loss at step 2658: 2.146\n",
      "Average minibatch loss at step 2660: 2.186\n",
      "Average minibatch loss at step 2662: 2.240\n",
      "Average minibatch loss at step 2664: 2.266\n",
      "Average minibatch loss at step 2666: 2.311\n",
      "Average minibatch loss at step 2668: 2.135\n",
      "Average minibatch loss at step 2670: 2.101\n",
      "Average minibatch loss at step 2672: 2.098\n",
      "Average minibatch loss at step 2674: 2.108\n",
      "Average minibatch loss at step 2676: 2.118\n",
      "Average minibatch loss at step 2678: 2.121\n",
      "Average minibatch loss at step 2680: 2.126\n",
      "Average minibatch loss at step 2682: 2.159\n",
      "Average minibatch loss at step 2684: 2.173\n",
      "Average minibatch loss at step 2686: 2.135\n",
      "Average minibatch loss at step 2688: 2.123\n",
      "Average minibatch loss at step 2690: 2.105\n",
      "Average minibatch loss at step 2692: 2.078\n",
      "Average minibatch loss at step 2694: 2.066\n",
      "Average minibatch loss at step 2696: 2.059\n",
      "Average minibatch loss at step 2698: 2.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 2700: 2.076\n",
      "Average minibatch loss at step 2702: 2.101\n",
      "Average minibatch loss at step 2704: 2.089\n",
      "Average minibatch loss at step 2706: 2.044\n",
      "Average minibatch loss at step 2708: 2.041\n",
      "Average minibatch loss at step 2710: 2.070\n",
      "Average minibatch loss at step 2712: 2.108\n",
      "Average minibatch loss at step 2714: 2.131\n",
      "Average minibatch loss at step 2716: 2.100\n",
      "Average minibatch loss at step 2718: 2.111\n",
      "Average minibatch loss at step 2720: 2.176\n",
      "Average minibatch loss at step 2722: 2.369\n",
      "Average minibatch loss at step 2724: 3.412\n",
      "Average minibatch loss at step 2726: 2.890\n",
      "Average minibatch loss at step 2728: 4.126\n",
      "Average minibatch loss at step 2730: 2.284\n",
      "Average minibatch loss at step 2732: 2.086\n",
      "Average minibatch loss at step 2734: 2.043\n",
      "Average minibatch loss at step 2736: 2.027\n",
      "Average minibatch loss at step 2738: 2.021\n",
      "Average minibatch loss at step 2740: 2.030\n",
      "Average minibatch loss at step 2742: 2.062\n",
      "Average minibatch loss at step 2744: 2.088\n",
      "Average minibatch loss at step 2746: 2.020\n",
      "Average minibatch loss at step 2748: 2.015\n",
      "Average minibatch loss at step 2750: 2.004\n",
      "Average minibatch loss at step 2752: 1.982\n",
      "Average minibatch loss at step 2754: 1.969\n",
      "Average minibatch loss at step 2756: 1.964\n",
      "Average minibatch loss at step 2758: 1.966\n",
      "Average minibatch loss at step 2760: 1.971\n",
      "Average minibatch loss at step 2762: 1.974\n",
      "Average minibatch loss at step 2764: 1.974\n",
      "Average minibatch loss at step 2766: 1.981\n",
      "Average minibatch loss at step 2768: 1.983\n",
      "Average minibatch loss at step 2770: 1.990\n",
      "Average minibatch loss at step 2772: 1.965\n",
      "Average minibatch loss at step 2774: 1.961\n",
      "Average minibatch loss at step 2776: 1.960\n",
      "Average minibatch loss at step 2778: 1.949\n",
      "Average minibatch loss at step 2780: 1.965\n",
      "Average minibatch loss at step 2782: 1.980\n",
      "Average minibatch loss at step 2784: 2.035\n",
      "Average minibatch loss at step 2786: 2.091\n",
      "Average minibatch loss at step 2788: 2.019\n",
      "Average minibatch loss at step 2790: 1.944\n",
      "Average minibatch loss at step 2792: 1.919\n",
      "Average minibatch loss at step 2794: 1.913\n",
      "Average minibatch loss at step 2796: 1.910\n",
      "Average minibatch loss at step 2798: 1.908\n",
      "Average minibatch loss at step 2800: 1.907\n",
      "Average minibatch loss at step 2802: 1.901\n",
      "Average minibatch loss at step 2804: 1.896\n",
      "Average minibatch loss at step 2806: 1.894\n",
      "Average minibatch loss at step 2808: 1.906\n",
      "Average minibatch loss at step 2810: 1.940\n",
      "Average minibatch loss at step 2812: 1.955\n",
      "Average minibatch loss at step 2814: 2.036\n",
      "Average minibatch loss at step 2816: 2.023\n",
      "Average minibatch loss at step 2818: 1.928\n",
      "Average minibatch loss at step 2820: 1.878\n",
      "Average minibatch loss at step 2822: 1.876\n",
      "Average minibatch loss at step 2824: 1.876\n",
      "Average minibatch loss at step 2826: 1.885\n",
      "Average minibatch loss at step 2828: 1.923\n",
      "Average minibatch loss at step 2830: 1.941\n",
      "Average minibatch loss at step 2832: 1.875\n",
      "Average minibatch loss at step 2834: 1.865\n",
      "Average minibatch loss at step 2836: 1.915\n",
      "Average minibatch loss at step 2838: 1.913\n",
      "Average minibatch loss at step 2840: 1.894\n",
      "Average minibatch loss at step 2842: 1.893\n",
      "Average minibatch loss at step 2844: 1.845\n",
      "Average minibatch loss at step 2846: 1.826\n",
      "Average minibatch loss at step 2848: 1.826\n",
      "Average minibatch loss at step 2850: 1.825\n",
      "Average minibatch loss at step 2852: 1.824\n",
      "Average minibatch loss at step 2854: 1.831\n",
      "Average minibatch loss at step 2856: 1.833\n",
      "Average minibatch loss at step 2858: 1.856\n",
      "Average minibatch loss at step 2860: 1.889\n",
      "Average minibatch loss at step 2862: 1.832\n",
      "Average minibatch loss at step 2864: 1.784\n",
      "Average minibatch loss at step 2866: 1.778\n",
      "Average minibatch loss at step 2868: 1.796\n",
      "Average minibatch loss at step 2870: 1.847\n",
      "Average minibatch loss at step 2872: 1.935\n",
      "Average minibatch loss at step 2874: 1.867\n",
      "Average minibatch loss at step 2876: 1.778\n",
      "Average minibatch loss at step 2878: 1.760\n",
      "Average minibatch loss at step 2880: 1.775\n",
      "Average minibatch loss at step 2882: 1.857\n",
      "Average minibatch loss at step 2884: 1.890\n",
      "Average minibatch loss at step 2886: 1.826\n",
      "Average minibatch loss at step 2888: 1.757\n",
      "Average minibatch loss at step 2890: 1.737\n",
      "Average minibatch loss at step 2892: 1.759\n",
      "Average minibatch loss at step 2894: 1.751\n",
      "Average minibatch loss at step 2896: 1.754\n",
      "Average minibatch loss at step 2898: 1.761\n",
      "Average minibatch loss at step 2900: 1.780\n",
      "Average minibatch loss at step 2902: 1.806\n",
      "Average minibatch loss at step 2904: 1.747\n",
      "Average minibatch loss at step 2906: 1.707\n",
      "Average minibatch loss at step 2908: 1.737\n",
      "Average minibatch loss at step 2910: 1.860\n",
      "Average minibatch loss at step 2912: 1.994\n",
      "Average minibatch loss at step 2914: 4.516\n",
      "Average minibatch loss at step 2916: 3.582\n",
      "Average minibatch loss at step 2918: 1.863\n",
      "Average minibatch loss at step 2920: 1.761\n",
      "Average minibatch loss at step 2922: 1.732\n",
      "Average minibatch loss at step 2924: 1.722\n",
      "Average minibatch loss at step 2926: 1.713\n",
      "Average minibatch loss at step 2928: 1.732\n",
      "Average minibatch loss at step 2930: 1.748\n",
      "Average minibatch loss at step 2932: 1.736\n",
      "Average minibatch loss at step 2934: 1.748\n",
      "Average minibatch loss at step 2936: 1.745\n",
      "Average minibatch loss at step 2938: 1.713\n",
      "Average minibatch loss at step 2940: 1.698\n",
      "Average minibatch loss at step 2942: 1.751\n",
      "Average minibatch loss at step 2944: 1.735\n",
      "Average minibatch loss at step 2946: 1.661\n",
      "Average minibatch loss at step 2948: 1.644\n",
      "Average minibatch loss at step 2950: 1.647\n",
      "Average minibatch loss at step 2952: 1.649\n",
      "Average minibatch loss at step 2954: 1.650\n",
      "Average minibatch loss at step 2956: 1.678\n",
      "Average minibatch loss at step 2958: 1.667\n",
      "Average minibatch loss at step 2960: 1.642\n",
      "Average minibatch loss at step 2962: 1.634\n",
      "Average minibatch loss at step 2964: 1.628\n",
      "Average minibatch loss at step 2966: 1.622\n",
      "Average minibatch loss at step 2968: 1.628\n",
      "Average minibatch loss at step 2970: 1.667\n",
      "Average minibatch loss at step 2972: 1.785\n",
      "Average minibatch loss at step 2974: 1.691\n",
      "Average minibatch loss at step 2976: 1.628\n",
      "Average minibatch loss at step 2978: 1.630\n",
      "Average minibatch loss at step 2980: 1.611\n",
      "Average minibatch loss at step 2982: 1.587\n",
      "Average minibatch loss at step 2984: 1.590\n",
      "Average minibatch loss at step 2986: 1.651\n",
      "Average minibatch loss at step 2988: 1.644\n",
      "Average minibatch loss at step 2990: 1.610\n",
      "Average minibatch loss at step 2992: 1.595\n",
      "Average minibatch loss at step 2994: 1.618\n",
      "Average minibatch loss at step 2996: 1.612\n",
      "Average minibatch loss at step 2998: 1.587\n",
      "Average minibatch loss at step 3000: 1.575\n",
      "\n",
      "learning rate: 0.062500\n",
      "\n",
      "Average minibatch loss at step 3002: 1.521\n",
      "Average minibatch loss at step 3004: 1.514\n",
      "Average minibatch loss at step 3006: 1.511\n",
      "Average minibatch loss at step 3008: 1.507\n",
      "Average minibatch loss at step 3010: 1.504\n",
      "Average minibatch loss at step 3012: 1.501\n",
      "Average minibatch loss at step 3014: 1.499\n",
      "Average minibatch loss at step 3016: 1.496\n",
      "Average minibatch loss at step 3018: 1.493\n",
      "Average minibatch loss at step 3020: 1.490\n",
      "Average minibatch loss at step 3022: 1.489\n",
      "Average minibatch loss at step 3024: 1.486\n",
      "Average minibatch loss at step 3026: 1.484\n",
      "Average minibatch loss at step 3028: 1.481\n",
      "Average minibatch loss at step 3030: 1.479\n",
      "Average minibatch loss at step 3032: 1.476\n",
      "Average minibatch loss at step 3034: 1.474\n",
      "Average minibatch loss at step 3036: 1.472\n",
      "Average minibatch loss at step 3038: 1.470\n",
      "Average minibatch loss at step 3040: 1.467\n",
      "Average minibatch loss at step 3042: 1.465\n",
      "Average minibatch loss at step 3044: 1.462\n",
      "Average minibatch loss at step 3046: 1.460\n",
      "Average minibatch loss at step 3048: 1.458\n",
      "Average minibatch loss at step 3050: 1.456\n",
      "Average minibatch loss at step 3052: 1.454\n",
      "Average minibatch loss at step 3054: 1.451\n",
      "Average minibatch loss at step 3056: 1.449\n",
      "Average minibatch loss at step 3058: 1.447\n",
      "Average minibatch loss at step 3060: 1.445\n",
      "Average minibatch loss at step 3062: 1.443\n",
      "Average minibatch loss at step 3064: 1.442\n",
      "Average minibatch loss at step 3066: 1.441\n",
      "Average minibatch loss at step 3068: 1.439\n",
      "Average minibatch loss at step 3070: 1.438\n",
      "Average minibatch loss at step 3072: 1.434\n",
      "Average minibatch loss at step 3074: 1.430\n",
      "Average minibatch loss at step 3076: 1.426\n",
      "Average minibatch loss at step 3078: 1.423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 3080: 1.421\n",
      "Average minibatch loss at step 3082: 1.419\n",
      "Average minibatch loss at step 3084: 1.418\n",
      "Average minibatch loss at step 3086: 1.417\n",
      "Average minibatch loss at step 3088: 1.416\n",
      "Average minibatch loss at step 3090: 1.416\n",
      "Average minibatch loss at step 3092: 1.417\n",
      "Average minibatch loss at step 3094: 1.422\n",
      "Average minibatch loss at step 3096: 1.421\n",
      "Average minibatch loss at step 3098: 1.414\n",
      "Average minibatch loss at step 3100: 1.408\n",
      "Average minibatch loss at step 3102: 1.404\n",
      "Average minibatch loss at step 3104: 1.398\n",
      "Average minibatch loss at step 3106: 1.393\n",
      "Average minibatch loss at step 3108: 1.393\n",
      "Average minibatch loss at step 3110: 1.397\n",
      "Average minibatch loss at step 3112: 1.397\n",
      "Average minibatch loss at step 3114: 1.389\n",
      "Average minibatch loss at step 3116: 1.382\n",
      "Average minibatch loss at step 3118: 1.378\n",
      "Average minibatch loss at step 3120: 1.375\n",
      "Average minibatch loss at step 3122: 1.374\n",
      "Average minibatch loss at step 3124: 1.376\n",
      "Average minibatch loss at step 3126: 1.383\n",
      "Average minibatch loss at step 3128: 1.395\n",
      "Average minibatch loss at step 3130: 1.414\n",
      "Average minibatch loss at step 3132: 1.400\n",
      "Average minibatch loss at step 3134: 1.378\n",
      "Average minibatch loss at step 3136: 1.372\n",
      "Average minibatch loss at step 3138: 1.372\n",
      "Average minibatch loss at step 3140: 1.367\n",
      "Average minibatch loss at step 3142: 1.359\n",
      "Average minibatch loss at step 3144: 1.354\n",
      "Average minibatch loss at step 3146: 1.354\n",
      "Average minibatch loss at step 3148: 1.359\n",
      "Average minibatch loss at step 3150: 1.362\n",
      "Average minibatch loss at step 3152: 1.354\n",
      "Average minibatch loss at step 3154: 1.345\n",
      "Average minibatch loss at step 3156: 1.340\n",
      "Average minibatch loss at step 3158: 1.341\n",
      "Average minibatch loss at step 3160: 1.356\n",
      "Average minibatch loss at step 3162: 1.389\n",
      "Average minibatch loss at step 3164: 1.384\n",
      "Average minibatch loss at step 3166: 1.351\n",
      "Average minibatch loss at step 3168: 1.332\n",
      "Average minibatch loss at step 3170: 1.328\n",
      "Average minibatch loss at step 3172: 1.324\n",
      "Average minibatch loss at step 3174: 1.321\n",
      "Average minibatch loss at step 3176: 1.320\n",
      "Average minibatch loss at step 3178: 1.321\n",
      "Average minibatch loss at step 3180: 1.322\n",
      "Average minibatch loss at step 3182: 1.326\n",
      "Average minibatch loss at step 3184: 1.332\n",
      "Average minibatch loss at step 3186: 1.326\n",
      "Average minibatch loss at step 3188: 1.328\n",
      "Average minibatch loss at step 3190: 1.346\n",
      "Average minibatch loss at step 3192: 1.326\n",
      "Average minibatch loss at step 3194: 1.309\n",
      "Average minibatch loss at step 3196: 1.333\n",
      "Average minibatch loss at step 3198: 1.347\n",
      "Average minibatch loss at step 3200: 1.318\n",
      "Average minibatch loss at step 3202: 1.300\n",
      "Average minibatch loss at step 3204: 1.299\n",
      "Average minibatch loss at step 3206: 1.306\n",
      "Average minibatch loss at step 3208: 1.316\n",
      "Average minibatch loss at step 3210: 1.301\n",
      "Average minibatch loss at step 3212: 1.288\n",
      "Average minibatch loss at step 3214: 1.293\n",
      "Average minibatch loss at step 3216: 1.301\n",
      "Average minibatch loss at step 3218: 1.329\n",
      "Average minibatch loss at step 3220: 1.322\n",
      "Average minibatch loss at step 3222: 1.304\n",
      "Average minibatch loss at step 3224: 1.330\n",
      "Average minibatch loss at step 3226: 1.325\n",
      "Average minibatch loss at step 3228: 1.277\n",
      "Average minibatch loss at step 3230: 1.264\n",
      "Average minibatch loss at step 3232: 1.260\n",
      "Average minibatch loss at step 3234: 1.265\n",
      "Average minibatch loss at step 3236: 1.287\n",
      "Average minibatch loss at step 3238: 1.273\n",
      "Average minibatch loss at step 3240: 1.261\n",
      "Average minibatch loss at step 3242: 1.252\n",
      "Average minibatch loss at step 3244: 1.251\n",
      "Average minibatch loss at step 3246: 1.255\n",
      "Average minibatch loss at step 3248: 1.255\n",
      "Average minibatch loss at step 3250: 1.255\n",
      "Average minibatch loss at step 3252: 1.268\n",
      "Average minibatch loss at step 3254: 1.268\n",
      "Average minibatch loss at step 3256: 1.245\n",
      "Average minibatch loss at step 3258: 1.244\n",
      "Average minibatch loss at step 3260: 1.274\n",
      "Average minibatch loss at step 3262: 1.289\n",
      "Average minibatch loss at step 3264: 1.284\n",
      "Average minibatch loss at step 3266: 1.273\n",
      "Average minibatch loss at step 3268: 1.239\n",
      "Average minibatch loss at step 3270: 1.242\n",
      "Average minibatch loss at step 3272: 1.235\n",
      "Average minibatch loss at step 3274: 1.221\n",
      "Average minibatch loss at step 3276: 1.236\n",
      "Average minibatch loss at step 3278: 1.269\n",
      "Average minibatch loss at step 3280: 1.273\n",
      "Average minibatch loss at step 3282: 1.248\n",
      "Average minibatch loss at step 3284: 1.229\n",
      "Average minibatch loss at step 3286: 1.230\n",
      "Average minibatch loss at step 3288: 1.220\n",
      "Average minibatch loss at step 3290: 1.201\n",
      "Average minibatch loss at step 3292: 1.193\n",
      "Average minibatch loss at step 3294: 1.190\n",
      "Average minibatch loss at step 3296: 1.188\n",
      "Average minibatch loss at step 3298: 1.188\n",
      "Average minibatch loss at step 3300: 1.188\n",
      "Average minibatch loss at step 3302: 1.188\n",
      "Average minibatch loss at step 3304: 1.195\n",
      "Average minibatch loss at step 3306: 1.218\n",
      "Average minibatch loss at step 3308: 1.234\n",
      "Average minibatch loss at step 3310: 1.247\n",
      "Average minibatch loss at step 3312: 1.236\n",
      "Average minibatch loss at step 3314: 1.298\n",
      "Average minibatch loss at step 3316: 1.231\n",
      "Average minibatch loss at step 3318: 1.195\n",
      "Average minibatch loss at step 3320: 1.204\n",
      "Average minibatch loss at step 3322: 1.214\n",
      "Average minibatch loss at step 3324: 1.208\n",
      "Average minibatch loss at step 3326: 1.227\n",
      "Average minibatch loss at step 3328: 1.205\n",
      "Average minibatch loss at step 3330: 1.170\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-10bb3c4fcc0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-eec74b4f0e58>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#nn.utils.clip_grad_norm(encoder.parameters(), 0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "epochs = 7000 #7000\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "val_df = pd.read_csv('datasets/val.csv')\n",
    "iteration = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch % 1500 == 0: #500, 275\n",
    "        learning_rate = learning_rate / 2 #2\n",
    "        # Filter parameters that do not require gradients\n",
    "        encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "        # Optimizers\n",
    "        encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "        print('')\n",
    "        print('learning rate: %f' % learning_rate)\n",
    "        print('')\n",
    "        \n",
    "    generator = BatchGenerator(batch_size, train_df[:64]) #64\n",
    "\n",
    "    while True:\n",
    "        try: \n",
    "            batch = generator.get_batch()\n",
    "        except StopIteration: break\n",
    "        loss = train_model(batch)\n",
    "        \n",
    "        if iteration % 2 == 0:\n",
    "            print('Average minibatch loss at step %d: %.3f' % (iteration, loss))\n",
    "            writer.add_scalar('train_loss', loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "        \n",
    "        \"\"\"if iteration % 8 == 0:    \n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            val_loss = validation_loss(val_df[:8]) # truncating validation dataframe\n",
    "            print('Validation loss: %.3f' % val_loss)\n",
    "            \n",
    "            writer.add_scalar('valid_loss', val_loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\"\"\"\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(encoder.state_dict(), 'encoder')\n",
    "torch.save(decoder.state_dict(), 'decoder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
