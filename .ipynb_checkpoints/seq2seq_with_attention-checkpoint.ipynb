{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "from fastText import load_model\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext embeddings trained on train and val sets\n",
    "# ./fasttext skipgram -input input_text_file -output output_model -dim 128 (fastText-0.1.0)\n",
    "fasttext_model = load_model('word_vectors/fasttext_model.bin')\n",
    "num_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab contains frequent words apperaing in the text along with their frequencies\n",
    "# minimum frequency = 6\n",
    "vocab_file = open('finished_files/vocab')\n",
    "# Store appearing words\n",
    "vocab_words = {}\n",
    "for line in vocab_file:\n",
    "    li = line.split()\n",
    "    if len(li) == 2:\n",
    "        word, freq = li\n",
    "        vocab_words[word] = freq\n",
    "# Final word to id dictionary    \n",
    "word2id = {}\n",
    "tokens = ['<pad>', '<unk>', '<eos>']\n",
    "for token in tokens:\n",
    "    word2id[token] = len(word2id)\n",
    "# Retrieve words from fasttext model and keep only those which are also present in 'vocab'\n",
    "fasttext_words = fasttext_model.get_words()\n",
    "for word in fasttext_words:\n",
    "    if word in vocab_words:\n",
    "        word2id[word] = len(word2id)        \n",
    "vocab_size = len(word2id)\n",
    "# Reverse dictionary\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "# Embeddings\n",
    "embeddings = np.zeros((vocab_size, num_dims))\n",
    "# <pad> token vector contains all zeros. Rest sampled from a normal distribution\n",
    "mu, sigma = 0, 0.05\n",
    "for i in range(1, len(tokens)):\n",
    "    embeddings[i] = np.random.normal(mu, sigma, num_dims)\n",
    "# Get word vectors from fasttext model and store in embeddings matrix\n",
    "for i in range(len(tokens), vocab_size):\n",
    "    embeddings[i] = fasttext_model.get_word_vector(id2word[i])\n",
    "    \n",
    "del fasttext_model, vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "for i in range(10000):\n",
    "    temp[i] = id2word[i]\n",
    "id2word = temp\n",
    "embeddings = embeddings[:10000]\n",
    "word2id = dict(zip(id2word.values(), id2word.keys()))\n",
    "\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_points = 500\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "words = [id2word[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_article_size = 50 #400\n",
    "max_abstract_size = 15 #100\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self):\n",
    "        self.abstract = (None, None)\n",
    "        self.article = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator: \n",
    "    \n",
    "    def __init__(self, batch_size, dataframe):\n",
    "        self.batch_size = batch_size\n",
    "        # train, valid, or test dataframe imported from csv\n",
    "        self.df = dataframe\n",
    "        self.generator = self.row_generator()\n",
    "        \n",
    "        \n",
    "    def row_generator(self):\n",
    "        for row in self.df.itertuples(index=False):\n",
    "            yield row\n",
    "            \n",
    "    def build_batch(self, rows):\n",
    "        # If number of rows less than batch size, get extra rows from the beginning of the dataframe\n",
    "        if len(rows) < self.batch_size:\n",
    "            temp_generator = self.row_generator()\n",
    "            for i in range(self.batch_size - len(rows)):\n",
    "                rows.append(self.get_row(temp_generator))\n",
    "                \n",
    "        # Get lengths of all the sequences in the batch upto max number of tokens\n",
    "        # + 1 is for the <eos> token\n",
    "        abstract_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.abstract.split()[:max_abstract_size]) for row in rows]) + 1\n",
    "        article_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.article.split()[:max_article_size]) for row in rows]) + 1 \n",
    "        abs_len = torch.max(abstract_lengths)\n",
    "        art_len = torch.max(article_lengths) \n",
    "        \n",
    "        # Variables containing abstracts and articles of the batch\n",
    "        abstracts = torch.cuda.LongTensor(abs_len, self.batch_size).fill_(0) # zero padding\n",
    "        articles = torch.cuda.LongTensor(art_len, self.batch_size).fill_(0) # zero padding\n",
    "        \n",
    "        # Sort rows in descending order of sequence (article) lengths\n",
    "        article_lengths, indices = torch.sort(article_lengths, descending=True)\n",
    "        rows = [rows[i] for i in indices]\n",
    "        abstract_lengths = torch.cuda.LongTensor([abstract_lengths[i] for i in indices])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Tokenize abstract and take max_abstract_size number of tokens\n",
    "            tokens = rows[i].abstract.split()[:max_abstract_size]\n",
    "            tokens.append('<eos>')\n",
    "            # Convert each token to word index\n",
    "            # <unk> token index for unknown words\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            # Store as column in abstracts variable with zero padding\n",
    "            abstracts[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "            # Same for articles\n",
    "            tokens = rows[i].article.split()[:max_article_size]\n",
    "            tokens.append('<eos>')\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            articles[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "        batch = Batch()\n",
    "        batch.article = (Variable(articles), article_lengths)\n",
    "        batch.abstract = (Variable(abstracts), abstract_lengths)\n",
    "        return batch\n",
    "            \n",
    "    def get_row(self, generator):\n",
    "        row = generator.__next__()\n",
    "        while not isinstance(row.article, str):\n",
    "            row = generator.__next__()\n",
    "        return row\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        rows = []\n",
    "        for b in range(self.batch_size):\n",
    "            try: rows.append(self.get_row(self.generator))\n",
    "            except StopIteration: break\n",
    "        if rows: return self.build_batch(rows)\n",
    "        else: raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embed): Embedding(100000, 128)\n",
       "  (lstm): LSTM(128, 512, bidirectional=True)\n",
       "  (linear_transform): Linear(in_features=1024, out_features=100000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Hidden states of the encoder\n",
    "        self.hidden_states = None\n",
    "        self.sequence_lengths = None\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm module\n",
    "        self.lstm = nn.LSTM(num_dims, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.lstm.cuda()\n",
    "        \n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(2 * hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, articles, article_lengths):\n",
    "        # Embedding lookup\n",
    "        input = self.embed(articles) # (T,B,N)\n",
    "        # batch is sorted in descending order of sequence lengths\n",
    "        packed_input = pack_padded_sequence(input, list(article_lengths))\n",
    "        packed_output, last_hidden = self.lstm(packed_input)\n",
    "        self.hidden_states, self.sequence_lengths = pad_packed_sequence(packed_output) # hidden_states (T,B,H*2)\n",
    "        \n",
    "        # Sum hidden states for all time steps for bidirectional lstm\n",
    "        #self.hidden_states = unpacked[:,:,:hidden_size] + unpacked[:,:,hidden_size:] for summing hidden states\n",
    "        \n",
    "        # Concatenate hidden and cell states of last time step for bidirectional lstm\n",
    "        h_n = torch.cat((last_hidden[0][0], last_hidden[0][1]), dim=1)\n",
    "        c_n = torch.cat((last_hidden[1][0], last_hidden[1][1]), dim=1)\n",
    "        \n",
    "        hiddenT = (h_n, c_n)\n",
    "        output = self.linear_transform(h_n)\n",
    "        \n",
    "        # Final hidden state\n",
    "        return hiddenT, output\n",
    "    \n",
    "encoder = Encoder(batch_size)\n",
    "encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embed): Embedding(100000, 128)\n",
       "  (lstm_cell): LSTMCell(128, 1024)\n",
       "  (linear_transform): Linear(in_features=1024, out_features=100000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "    \n",
    "        self.hidden = None\n",
    "        self.lstm_cell = nn.LSTMCell(num_dims, 2 * hidden_size).cuda()\n",
    "\n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(2 * hidden_size, vocab_size)\n",
    "        \n",
    "        #self.W_att = nn.Parameter()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def attention(h_t, batch_size):\n",
    "        score = score_function(h_t, batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def score_function(h_t, batch_size):\n",
    "        seq_len = encoder.hidden_states.shape[0]\n",
    "        score = torch.cuda.FloatTensor(seq_len, batch_size)\n",
    "        for t in seq_len:\n",
    "            score[t] = torch.diag(torch.mm(encoder.hidden_states[t], torch.transpose(h_t, 1, 0)))\n",
    "        return score # (T,B)\n",
    "    \n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is a LongTensor of size B\n",
    "        input = self.embed(input) #(B,N)\n",
    "\n",
    "        self.hidden = self.lstm_cell(input, self.hidden)\n",
    "        \n",
    "\n",
    "        # output has shape (B,V)\n",
    "        output = self.linear_transform(self.hidden[0])\n",
    "        return output\n",
    "    \n",
    "decoder = Decoder()\n",
    "decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, param in decoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 4.0 #3.0, 3.5\n",
    "\n",
    "# Filter parameters that do not require gradients\n",
    "encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "# Optimizers\n",
    "encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "# Loss function\n",
    "# Way to accumulate loss on sequences with variable lengths in batches :\n",
    "# size_average: By default, the losses are averaged over observations for each minibatch.\n",
    "# However, if the field size_average is set to False, the losses are instead summed for each minibatch. \n",
    "# Ignored if reduce is False.\n",
    "# Set size_average to False and divide the loss by the number of non-padding tokens.\n",
    "# ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient. \n",
    "# When size_average is True, the loss is averaged over non-ignored targets.\n",
    "# Set ignore_index to the padding value\n",
    "#loss_function = nn.CrossEntropyLoss(size_average=False, ignore_index=0).cuda() # 0 is the index of <pad>\n",
    "loss_function = nn.NLLLoss(size_average=False, ignore_index=0).cuda() # 0 is the index of <pad>\n",
    "\n",
    "def train_model(batch):\n",
    "    loss = 0\n",
    "    # Clear optimizer gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # articles, abstracts are LongTensor vairables of shape (max_sequence_length, B)\n",
    "    # containig word indices from the respective vocabs\n",
    "    # lengths are LongTensor varibles of shape batch_size containing\n",
    "    # lengths of all the sequences in the batch\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    hiddenT, output = encoder(articles, article_lengths)\n",
    "    # Initialize decoder hidden state\n",
    "    decoder.hidden = hiddenT\n",
    "    # First input to the decoder is the predicted word from the last state of encoder\n",
    "    output = softmax(output, batch_size)\n",
    "    _, input = torch.topk(output, 1, dim=1)\n",
    "    # Looping over all the sequences\n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input)\n",
    "        output = softmax(output, batch_size)\n",
    "        _, input = torch.topk(output, 1, dim=1)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "        \n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    loss.backward()\n",
    "    \n",
    "    #nn.utils.clip_grad_norm(encoder.parameters(), 0.5)\n",
    "    #nn.utils.clip_grad_norm(decoder.parameters(), 0.5)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    # Initialize hidden_list for next batch of inputs\n",
    "    decoder.hidden_list = []\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(df):\n",
    "    batch_size = 1\n",
    "    generator = BatchGenerator(batch_size, df)\n",
    "    loss = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch = generator.get_batch()\n",
    "            step += 1\n",
    "        except StopIteration: break\n",
    "        loss += calc_loss(batch, batch_size)\n",
    "    loss = loss/step\n",
    "    return loss\n",
    "\n",
    "def calc_loss(batch, batch_size):\n",
    "    loss = 0\n",
    "    encoder.hidden = encoder.init_hidden(batch_size, volatile=True)\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    \n",
    "    articles.volatile = True\n",
    "    abstracts.volatile = True\n",
    "        \n",
    "    hiddenT, output = encoder(articles, article_lengths) ###\n",
    "    for layer in range(hidden_layers):\n",
    "        decoder.hidden_list.append((hiddenT[0][layer], hiddenT[1][layer])) \n",
    "    #input = Variable(torch.cuda.LongTensor(batch_size).fill_(2), volatile=True)\n",
    "    input = most_likely(output, batch_size)\n",
    "    \n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input)\n",
    "        input = most_likely(output, batch_size)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    decoder.hidden_list = []\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(output, batch_size):\n",
    "    if batch_size > 1:\n",
    "        log_softmax = nn.LogSoftmax(dim=1)\n",
    "        output = log_softmax(output)\n",
    "    else: \n",
    "        log_softmax = nn.LogSoftmax(dim=0)\n",
    "        output = log_softmax(output)\n",
    "        _, next_input = torch.topk(output, 1)##\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate: 2.000000\n",
      "\n",
      "Average minibatch loss at step 2: 11.483\n",
      "Average minibatch loss at step 4: 11.416\n",
      "Average minibatch loss at step 6: 11.333\n",
      "Average minibatch loss at step 8: 11.196\n",
      "Average minibatch loss at step 10: 10.756\n",
      "Average minibatch loss at step 12: 11.005\n",
      "Average minibatch loss at step 14: 10.454\n",
      "Average minibatch loss at step 16: 10.442\n",
      "Average minibatch loss at step 18: 9.395\n",
      "Average minibatch loss at step 20: 9.630\n",
      "Average minibatch loss at step 22: 8.832\n",
      "Average minibatch loss at step 24: 8.473\n",
      "Average minibatch loss at step 26: 9.308\n",
      "Average minibatch loss at step 28: 10.397\n",
      "Average minibatch loss at step 30: 9.279\n",
      "Average minibatch loss at step 32: 8.844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-10bb3c4fcc0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-301997ce467c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "epochs = 7000 #7000\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "val_df = pd.read_csv('datasets/val.csv')\n",
    "iteration = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch % 1500 == 0: #500, 275\n",
    "        learning_rate = learning_rate / 2 #2\n",
    "        # Filter parameters that do not require gradients\n",
    "        encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "        # Optimizers\n",
    "        encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "        print('')\n",
    "        print('learning rate: %f' % learning_rate)\n",
    "        print('')\n",
    "        \n",
    "    generator = BatchGenerator(batch_size, train_df[:64]) #64\n",
    "\n",
    "    while True:\n",
    "        try: \n",
    "            batch = generator.get_batch()\n",
    "        except StopIteration: break\n",
    "        loss = train_model(batch)\n",
    "        \n",
    "        if iteration % 2 == 0:\n",
    "            print('Average minibatch loss at step %d: %.3f' % (iteration, loss))\n",
    "            writer.add_scalar('train_loss', loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "        \n",
    "        \"\"\"if iteration % 8 == 0:    \n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            val_loss = validation_loss(val_df[:8]) # truncating validation dataframe\n",
    "            print('Validation loss: %.3f' % val_loss)\n",
    "            \n",
    "            writer.add_scalar('valid_loss', val_loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\"\"\"\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(encoder.state_dict(), 'encoder')\n",
    "torch.save(decoder.state_dict(), 'decoder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
