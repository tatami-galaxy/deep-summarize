{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "from fastText import load_model\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext embeddings trained on train and val sets\n",
    "# ./fasttext skipgram -input input_text_file -output output_model -dim 128 (fastText-0.1.0)\n",
    "fasttext_model = load_model('word_vectors/fasttext_model.bin')\n",
    "num_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab contains frequent words apperaing in the text along with their frequencies\n",
    "# minimum frequency = 6\n",
    "vocab_file = open('finished_files/vocab')\n",
    "# Store appearing words\n",
    "vocab_words = {}\n",
    "for line in vocab_file:\n",
    "    li = line.split()\n",
    "    if len(li) == 2:\n",
    "        word, freq = li\n",
    "        vocab_words[word] = freq\n",
    "# Final word to id dictionary    \n",
    "word2id = {}\n",
    "tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for token in tokens:\n",
    "    word2id[token] = len(word2id)\n",
    "# Retrieve words from fasttext model and keep only those which are also present in 'vocab'\n",
    "fasttext_words = fasttext_model.get_words()\n",
    "for word in fasttext_words:\n",
    "    if word in vocab_words:\n",
    "        word2id[word] = len(word2id)        \n",
    "vocab_size = len(word2id)\n",
    "# Reverse dictionary\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "# Embeddings\n",
    "embeddings = np.zeros((vocab_size, num_dims))\n",
    "# <pad> token vector contains all zeros. Rest sampled from a normal distribution\n",
    "mu, sigma = 0, 0.05\n",
    "for i in range(1, len(tokens)):\n",
    "    embeddings[i] = np.random.normal(mu, sigma, num_dims)\n",
    "# Get word vectors from fasttext model and store in embeddings matrix\n",
    "for i in range(len(tokens), vocab_size):\n",
    "    embeddings[i] = fasttext_model.get_word_vector(id2word[i])\n",
    "    \n",
    "del fasttext_model, vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "for i in range(10000):\n",
    "    temp[i] = id2word[i]\n",
    "id2word = temp\n",
    "embeddings = embeddings[:10000]\n",
    "word2id = dict(zip(id2word.values(), id2word.keys()))\n",
    "\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_points = 500\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "words = [id2word[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_article_size = 50 #400\n",
    "max_abstract_size = 15 #100\n",
    "hidden_size = 512\n",
    "hidden_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self):\n",
    "        self.abstract = (None, None)\n",
    "        self.article = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator: \n",
    "    \n",
    "    def __init__(self, batch_size, dataframe):\n",
    "        self.batch_size = batch_size\n",
    "        # train, valid, or test dataframe imported from csv\n",
    "        self.df = dataframe\n",
    "        self.generator = self.row_generator()\n",
    "        \n",
    "        \n",
    "    def row_generator(self):\n",
    "        for row in self.df.itertuples(index=False):\n",
    "            yield row\n",
    "            \n",
    "    def build_batch(self, rows):\n",
    "        # If number of rows less than batch size, get extra rows from the beginning of the dataframe\n",
    "        if len(rows) < self.batch_size:\n",
    "            temp_generator = self.row_generator()\n",
    "            for i in range(self.batch_size - len(rows)):\n",
    "                rows.append(self.get_row(temp_generator))\n",
    "                \n",
    "        # Get lengths of all the sequences in the batch upto max number of tokens\n",
    "        # + 1 is for the <eos> token\n",
    "        abstract_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.abstract.split()[:max_abstract_size]) for row in rows]) + 1\n",
    "        article_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.article.split()[:max_article_size]) for row in rows]) + 1 \n",
    "        abs_len = torch.max(abstract_lengths)\n",
    "        art_len = torch.max(article_lengths) \n",
    "        \n",
    "        # Variables containing abstracts and articles of the batch\n",
    "        abstracts = torch.cuda.LongTensor(abs_len, self.batch_size).fill_(0) # zero padding\n",
    "        articles = torch.cuda.LongTensor(art_len, self.batch_size).fill_(0) # zero padding\n",
    "        \n",
    "        # Sort rows in descending order of sequence (article) lengths\n",
    "        article_lengths, indices = torch.sort(article_lengths, descending=True)\n",
    "        rows = [rows[i] for i in indices]\n",
    "        abstract_lengths = torch.cuda.LongTensor([abstract_lengths[i] for i in indices])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Tokenize abstract and take max_abstract_size number of tokens\n",
    "            tokens = rows[i].abstract.split()[:max_abstract_size]\n",
    "            tokens.append('<eos>')\n",
    "            # Convert each token to word index\n",
    "            # <unk> token index for unknown words\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            # Store as column in abstracts variable with zero padding\n",
    "            abstracts[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "            # Same for articles\n",
    "            tokens = rows[i].article.split()[:max_article_size]\n",
    "            tokens.append('<eos>')\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            articles[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "        batch = Batch()\n",
    "        batch.article = (Variable(articles), article_lengths)\n",
    "        batch.abstract = (Variable(abstracts), abstract_lengths)\n",
    "        return batch\n",
    "            \n",
    "    def get_row(self, generator):\n",
    "        row = generator.__next__()\n",
    "        while not isinstance(row.article, str):\n",
    "            row = generator.__next__()\n",
    "        return row\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        rows = []\n",
    "        for b in range(self.batch_size):\n",
    "            try: rows.append(self.get_row(self.generator))\n",
    "            except StopIteration: break\n",
    "        if rows: return self.build_batch(rows)\n",
    "        else: raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embed): Embedding(10000, 128)\n",
       "  (lstm): LSTM(128, 512, num_layers=2)\n",
       "  (linear_transform): Linear(in_features=512, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Hidden layer and cell state of model\n",
    "        # Initialize before calling model\n",
    "        self.hidden = None\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm module\n",
    "        self.lstm = nn.LSTM(num_dims, hidden_size, hidden_layers)\n",
    "        self.lstm.cuda()\n",
    "        \n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    # Funtion to initialize hidden layers\n",
    "    def init_hidden(self, batch_size, volatile=False):\n",
    "        tensor1 = torch.cuda.FloatTensor(hidden_layers, batch_size, hidden_size).fill_(0)\n",
    "        tensor2 = torch.cuda.FloatTensor(hidden_layers, batch_size, hidden_size).fill_(0)\n",
    "        return (Variable(tensor1, volatile=volatile), Variable(tensor2, volatile=volatile))\n",
    "    \n",
    "    def forward(self, articles, article_lengths):\n",
    "        # Embedding lookup\n",
    "        input = self.embed(articles)\n",
    "        # input to pack_padded_sequence can be of Txbx*\n",
    "        # where T is the length of longest sequence\n",
    "        # b is batch size\n",
    "        # batch is sorted in descending order of sequence lengths\n",
    "        #packed_input = pack_padded_sequence(input, list(article_lengths))\n",
    "        #packed_output, self.hidden = self.lstm(packed_input, self.hidden)\n",
    "        _, self.hidden = self.lstm(input, self.hidden)\n",
    "        \n",
    "        output = self.linear_transform(self.hidden[0][hidden_layers - 1])\n",
    "        \n",
    "        # Final hidden state\n",
    "        return self.hidden, output\n",
    "    \n",
    "encoder = Encoder(batch_size)\n",
    "encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embed): Embedding(10000, 128)\n",
       "  (cell_list): ModuleList(\n",
       "    (0): LSTMCell(128, 512)\n",
       "    (1): LSTMCell(512, 512)\n",
       "  )\n",
       "  (linear_transform): Linear(in_features=512, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "    \n",
    "        # Cell and hidden states\n",
    "        self.cell_list = []\n",
    "        self.hidden_list = []\n",
    "    \n",
    "        # First cell takes word embeddings as input\n",
    "        self.cell_list.append(nn.LSTMCell(num_dims, hidden_size).cuda())\n",
    "        for cell in range(1, hidden_layers):\n",
    "            self.cell_list.append(nn.LSTMCell(hidden_size, hidden_size).cuda())\n",
    "        # ModlueList Holds submodules in a list. \n",
    "        # ModuleList can be indexed like a regular Python list, \n",
    "        # but modules it contains are properly registered, \n",
    "        # and will be visible by all Module methods.\n",
    "        self.cell_list=nn.ModuleList(self.cell_list) \n",
    "        \n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is a LongTensor of size batch_size\n",
    "        input = self.embed(input) \n",
    "        # Each item in hidden list is a tuple of previous cell and hidden states\n",
    "        for layer in range(hidden_layers):\n",
    "            self.hidden_list[layer] = self.cell_list[layer](input, self.hidden_list[layer])\n",
    "            input = self.hidden_list[layer][0]\n",
    "        # output has shape (batch_size, vocab_size)\n",
    "        output = self.linear_transform(self.hidden_list[hidden_layers - 1][0])\n",
    "        return output\n",
    "    \n",
    "decoder = Decoder()\n",
    "decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, param in decoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.5 #2.0\n",
    "\n",
    "# Filter parameters that do not require gradients\n",
    "encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "# Optimizers\n",
    "encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "# Loss function\n",
    "# Way to accumulate loss on sequences with variable lengths in batches :\n",
    "# size_average: By default, the losses are averaged over observations for each minibatch.\n",
    "# However, if the field size_average is set to False, the losses are instead summed for each minibatch. \n",
    "# Ignored if reduce is False.\n",
    "# Set size_average to False and divide the loss by the number of non-padding tokens.\n",
    "# ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient. \n",
    "# When size_average is True, the loss is averaged over non-ignored targets.\n",
    "# Set ignore_index to the padding value\n",
    "loss_function = nn.CrossEntropyLoss(size_average=False, ignore_index=0).cuda() # 0 is the index of <pad>###\n",
    "\n",
    "def train_model(batch):\n",
    "    loss = 0\n",
    "    # Clear optimizer gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Clear hidden state of LSTM\n",
    "    encoder.hidden = encoder.init_hidden(batch_size)\n",
    "    # articles, abstracts are LongTensor vairables of shape (max_sequence_length, batch_size)\n",
    "    # containig word indices from the respective vocabs\n",
    "    # lengths are LongTensor varibles of shape batch_size containing\n",
    "    # lengths of all the sequences in the batch\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    hiddenT, output = encoder(articles, article_lengths)\n",
    "    \n",
    "    # Seperate hidden states corresponding to the the two layers of the encoder\n",
    "    # and append to hidden state list of decoder as tuples for each layer.\n",
    "    for layer in range(hidden_layers):\n",
    "        decoder.hidden_list.append((hiddenT[0][layer], hiddenT[1][layer]))\n",
    "    #input = Variable(torch.cuda.LongTensor(batch_size).fill_(2)) # 2 is the index of <sos>\n",
    "    input = most_likely(output, batch_size)\n",
    "\n",
    "    # Looping over all the sequences\n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input)\n",
    "        input = most_likely(output, batch_size)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "        \n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    loss.backward()\n",
    "    \n",
    "    #nn.utils.clip_grad_norm(encoder.parameters(), 0.5)\n",
    "    #nn.utils.clip_grad_norm(decoder.parameters(), 0.5)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    # Initialize hidden_list for next batch of inputs\n",
    "    decoder.hidden_list = []\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(df):\n",
    "    batch_size = 1\n",
    "    generator = BatchGenerator(batch_size, df)\n",
    "    loss = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch = generator.get_batch()\n",
    "            step += 1\n",
    "        except StopIteration: break\n",
    "        loss += calc_loss(batch, batch_size)\n",
    "    loss = loss/step\n",
    "    return loss\n",
    "\n",
    "def calc_loss(batch, batch_size):\n",
    "    loss = 0\n",
    "    encoder.hidden = encoder.init_hidden(batch_size, volatile=True)\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    \n",
    "    articles.volatile = True\n",
    "    abstracts.volatile = True\n",
    "        \n",
    "    hiddenT, output = encoder(articles, article_lengths) ###\n",
    "    for layer in range(hidden_layers):\n",
    "        decoder.hidden_list.append((hiddenT[0][layer], hiddenT[1][layer])) \n",
    "    #input = Variable(torch.cuda.LongTensor(batch_size).fill_(2), volatile=True)\n",
    "    input = most_likely(output, batch_size)\n",
    "    \n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input)\n",
    "        input = most_likely(output, batch_size)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    decoder.hidden_list = []\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely(output, batch_size):\n",
    "    if batch_size > 1:\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        output = softmax(output)\n",
    "        _, next_input = torch.topk(output, 1, dim=1)\n",
    "    else: \n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        output = softmax(output)\n",
    "        _, next_input = torch.topk(output, 1)\n",
    "    return next_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate: 0.750000\n",
      "\n",
      "Average minibatch loss at step 2: 9.200\n",
      "Average minibatch loss at step 4: 9.160\n",
      "Average minibatch loss at step 6: 9.117\n",
      "Average minibatch loss at step 8: 9.076\n",
      "Validation loss: 9.026\n",
      "Average minibatch loss at step 10: 9.034\n",
      "Average minibatch loss at step 12: 8.945\n",
      "Average minibatch loss at step 14: 8.850\n",
      "Average minibatch loss at step 16: 8.716\n",
      "Validation loss: 8.583\n",
      "Average minibatch loss at step 18: 8.489\n",
      "Average minibatch loss at step 20: 8.075\n",
      "Average minibatch loss at step 22: 7.761\n",
      "Average minibatch loss at step 24: 7.664\n",
      "Validation loss: 7.492\n",
      "Average minibatch loss at step 26: 7.619\n",
      "Average minibatch loss at step 28: 7.358\n",
      "Average minibatch loss at step 30: 7.514\n",
      "Average minibatch loss at step 32: 7.222\n",
      "Validation loss: 7.433\n",
      "Average minibatch loss at step 34: 7.185\n",
      "Average minibatch loss at step 36: 7.265\n",
      "Average minibatch loss at step 38: 7.090\n",
      "Average minibatch loss at step 40: 7.206\n",
      "Validation loss: 7.266\n",
      "Average minibatch loss at step 42: 7.195\n",
      "Average minibatch loss at step 44: 6.974\n",
      "Average minibatch loss at step 46: 7.381\n",
      "Average minibatch loss at step 48: 6.804\n",
      "Validation loss: 6.849\n",
      "Average minibatch loss at step 50: 6.921\n",
      "Average minibatch loss at step 52: 7.111\n",
      "Average minibatch loss at step 54: 7.115\n",
      "Average minibatch loss at step 56: 6.982\n",
      "Validation loss: 7.107\n",
      "Average minibatch loss at step 58: 7.012\n",
      "Average minibatch loss at step 60: 6.747\n",
      "Average minibatch loss at step 62: 7.308\n",
      "Average minibatch loss at step 64: 6.610\n",
      "Validation loss: 6.605\n",
      "Average minibatch loss at step 66: 6.913\n",
      "Average minibatch loss at step 68: 6.835\n",
      "Average minibatch loss at step 70: 6.803\n",
      "Average minibatch loss at step 72: 7.075\n",
      "Validation loss: 6.686\n",
      "Average minibatch loss at step 74: 6.835\n",
      "Average minibatch loss at step 76: 6.534\n",
      "Average minibatch loss at step 78: 6.930\n",
      "Average minibatch loss at step 80: 6.634\n",
      "Validation loss: 7.425\n",
      "Average minibatch loss at step 82: 7.033\n",
      "Average minibatch loss at step 84: 6.865\n",
      "Average minibatch loss at step 86: 6.923\n",
      "Average minibatch loss at step 88: 6.750\n",
      "Validation loss: 7.077\n",
      "Average minibatch loss at step 90: 6.934\n",
      "Average minibatch loss at step 92: 6.491\n",
      "Average minibatch loss at step 94: 7.096\n",
      "Average minibatch loss at step 96: 6.409\n",
      "Validation loss: 6.524\n",
      "Average minibatch loss at step 98: 6.904\n",
      "Average minibatch loss at step 100: 6.726\n",
      "Average minibatch loss at step 102: 6.586\n",
      "Average minibatch loss at step 104: 6.707\n",
      "Validation loss: 6.615\n",
      "Average minibatch loss at step 106: 6.727\n",
      "Average minibatch loss at step 108: 6.463\n",
      "Average minibatch loss at step 110: 6.391\n",
      "Average minibatch loss at step 112: 6.895\n",
      "Validation loss: 6.560\n",
      "Average minibatch loss at step 114: 6.501\n",
      "Average minibatch loss at step 116: 6.604\n",
      "Average minibatch loss at step 118: 6.680\n",
      "Average minibatch loss at step 120: 6.878\n",
      "Validation loss: 6.830\n",
      "Average minibatch loss at step 122: 6.702\n",
      "Average minibatch loss at step 124: 6.347\n",
      "Average minibatch loss at step 126: 6.284\n",
      "Average minibatch loss at step 128: 6.752\n",
      "Validation loss: 6.704\n",
      "Average minibatch loss at step 130: 6.411\n",
      "Average minibatch loss at step 132: 6.511\n",
      "Average minibatch loss at step 134: 6.589\n",
      "Average minibatch loss at step 136: 7.358\n",
      "Validation loss: 6.989\n",
      "Average minibatch loss at step 138: 6.898\n",
      "Average minibatch loss at step 140: 6.378\n",
      "Average minibatch loss at step 142: 6.249\n",
      "Average minibatch loss at step 144: 6.422\n",
      "Validation loss: 7.062\n",
      "Average minibatch loss at step 146: 6.723\n",
      "Average minibatch loss at step 148: 6.605\n",
      "Average minibatch loss at step 150: 6.460\n",
      "Average minibatch loss at step 152: 6.560\n",
      "Validation loss: 6.396\n",
      "Average minibatch loss at step 154: 6.556\n",
      "Average minibatch loss at step 156: 6.337\n",
      "Average minibatch loss at step 158: 6.704\n",
      "Average minibatch loss at step 160: 6.646\n",
      "Validation loss: 7.075\n",
      "Average minibatch loss at step 162: 6.783\n",
      "Average minibatch loss at step 164: 6.520\n",
      "Average minibatch loss at step 166: 6.381\n",
      "Average minibatch loss at step 168: 6.481\n",
      "Validation loss: 6.343\n",
      "Average minibatch loss at step 170: 6.452\n",
      "Average minibatch loss at step 172: 6.267\n",
      "Average minibatch loss at step 174: 6.758\n",
      "Average minibatch loss at step 176: 6.264\n",
      "Validation loss: 6.729\n",
      "Average minibatch loss at step 178: 6.431\n",
      "Average minibatch loss at step 180: 6.424\n",
      "Average minibatch loss at step 182: 6.348\n",
      "Average minibatch loss at step 184: 6.543\n",
      "Validation loss: 6.355\n",
      "Average minibatch loss at step 186: 6.504\n",
      "Average minibatch loss at step 188: 6.382\n",
      "Average minibatch loss at step 190: 6.763\n",
      "Average minibatch loss at step 192: 6.166\n",
      "Validation loss: 6.547\n",
      "Average minibatch loss at step 194: 6.296\n",
      "Average minibatch loss at step 196: 6.419\n",
      "Average minibatch loss at step 198: 6.295\n",
      "Average minibatch loss at step 200: 6.366\n",
      "Validation loss: 6.358\n",
      "Average minibatch loss at step 202: 6.759\n",
      "Average minibatch loss at step 204: 6.258\n",
      "Average minibatch loss at step 206: 6.557\n",
      "Average minibatch loss at step 208: 6.207\n",
      "Validation loss: 6.983\n",
      "Average minibatch loss at step 210: 6.457\n",
      "Average minibatch loss at step 212: 6.570\n",
      "Average minibatch loss at step 214: 6.226\n",
      "Average minibatch loss at step 216: 6.353\n",
      "Validation loss: 7.042\n",
      "Average minibatch loss at step 218: 6.476\n",
      "Average minibatch loss at step 220: 6.234\n",
      "Average minibatch loss at step 222: 6.573\n",
      "Average minibatch loss at step 224: 6.313\n",
      "Validation loss: 7.032\n",
      "Average minibatch loss at step 226: 6.575\n",
      "Average minibatch loss at step 228: 6.395\n",
      "Average minibatch loss at step 230: 6.365\n",
      "Average minibatch loss at step 232: 6.344\n",
      "Validation loss: 6.274\n",
      "Average minibatch loss at step 234: 6.366\n",
      "Average minibatch loss at step 236: 6.398\n",
      "Average minibatch loss at step 238: 6.614\n",
      "Average minibatch loss at step 240: 6.068\n",
      "Validation loss: 6.552\n",
      "Average minibatch loss at step 242: 6.203\n",
      "Average minibatch loss at step 244: 6.426\n",
      "Average minibatch loss at step 246: 6.176\n",
      "Average minibatch loss at step 248: 6.271\n",
      "Validation loss: 6.729\n",
      "Average minibatch loss at step 250: 6.318\n",
      "Average minibatch loss at step 252: 6.058\n",
      "Average minibatch loss at step 254: 6.226\n",
      "Average minibatch loss at step 256: 6.475\n",
      "Validation loss: 6.416\n",
      "Average minibatch loss at step 258: 6.197\n",
      "Average minibatch loss at step 260: 6.439\n",
      "Average minibatch loss at step 262: 6.216\n",
      "Average minibatch loss at step 264: 6.495\n",
      "Validation loss: 6.300\n",
      "Average minibatch loss at step 266: 6.377\n",
      "Average minibatch loss at step 268: 6.042\n",
      "Average minibatch loss at step 270: 6.077\n",
      "Average minibatch loss at step 272: 6.782\n",
      "Validation loss: 7.013\n",
      "Average minibatch loss at step 274: 6.380\n",
      "Average minibatch loss at step 276: 6.228\n",
      "Average minibatch loss at step 278: 6.120\n",
      "Average minibatch loss at step 280: 6.383\n",
      "Validation loss: 7.159\n",
      "Average minibatch loss at step 282: 6.437\n",
      "Average minibatch loss at step 284: 6.020\n",
      "Average minibatch loss at step 286: 5.986\n",
      "Average minibatch loss at step 288: 5.964\n",
      "Validation loss: 6.391\n",
      "Average minibatch loss at step 290: 6.711\n",
      "Average minibatch loss at step 292: 6.239\n",
      "Average minibatch loss at step 294: 6.194\n",
      "Average minibatch loss at step 296: 6.366\n",
      "Validation loss: 6.239\n",
      "Average minibatch loss at step 298: 6.249\n",
      "Average minibatch loss at step 300: 6.093\n",
      "Average minibatch loss at step 302: 6.589\n",
      "Average minibatch loss at step 304: 6.075\n",
      "Validation loss: 6.321\n",
      "Average minibatch loss at step 306: 6.059\n",
      "Average minibatch loss at step 308: 6.279\n",
      "Average minibatch loss at step 310: 6.562\n",
      "Average minibatch loss at step 312: 6.198\n",
      "Validation loss: 6.628\n",
      "Average minibatch loss at step 314: 6.172\n",
      "Average minibatch loss at step 316: 6.134\n",
      "Average minibatch loss at step 318: 6.192\n",
      "Average minibatch loss at step 320: 6.157\n",
      "Validation loss: 6.322\n",
      "Average minibatch loss at step 322: 6.646\n",
      "Average minibatch loss at step 324: 6.237\n",
      "Average minibatch loss at step 326: 6.208\n",
      "Average minibatch loss at step 328: 6.245\n",
      "Validation loss: 6.756\n",
      "Average minibatch loss at step 330: 6.261\n",
      "Average minibatch loss at step 332: 6.075\n",
      "Average minibatch loss at step 334: 6.326\n",
      "Average minibatch loss at step 336: 6.218\n",
      "Validation loss: 7.018\n",
      "Average minibatch loss at step 338: 6.325\n",
      "Average minibatch loss at step 340: 6.222\n",
      "Average minibatch loss at step 342: 6.351\n",
      "Average minibatch loss at step 344: 6.090\n",
      "Validation loss: 6.268\n",
      "Average minibatch loss at step 346: 6.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 348: 5.992\n",
      "Average minibatch loss at step 350: 5.889\n",
      "Average minibatch loss at step 352: 5.872\n",
      "Validation loss: 6.471\n",
      "Average minibatch loss at step 354: 6.667\n",
      "Average minibatch loss at step 356: 6.160\n",
      "Average minibatch loss at step 358: 6.040\n",
      "Average minibatch loss at step 360: 6.372\n",
      "Validation loss: 6.806\n",
      "Average minibatch loss at step 362: 6.151\n",
      "Average minibatch loss at step 364: 5.936\n",
      "Average minibatch loss at step 366: 6.163\n",
      "Average minibatch loss at step 368: 5.822\n",
      "Validation loss: 6.381\n",
      "Average minibatch loss at step 370: 6.119\n",
      "Average minibatch loss at step 372: 6.565\n",
      "Average minibatch loss at step 374: 6.258\n",
      "Average minibatch loss at step 376: 6.028\n",
      "Validation loss: 6.358\n",
      "Average minibatch loss at step 378: 6.105\n",
      "Average minibatch loss at step 380: 5.938\n",
      "Average minibatch loss at step 382: 6.640\n",
      "Average minibatch loss at step 384: 6.496\n",
      "Validation loss: 6.782\n",
      "Average minibatch loss at step 386: 6.200\n",
      "Average minibatch loss at step 388: 6.201\n",
      "Average minibatch loss at step 390: 6.123\n",
      "Average minibatch loss at step 392: 6.151\n",
      "Validation loss: 6.335\n",
      "Average minibatch loss at step 394: 6.126\n",
      "Average minibatch loss at step 396: 6.122\n",
      "Average minibatch loss at step 398: 5.818\n",
      "Average minibatch loss at step 400: 5.865\n",
      "Validation loss: 6.263\n",
      "Average minibatch loss at step 402: 6.484\n",
      "Average minibatch loss at step 404: 6.729\n",
      "Average minibatch loss at step 406: 6.139\n",
      "Average minibatch loss at step 408: 6.052\n",
      "Validation loss: 6.566\n",
      "Average minibatch loss at step 410: 6.077\n",
      "Average minibatch loss at step 412: 5.940\n",
      "Average minibatch loss at step 414: 5.839\n",
      "Average minibatch loss at step 416: 5.785\n",
      "Validation loss: 6.283\n",
      "Average minibatch loss at step 418: 6.617\n",
      "Average minibatch loss at step 420: 6.515\n",
      "Average minibatch loss at step 422: 5.984\n",
      "Average minibatch loss at step 424: 6.002\n",
      "Validation loss: 6.265\n",
      "Average minibatch loss at step 426: 6.229\n",
      "Average minibatch loss at step 428: 5.866\n",
      "Average minibatch loss at step 430: 5.974\n",
      "Average minibatch loss at step 432: 5.933\n",
      "Validation loss: 6.919\n",
      "Average minibatch loss at step 434: 6.245\n",
      "Average minibatch loss at step 436: 6.307\n",
      "Average minibatch loss at step 438: 6.093\n",
      "Average minibatch loss at step 440: 6.018\n",
      "Validation loss: 6.373\n",
      "Average minibatch loss at step 442: 6.146\n",
      "Average minibatch loss at step 444: 6.003\n",
      "Average minibatch loss at step 446: 5.779\n",
      "Average minibatch loss at step 448: 5.977\n",
      "Validation loss: 6.348\n",
      "Average minibatch loss at step 450: 6.579\n",
      "Average minibatch loss at step 452: 6.087\n",
      "Average minibatch loss at step 454: 5.948\n",
      "Average minibatch loss at step 456: 6.060\n",
      "Validation loss: 6.598\n",
      "Average minibatch loss at step 458: 6.247\n",
      "Average minibatch loss at step 460: 5.797\n",
      "Average minibatch loss at step 462: 5.818\n",
      "Average minibatch loss at step 464: 6.376\n",
      "Validation loss: 7.500\n",
      "Average minibatch loss at step 466: 6.564\n",
      "Average minibatch loss at step 468: 6.161\n",
      "Average minibatch loss at step 470: 5.989\n",
      "Average minibatch loss at step 472: 6.035\n",
      "Validation loss: 6.486\n",
      "Average minibatch loss at step 474: 6.053\n",
      "Average minibatch loss at step 476: 5.893\n",
      "Average minibatch loss at step 478: 5.790\n",
      "Average minibatch loss at step 480: 5.927\n",
      "Validation loss: 6.361\n",
      "Average minibatch loss at step 482: 6.178\n",
      "Average minibatch loss at step 484: 6.281\n",
      "Average minibatch loss at step 486: 5.971\n",
      "Average minibatch loss at step 488: 5.949\n",
      "Validation loss: 6.315\n",
      "Average minibatch loss at step 490: 6.036\n",
      "Average minibatch loss at step 492: 5.845\n",
      "Average minibatch loss at step 494: 6.231\n",
      "Average minibatch loss at step 496: 5.815\n",
      "Validation loss: 6.832\n",
      "Average minibatch loss at step 498: 5.994\n",
      "Average minibatch loss at step 500: 6.184\n",
      "Average minibatch loss at step 502: 5.946\n",
      "Average minibatch loss at step 504: 6.029\n",
      "Validation loss: 6.672\n",
      "Average minibatch loss at step 506: 6.027\n",
      "Average minibatch loss at step 508: 5.810\n",
      "Average minibatch loss at step 510: 5.790\n",
      "Average minibatch loss at step 512: 6.238\n",
      "Validation loss: 6.851\n",
      "Average minibatch loss at step 514: 5.952\n",
      "Average minibatch loss at step 516: 6.110\n",
      "Average minibatch loss at step 518: 5.927\n",
      "Average minibatch loss at step 520: 5.993\n",
      "Validation loss: 6.597\n",
      "Average minibatch loss at step 522: 6.021\n",
      "Average minibatch loss at step 524: 5.797\n",
      "Average minibatch loss at step 526: 5.749\n",
      "Average minibatch loss at step 528: 5.831\n",
      "Validation loss: 6.532\n",
      "Average minibatch loss at step 530: 6.134\n",
      "Average minibatch loss at step 532: 5.988\n",
      "Average minibatch loss at step 534: 5.875\n",
      "Average minibatch loss at step 536: 6.074\n",
      "Validation loss: 6.632\n",
      "Average minibatch loss at step 538: 6.231\n",
      "Average minibatch loss at step 540: 5.721\n",
      "Average minibatch loss at step 542: 5.802\n",
      "Average minibatch loss at step 544: 6.318\n",
      "Validation loss: 7.058\n",
      "Average minibatch loss at step 546: 5.990\n",
      "Average minibatch loss at step 548: 5.997\n",
      "Average minibatch loss at step 550: 5.882\n",
      "Average minibatch loss at step 552: 5.917\n",
      "Validation loss: 6.523\n",
      "Average minibatch loss at step 554: 6.113\n",
      "Average minibatch loss at step 556: 5.751\n",
      "Average minibatch loss at step 558: 5.705\n",
      "Average minibatch loss at step 560: 5.905\n",
      "Validation loss: 6.547\n",
      "Average minibatch loss at step 562: 5.997\n",
      "Average minibatch loss at step 564: 5.974\n",
      "Average minibatch loss at step 566: 5.985\n",
      "Average minibatch loss at step 568: 6.136\n",
      "Validation loss: 6.325\n",
      "Average minibatch loss at step 570: 5.892\n",
      "Average minibatch loss at step 572: 5.694\n",
      "Average minibatch loss at step 574: 5.665\n",
      "Average minibatch loss at step 576: 5.911\n",
      "Validation loss: 6.987\n",
      "Average minibatch loss at step 578: 6.466\n",
      "Average minibatch loss at step 580: 6.031\n",
      "Average minibatch loss at step 582: 5.870\n",
      "Average minibatch loss at step 584: 5.880\n",
      "Validation loss: 6.402\n",
      "Average minibatch loss at step 586: 5.952\n",
      "Average minibatch loss at step 588: 5.804\n",
      "Average minibatch loss at step 590: 5.857\n",
      "Average minibatch loss at step 592: 6.095\n",
      "Validation loss: 6.619\n",
      "Average minibatch loss at step 594: 5.973\n",
      "Average minibatch loss at step 596: 6.364\n",
      "Average minibatch loss at step 598: 5.852\n",
      "Average minibatch loss at step 600: 5.908\n",
      "Validation loss: 6.494\n",
      "Average minibatch loss at step 602: 6.029\n",
      "Average minibatch loss at step 604: 5.863\n",
      "Average minibatch loss at step 606: 5.638\n",
      "Average minibatch loss at step 608: 5.550\n",
      "Validation loss: 6.352\n",
      "Average minibatch loss at step 610: 6.336\n",
      "Average minibatch loss at step 612: 5.979\n",
      "Average minibatch loss at step 614: 5.830\n",
      "Average minibatch loss at step 616: 5.968\n",
      "Validation loss: 6.677\n",
      "Average minibatch loss at step 618: 5.921\n",
      "Average minibatch loss at step 620: 5.820\n",
      "Average minibatch loss at step 622: 5.956\n",
      "Average minibatch loss at step 624: 5.744\n",
      "Validation loss: 6.284\n",
      "Average minibatch loss at step 626: 5.978\n",
      "Average minibatch loss at step 628: 6.110\n",
      "Average minibatch loss at step 630: 6.053\n",
      "Average minibatch loss at step 632: 5.930\n",
      "Validation loss: 6.281\n",
      "Average minibatch loss at step 634: 6.099\n",
      "Average minibatch loss at step 636: 5.669\n",
      "Average minibatch loss at step 638: 5.694\n",
      "Average minibatch loss at step 640: 5.783\n",
      "Validation loss: 6.408\n",
      "Average minibatch loss at step 642: 6.141\n",
      "Average minibatch loss at step 644: 6.080\n",
      "Average minibatch loss at step 646: 5.802\n",
      "Average minibatch loss at step 648: 5.913\n",
      "Validation loss: 6.722\n",
      "Average minibatch loss at step 650: 5.881\n",
      "Average minibatch loss at step 652: 5.687\n",
      "Average minibatch loss at step 654: 5.986\n",
      "Average minibatch loss at step 656: 5.591\n",
      "Validation loss: 6.493\n",
      "Average minibatch loss at step 658: 6.064\n",
      "Average minibatch loss at step 660: 6.095\n",
      "Average minibatch loss at step 662: 5.857\n",
      "Average minibatch loss at step 664: 5.861\n",
      "Validation loss: 6.431\n",
      "Average minibatch loss at step 666: 5.900\n",
      "Average minibatch loss at step 668: 5.648\n",
      "Average minibatch loss at step 670: 5.769\n",
      "Average minibatch loss at step 672: 5.600\n",
      "Validation loss: 6.284\n",
      "Average minibatch loss at step 674: 5.804\n",
      "Average minibatch loss at step 676: 6.181\n",
      "Average minibatch loss at step 678: 6.394\n",
      "Average minibatch loss at step 680: 5.848\n",
      "Validation loss: 6.524\n",
      "Average minibatch loss at step 682: 5.889\n",
      "Average minibatch loss at step 684: 5.770\n",
      "Average minibatch loss at step 686: 5.954\n",
      "Average minibatch loss at step 688: 5.688\n",
      "Validation loss: 6.368\n",
      "Average minibatch loss at step 690: 6.136\n",
      "Average minibatch loss at step 692: 5.960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 694: 6.079\n",
      "Average minibatch loss at step 696: 5.840\n",
      "Validation loss: 6.608\n",
      "Average minibatch loss at step 698: 5.877\n",
      "Average minibatch loss at step 700: 5.704\n",
      "Average minibatch loss at step 702: 5.751\n",
      "Average minibatch loss at step 704: 5.685\n",
      "Validation loss: 6.365\n",
      "Average minibatch loss at step 706: 6.093\n",
      "Average minibatch loss at step 708: 5.970\n",
      "Average minibatch loss at step 710: 5.989\n",
      "Average minibatch loss at step 712: 5.804\n",
      "Validation loss: 6.397\n",
      "Average minibatch loss at step 714: 5.904\n",
      "Average minibatch loss at step 716: 6.086\n",
      "Average minibatch loss at step 718: 5.824\n",
      "Average minibatch loss at step 720: 5.505\n",
      "Validation loss: 6.302\n",
      "Average minibatch loss at step 722: 6.002\n",
      "Average minibatch loss at step 724: 6.381\n",
      "Average minibatch loss at step 726: 5.820\n",
      "Average minibatch loss at step 728: 5.844\n",
      "Validation loss: 6.486\n",
      "Average minibatch loss at step 730: 5.887\n",
      "Average minibatch loss at step 732: 5.613\n",
      "Average minibatch loss at step 734: 5.862\n",
      "Average minibatch loss at step 736: 6.052\n",
      "Validation loss: 6.638\n",
      "Average minibatch loss at step 738: 5.744\n",
      "Average minibatch loss at step 740: 6.074\n",
      "Average minibatch loss at step 742: 5.949\n",
      "Average minibatch loss at step 744: 5.793\n",
      "Validation loss: 6.348\n",
      "Average minibatch loss at step 746: 6.072\n",
      "Average minibatch loss at step 748: 5.743\n",
      "Average minibatch loss at step 750: 5.560\n",
      "Average minibatch loss at step 752: 5.544\n",
      "Validation loss: 6.291\n",
      "Average minibatch loss at step 754: 5.709\n",
      "Average minibatch loss at step 756: 6.450\n",
      "Average minibatch loss at step 758: 5.852\n",
      "Average minibatch loss at step 760: 5.780\n",
      "Validation loss: 6.339\n",
      "Average minibatch loss at step 762: 5.861\n",
      "Average minibatch loss at step 764: 5.930\n",
      "Average minibatch loss at step 766: 5.547\n",
      "Average minibatch loss at step 768: 5.481\n",
      "Validation loss: 6.407\n",
      "Average minibatch loss at step 770: 5.787\n",
      "Average minibatch loss at step 772: 6.152\n",
      "Average minibatch loss at step 774: 5.794\n",
      "Average minibatch loss at step 776: 5.900\n",
      "Validation loss: 6.942\n",
      "Average minibatch loss at step 778: 5.894\n",
      "Average minibatch loss at step 780: 5.657\n",
      "Average minibatch loss at step 782: 5.621\n",
      "Average minibatch loss at step 784: 5.617\n",
      "Validation loss: 6.444\n",
      "Average minibatch loss at step 786: 5.676\n",
      "Average minibatch loss at step 788: 5.895\n",
      "Average minibatch loss at step 790: 6.206\n",
      "Average minibatch loss at step 792: 5.746\n",
      "Validation loss: 6.457\n",
      "Average minibatch loss at step 794: 5.852\n",
      "Average minibatch loss at step 796: 6.028\n",
      "Average minibatch loss at step 798: 5.613\n",
      "Average minibatch loss at step 800: 5.581\n",
      "Validation loss: 6.948\n",
      "Average minibatch loss at step 802: 5.877\n",
      "Average minibatch loss at step 804: 5.999\n",
      "Average minibatch loss at step 806: 5.811\n",
      "Average minibatch loss at step 808: 5.896\n",
      "Validation loss: 6.509\n",
      "Average minibatch loss at step 810: 6.157\n",
      "Average minibatch loss at step 812: 5.587\n",
      "Average minibatch loss at step 814: 5.694\n",
      "Average minibatch loss at step 816: 5.483\n",
      "Validation loss: 6.550\n",
      "Average minibatch loss at step 818: 5.760\n",
      "Average minibatch loss at step 820: 5.965\n",
      "Average minibatch loss at step 822: 5.711\n",
      "Average minibatch loss at step 824: 5.764\n",
      "Validation loss: 6.592\n",
      "Average minibatch loss at step 826: 5.854\n",
      "Average minibatch loss at step 828: 5.645\n",
      "Average minibatch loss at step 830: 5.727\n",
      "Average minibatch loss at step 832: 5.537\n",
      "Validation loss: 6.723\n",
      "Average minibatch loss at step 834: 5.658\n",
      "Average minibatch loss at step 836: 5.913\n",
      "Average minibatch loss at step 838: 5.800\n",
      "Average minibatch loss at step 840: 5.760\n",
      "Validation loss: 6.639\n",
      "Average minibatch loss at step 842: 5.805\n",
      "Average minibatch loss at step 844: 5.553\n",
      "Average minibatch loss at step 846: 5.592\n",
      "Average minibatch loss at step 848: 5.736\n",
      "Validation loss: 6.992\n",
      "Average minibatch loss at step 850: 5.763\n",
      "Average minibatch loss at step 852: 5.805\n",
      "Average minibatch loss at step 854: 5.704\n",
      "Average minibatch loss at step 856: 5.747\n",
      "Validation loss: 6.596\n",
      "Average minibatch loss at step 858: 5.750\n",
      "Average minibatch loss at step 860: 5.700\n",
      "Average minibatch loss at step 862: 5.919\n",
      "Average minibatch loss at step 864: 5.727\n",
      "Validation loss: 6.432\n",
      "Average minibatch loss at step 866: 5.697\n",
      "Average minibatch loss at step 868: 6.124\n",
      "Average minibatch loss at step 870: 5.717\n",
      "Average minibatch loss at step 872: 5.766\n",
      "Validation loss: 6.689\n",
      "Average minibatch loss at step 874: 5.792\n",
      "Average minibatch loss at step 876: 5.682\n",
      "Average minibatch loss at step 878: 5.604\n",
      "Average minibatch loss at step 880: 5.504\n",
      "Validation loss: 6.525\n",
      "Average minibatch loss at step 882: 5.757\n",
      "Average minibatch loss at step 884: 6.282\n",
      "Average minibatch loss at step 886: 5.723\n",
      "Average minibatch loss at step 888: 5.727\n",
      "Validation loss: 6.592\n",
      "Average minibatch loss at step 890: 5.796\n",
      "Average minibatch loss at step 892: 5.582\n",
      "Average minibatch loss at step 894: 5.825\n",
      "Average minibatch loss at step 896: 5.669\n",
      "Validation loss: 7.000\n",
      "Average minibatch loss at step 898: 5.714\n",
      "Average minibatch loss at step 900: 6.113\n",
      "Average minibatch loss at step 902: 5.696\n",
      "Average minibatch loss at step 904: 5.728\n",
      "Validation loss: 6.641\n",
      "Average minibatch loss at step 906: 5.758\n",
      "Average minibatch loss at step 908: 5.803\n",
      "Average minibatch loss at step 910: 5.482\n",
      "Average minibatch loss at step 912: 5.448\n",
      "Validation loss: 6.903\n",
      "Average minibatch loss at step 914: 5.676\n",
      "Average minibatch loss at step 916: 6.059\n",
      "Average minibatch loss at step 918: 5.671\n",
      "Average minibatch loss at step 920: 5.692\n",
      "Validation loss: 6.620\n",
      "Average minibatch loss at step 922: 5.847\n",
      "Average minibatch loss at step 924: 5.776\n",
      "Average minibatch loss at step 926: 5.515\n",
      "Average minibatch loss at step 928: 5.631\n",
      "Validation loss: 7.371\n",
      "Average minibatch loss at step 930: 5.997\n",
      "Average minibatch loss at step 932: 5.896\n",
      "Average minibatch loss at step 934: 5.673\n",
      "Average minibatch loss at step 936: 5.683\n",
      "Validation loss: 6.588\n",
      "Average minibatch loss at step 938: 5.730\n",
      "Average minibatch loss at step 940: 5.616\n",
      "Average minibatch loss at step 942: 5.494\n",
      "Average minibatch loss at step 944: 5.575\n",
      "Validation loss: 6.545\n",
      "Average minibatch loss at step 946: 5.611\n",
      "Average minibatch loss at step 948: 5.928\n",
      "Average minibatch loss at step 950: 5.992\n",
      "Average minibatch loss at step 952: 5.674\n",
      "Validation loss: 6.460\n",
      "Average minibatch loss at step 954: 6.054\n",
      "Average minibatch loss at step 956: 5.540\n",
      "Average minibatch loss at step 958: 5.684\n",
      "Average minibatch loss at step 960: 5.790\n",
      "Validation loss: 7.318\n",
      "Average minibatch loss at step 962: 6.011\n",
      "Average minibatch loss at step 964: 5.950\n",
      "Average minibatch loss at step 966: 5.716\n",
      "Average minibatch loss at step 968: 5.715\n",
      "Validation loss: 6.649\n",
      "Average minibatch loss at step 970: 5.729\n",
      "Average minibatch loss at step 972: 5.587\n",
      "Average minibatch loss at step 974: 5.492\n",
      "Average minibatch loss at step 976: 5.651\n",
      "Validation loss: 7.087\n",
      "Average minibatch loss at step 978: 5.819\n",
      "Average minibatch loss at step 980: 5.922\n",
      "Average minibatch loss at step 982: 5.770\n",
      "Average minibatch loss at step 984: 5.642\n",
      "Validation loss: 6.404\n",
      "Average minibatch loss at step 986: 5.701\n",
      "Average minibatch loss at step 988: 5.491\n",
      "Average minibatch loss at step 990: 5.420\n",
      "Average minibatch loss at step 992: 5.420\n",
      "Validation loss: 6.382\n",
      "Average minibatch loss at step 994: 5.708\n",
      "Average minibatch loss at step 996: 6.317\n",
      "Average minibatch loss at step 998: 5.652\n",
      "Average minibatch loss at step 1000: 5.648\n",
      "Validation loss: 6.591\n",
      "Average minibatch loss at step 1002: 5.694\n",
      "Average minibatch loss at step 1004: 5.538\n",
      "Average minibatch loss at step 1006: 5.606\n",
      "Average minibatch loss at step 1008: 5.514\n",
      "Validation loss: 6.719\n",
      "Average minibatch loss at step 1010: 5.604\n",
      "Average minibatch loss at step 1012: 5.712\n",
      "Average minibatch loss at step 1014: 5.611\n",
      "Average minibatch loss at step 1016: 5.688\n",
      "Validation loss: 6.536\n",
      "Average minibatch loss at step 1018: 6.088\n",
      "Average minibatch loss at step 1020: 5.533\n",
      "Average minibatch loss at step 1022: 5.620\n",
      "Average minibatch loss at step 1024: 5.639\n",
      "Validation loss: 6.886\n",
      "Average minibatch loss at step 1026: 5.689\n",
      "Average minibatch loss at step 1028: 5.873\n",
      "Average minibatch loss at step 1030: 5.696\n",
      "Average minibatch loss at step 1032: 5.694\n",
      "Validation loss: 6.551\n",
      "Average minibatch loss at step 1034: 5.705\n",
      "Average minibatch loss at step 1036: 5.500\n",
      "Average minibatch loss at step 1038: 5.440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1040: 5.422\n",
      "Validation loss: 7.056\n",
      "Average minibatch loss at step 1042: 5.702\n",
      "Average minibatch loss at step 1044: 6.046\n",
      "Average minibatch loss at step 1046: 5.652\n",
      "Average minibatch loss at step 1048: 5.627\n",
      "Validation loss: 6.598\n",
      "Average minibatch loss at step 1050: 5.656\n",
      "Average minibatch loss at step 1052: 5.539\n",
      "Average minibatch loss at step 1054: 5.517\n",
      "Average minibatch loss at step 1056: 5.535\n",
      "Validation loss: 6.924\n",
      "Average minibatch loss at step 1058: 5.595\n",
      "Average minibatch loss at step 1060: 5.783\n",
      "Average minibatch loss at step 1062: 5.592\n",
      "Average minibatch loss at step 1064: 5.598\n",
      "Validation loss: 6.614\n",
      "Average minibatch loss at step 1066: 5.677\n",
      "Average minibatch loss at step 1068: 5.583\n",
      "Average minibatch loss at step 1070: 5.601\n",
      "Average minibatch loss at step 1072: 5.404\n",
      "Validation loss: 6.478\n",
      "Average minibatch loss at step 1074: 5.606\n",
      "Average minibatch loss at step 1076: 5.788\n",
      "Average minibatch loss at step 1078: 5.596\n",
      "Average minibatch loss at step 1080: 5.702\n",
      "Validation loss: 6.585\n",
      "Average minibatch loss at step 1082: 5.943\n",
      "Average minibatch loss at step 1084: 5.435\n",
      "Average minibatch loss at step 1086: 5.451\n",
      "Average minibatch loss at step 1088: 5.447\n",
      "Validation loss: 7.013\n",
      "Average minibatch loss at step 1090: 5.612\n",
      "Average minibatch loss at step 1092: 5.689\n",
      "Average minibatch loss at step 1094: 5.615\n",
      "Average minibatch loss at step 1096: 5.583\n",
      "Validation loss: 6.591\n",
      "Average minibatch loss at step 1098: 5.742\n",
      "Average minibatch loss at step 1100: 5.749\n",
      "Average minibatch loss at step 1102: 5.509\n",
      "Average minibatch loss at step 1104: 5.352\n",
      "Validation loss: 6.426\n",
      "Average minibatch loss at step 1106: 5.664\n",
      "Average minibatch loss at step 1108: 5.831\n",
      "Average minibatch loss at step 1110: 5.680\n",
      "Average minibatch loss at step 1112: 5.657\n",
      "Validation loss: 6.750\n",
      "Average minibatch loss at step 1114: 5.626\n",
      "Average minibatch loss at step 1116: 5.445\n",
      "Average minibatch loss at step 1118: 5.414\n",
      "Average minibatch loss at step 1120: 5.520\n",
      "Validation loss: 6.735\n",
      "Average minibatch loss at step 1122: 5.700\n",
      "Average minibatch loss at step 1124: 5.720\n",
      "Average minibatch loss at step 1126: 5.571\n",
      "Average minibatch loss at step 1128: 5.577\n",
      "Validation loss: 6.570\n",
      "Average minibatch loss at step 1130: 5.668\n",
      "Average minibatch loss at step 1132: 5.394\n",
      "Average minibatch loss at step 1134: 5.475\n",
      "Average minibatch loss at step 1136: 5.870\n",
      "Validation loss: 7.111\n",
      "Average minibatch loss at step 1138: 5.629\n",
      "Average minibatch loss at step 1140: 5.715\n",
      "Average minibatch loss at step 1142: 5.682\n",
      "Average minibatch loss at step 1144: 5.604\n",
      "Validation loss: 6.701\n",
      "Average minibatch loss at step 1146: 5.598\n",
      "Average minibatch loss at step 1148: 5.477\n",
      "Average minibatch loss at step 1150: 5.371\n",
      "Average minibatch loss at step 1152: 5.523\n",
      "Validation loss: 6.698\n",
      "Average minibatch loss at step 1154: 5.781\n",
      "Average minibatch loss at step 1156: 5.873\n",
      "Average minibatch loss at step 1158: 5.565\n",
      "Average minibatch loss at step 1160: 5.579\n",
      "Validation loss: 6.614\n",
      "Average minibatch loss at step 1162: 5.762\n",
      "Average minibatch loss at step 1164: 5.408\n",
      "Average minibatch loss at step 1166: 5.425\n",
      "Average minibatch loss at step 1168: 5.486\n",
      "Validation loss: 7.041\n",
      "Average minibatch loss at step 1170: 5.641\n",
      "Average minibatch loss at step 1172: 5.845\n",
      "Average minibatch loss at step 1174: 5.636\n",
      "Average minibatch loss at step 1176: 5.572\n",
      "Validation loss: 6.542\n",
      "Average minibatch loss at step 1178: 5.582\n",
      "Average minibatch loss at step 1180: 5.377\n",
      "Average minibatch loss at step 1182: 5.351\n",
      "Average minibatch loss at step 1184: 5.299\n",
      "Validation loss: 6.581\n",
      "Average minibatch loss at step 1186: 5.654\n",
      "Average minibatch loss at step 1188: 5.990\n",
      "Average minibatch loss at step 1190: 5.534\n",
      "Average minibatch loss at step 1192: 5.532\n",
      "Validation loss: 6.638\n",
      "Average minibatch loss at step 1194: 5.763\n",
      "Average minibatch loss at step 1196: 5.438\n",
      "Average minibatch loss at step 1198: 5.350\n",
      "Average minibatch loss at step 1200: 5.260\n",
      "Validation loss: 6.758\n",
      "Average minibatch loss at step 1202: 5.777\n",
      "Average minibatch loss at step 1204: 5.878\n",
      "Average minibatch loss at step 1206: 5.559\n",
      "Average minibatch loss at step 1208: 5.528\n",
      "Validation loss: 6.671\n",
      "Average minibatch loss at step 1210: 5.632\n",
      "Average minibatch loss at step 1212: 5.410\n",
      "Average minibatch loss at step 1214: 5.412\n",
      "Average minibatch loss at step 1216: 5.450\n",
      "Validation loss: 7.229\n",
      "Average minibatch loss at step 1218: 5.872\n",
      "Average minibatch loss at step 1220: 5.744\n",
      "Average minibatch loss at step 1222: 5.514\n",
      "Average minibatch loss at step 1224: 5.516\n",
      "Validation loss: 6.595\n",
      "Average minibatch loss at step 1226: 5.552\n",
      "Average minibatch loss at step 1228: 5.371\n",
      "Average minibatch loss at step 1230: 5.351\n",
      "Average minibatch loss at step 1232: 5.410\n",
      "Validation loss: 6.522\n",
      "Average minibatch loss at step 1234: 5.489\n",
      "Average minibatch loss at step 1236: 5.682\n",
      "Average minibatch loss at step 1238: 5.507\n",
      "Average minibatch loss at step 1240: 5.544\n",
      "Validation loss: 6.688\n",
      "Average minibatch loss at step 1242: 5.595\n",
      "Average minibatch loss at step 1244: 5.341\n",
      "Average minibatch loss at step 1246: 5.308\n",
      "Average minibatch loss at step 1248: 5.346\n",
      "Validation loss: 6.562\n",
      "Average minibatch loss at step 1250: 5.511\n",
      "Average minibatch loss at step 1252: 5.790\n",
      "Average minibatch loss at step 1254: 5.490\n",
      "Average minibatch loss at step 1256: 5.485\n",
      "Validation loss: 6.681\n",
      "Average minibatch loss at step 1258: 5.527\n",
      "Average minibatch loss at step 1260: 5.371\n",
      "Average minibatch loss at step 1262: 5.442\n",
      "Average minibatch loss at step 1264: 5.605\n",
      "Validation loss: 6.906\n",
      "Average minibatch loss at step 1266: 5.488\n",
      "Average minibatch loss at step 1268: 5.646\n",
      "Average minibatch loss at step 1270: 5.499\n",
      "Average minibatch loss at step 1272: 5.501\n",
      "Validation loss: 6.599\n",
      "Average minibatch loss at step 1274: 5.684\n",
      "Average minibatch loss at step 1276: 5.359\n",
      "Average minibatch loss at step 1278: 5.361\n",
      "Average minibatch loss at step 1280: 5.499\n",
      "Validation loss: 7.388\n",
      "Average minibatch loss at step 1282: 5.662\n",
      "Average minibatch loss at step 1284: 5.609\n",
      "Average minibatch loss at step 1286: 5.486\n",
      "Average minibatch loss at step 1288: 5.463\n",
      "Validation loss: 6.706\n",
      "Average minibatch loss at step 1290: 5.528\n",
      "Average minibatch loss at step 1292: 5.373\n",
      "Average minibatch loss at step 1294: 5.322\n",
      "Average minibatch loss at step 1296: 5.202\n",
      "Validation loss: 6.606\n",
      "Average minibatch loss at step 1298: 5.501\n",
      "Average minibatch loss at step 1300: 5.841\n",
      "Average minibatch loss at step 1302: 5.462\n",
      "Average minibatch loss at step 1304: 5.456\n",
      "Validation loss: 6.580\n",
      "Average minibatch loss at step 1306: 5.516\n",
      "Average minibatch loss at step 1308: 5.360\n",
      "Average minibatch loss at step 1310: 5.300\n",
      "Average minibatch loss at step 1312: 5.363\n",
      "Validation loss: 7.184\n",
      "Average minibatch loss at step 1314: 5.530\n",
      "Average minibatch loss at step 1316: 5.610\n",
      "Average minibatch loss at step 1318: 5.493\n",
      "Average minibatch loss at step 1320: 5.480\n",
      "Validation loss: 6.698\n",
      "Average minibatch loss at step 1322: 5.549\n",
      "Average minibatch loss at step 1324: 5.316\n",
      "Average minibatch loss at step 1326: 5.514\n",
      "Average minibatch loss at step 1328: 5.233\n",
      "Validation loss: 6.806\n",
      "Average minibatch loss at step 1330: 5.551\n",
      "Average minibatch loss at step 1332: 5.564\n",
      "Average minibatch loss at step 1334: 5.471\n",
      "Average minibatch loss at step 1336: 5.504\n",
      "Validation loss: 6.570\n",
      "Average minibatch loss at step 1338: 5.491\n",
      "Average minibatch loss at step 1340: 5.301\n",
      "Average minibatch loss at step 1342: 5.259\n",
      "Average minibatch loss at step 1344: 5.274\n",
      "Validation loss: 6.917\n",
      "Average minibatch loss at step 1346: 5.441\n",
      "Average minibatch loss at step 1348: 5.744\n",
      "Average minibatch loss at step 1350: 5.482\n",
      "Average minibatch loss at step 1352: 5.488\n",
      "Validation loss: 6.748\n",
      "Average minibatch loss at step 1354: 5.497\n",
      "Average minibatch loss at step 1356: 5.329\n",
      "Average minibatch loss at step 1358: 5.428\n",
      "Average minibatch loss at step 1360: 5.492\n",
      "Validation loss: 7.307\n",
      "Average minibatch loss at step 1362: 5.532\n",
      "Average minibatch loss at step 1364: 5.618\n",
      "Average minibatch loss at step 1366: 5.441\n",
      "Average minibatch loss at step 1368: 5.437\n",
      "Validation loss: 6.547\n",
      "Average minibatch loss at step 1370: 5.525\n",
      "Average minibatch loss at step 1372: 5.310\n",
      "Average minibatch loss at step 1374: 5.271\n",
      "Average minibatch loss at step 1376: 5.198\n",
      "Validation loss: 6.808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1378: 5.394\n",
      "Average minibatch loss at step 1380: 5.576\n",
      "Average minibatch loss at step 1382: 5.437\n",
      "Average minibatch loss at step 1384: 5.420\n",
      "Validation loss: 6.646\n",
      "Average minibatch loss at step 1386: 5.498\n",
      "Average minibatch loss at step 1388: 5.326\n",
      "Average minibatch loss at step 1390: 5.300\n",
      "Average minibatch loss at step 1392: 5.189\n",
      "Validation loss: 6.800\n",
      "Average minibatch loss at step 1394: 5.376\n",
      "Average minibatch loss at step 1396: 5.551\n",
      "Average minibatch loss at step 1398: 5.411\n",
      "Average minibatch loss at step 1400: 5.501\n",
      "Validation loss: 6.786\n",
      "Average minibatch loss at step 1402: 5.480\n",
      "Average minibatch loss at step 1404: 5.270\n",
      "Average minibatch loss at step 1406: 5.293\n",
      "Average minibatch loss at step 1408: 5.481\n",
      "Validation loss: 7.002\n",
      "Average minibatch loss at step 1410: 5.429\n",
      "Average minibatch loss at step 1412: 5.547\n",
      "Average minibatch loss at step 1414: 5.413\n",
      "Average minibatch loss at step 1416: 5.432\n",
      "Validation loss: 6.722\n",
      "Average minibatch loss at step 1418: 5.512\n",
      "Average minibatch loss at step 1420: 5.307\n",
      "Average minibatch loss at step 1422: 5.254\n",
      "Average minibatch loss at step 1424: 5.158\n",
      "Validation loss: 6.789\n",
      "Average minibatch loss at step 1426: 5.367\n",
      "Average minibatch loss at step 1428: 5.522\n",
      "Average minibatch loss at step 1430: 5.392\n",
      "Average minibatch loss at step 1432: 5.438\n",
      "Validation loss: 6.771\n",
      "Average minibatch loss at step 1434: 5.524\n",
      "Average minibatch loss at step 1436: 5.294\n",
      "Average minibatch loss at step 1438: 5.417\n",
      "Average minibatch loss at step 1440: 5.450\n",
      "Validation loss: 7.727\n",
      "Average minibatch loss at step 1442: 5.752\n",
      "Average minibatch loss at step 1444: 5.519\n",
      "Average minibatch loss at step 1446: 5.505\n",
      "Average minibatch loss at step 1448: 5.495\n",
      "Validation loss: 6.751\n",
      "Average minibatch loss at step 1450: 5.468\n",
      "Average minibatch loss at step 1452: 5.305\n",
      "Average minibatch loss at step 1454: 5.244\n",
      "Average minibatch loss at step 1456: 5.213\n",
      "Validation loss: 7.126\n",
      "Average minibatch loss at step 1458: 5.364\n",
      "Average minibatch loss at step 1460: 5.520\n",
      "Average minibatch loss at step 1462: 5.418\n",
      "Average minibatch loss at step 1464: 5.428\n",
      "Validation loss: 6.828\n",
      "Average minibatch loss at step 1466: 5.469\n",
      "Average minibatch loss at step 1468: 5.390\n",
      "Average minibatch loss at step 1470: 5.263\n",
      "Average minibatch loss at step 1472: 5.158\n",
      "Validation loss: 6.685\n",
      "Average minibatch loss at step 1474: 5.445\n",
      "Average minibatch loss at step 1476: 5.779\n",
      "Average minibatch loss at step 1478: 5.409\n",
      "Average minibatch loss at step 1480: 5.416\n",
      "Validation loss: 6.790\n",
      "Average minibatch loss at step 1482: 5.480\n",
      "Average minibatch loss at step 1484: 5.262\n",
      "Average minibatch loss at step 1486: 5.221\n",
      "Average minibatch loss at step 1488: 5.262\n",
      "Validation loss: 7.192\n",
      "Average minibatch loss at step 1490: 5.449\n",
      "Average minibatch loss at step 1492: 5.583\n",
      "Average minibatch loss at step 1494: 5.477\n",
      "Average minibatch loss at step 1496: 5.439\n",
      "Validation loss: 6.757\n",
      "Average minibatch loss at step 1498: 5.454\n",
      "Average minibatch loss at step 1500: 5.265\n",
      "Average minibatch loss at step 1502: 5.397\n",
      "Average minibatch loss at step 1504: 5.319\n",
      "Validation loss: 7.326\n",
      "Average minibatch loss at step 1506: 5.567\n",
      "Average minibatch loss at step 1508: 5.634\n",
      "Average minibatch loss at step 1510: 5.389\n",
      "Average minibatch loss at step 1512: 5.394\n",
      "Validation loss: 6.830\n",
      "Average minibatch loss at step 1514: 5.435\n",
      "Average minibatch loss at step 1516: 5.315\n",
      "Average minibatch loss at step 1518: 5.253\n",
      "Average minibatch loss at step 1520: 5.287\n",
      "Validation loss: 7.189\n",
      "Average minibatch loss at step 1522: 5.510\n",
      "Average minibatch loss at step 1524: 5.675\n",
      "Average minibatch loss at step 1526: 5.401\n",
      "Average minibatch loss at step 1528: 5.384\n",
      "Validation loss: 6.715\n",
      "Average minibatch loss at step 1530: 5.543\n",
      "Average minibatch loss at step 1532: 5.339\n",
      "Average minibatch loss at step 1534: 5.239\n",
      "Average minibatch loss at step 1536: 5.144\n",
      "Validation loss: 6.837\n",
      "Average minibatch loss at step 1538: 5.397\n",
      "Average minibatch loss at step 1540: 5.608\n",
      "Average minibatch loss at step 1542: 5.437\n",
      "Average minibatch loss at step 1544: 5.433\n",
      "Validation loss: 6.711\n",
      "Average minibatch loss at step 1546: 5.452\n",
      "Average minibatch loss at step 1548: 5.286\n",
      "Average minibatch loss at step 1550: 5.376\n",
      "Average minibatch loss at step 1552: 5.220\n",
      "Validation loss: 6.764\n",
      "Average minibatch loss at step 1554: 5.410\n",
      "Average minibatch loss at step 1556: 5.659\n",
      "Average minibatch loss at step 1558: 5.365\n",
      "Average minibatch loss at step 1560: 5.351\n",
      "Validation loss: 6.655\n",
      "Average minibatch loss at step 1562: 5.470\n",
      "Average minibatch loss at step 1564: 5.274\n",
      "Average minibatch loss at step 1566: 5.221\n",
      "Average minibatch loss at step 1568: 5.143\n",
      "Validation loss: 6.922\n",
      "Average minibatch loss at step 1570: 5.326\n",
      "Average minibatch loss at step 1572: 5.502\n",
      "Average minibatch loss at step 1574: 5.404\n",
      "Average minibatch loss at step 1576: 5.378\n",
      "Validation loss: 6.727\n",
      "Average minibatch loss at step 1578: 5.410\n",
      "Average minibatch loss at step 1580: 5.230\n",
      "Average minibatch loss at step 1582: 5.191\n",
      "Average minibatch loss at step 1584: 5.095\n",
      "Validation loss: 6.653\n",
      "Average minibatch loss at step 1586: 5.466\n",
      "Average minibatch loss at step 1588: 5.587\n",
      "Average minibatch loss at step 1590: 5.383\n",
      "Average minibatch loss at step 1592: 5.353\n",
      "Validation loss: 6.756\n",
      "Average minibatch loss at step 1594: 5.442\n",
      "Average minibatch loss at step 1596: 5.282\n",
      "Average minibatch loss at step 1598: 5.391\n",
      "Average minibatch loss at step 1600: 5.184\n",
      "Validation loss: 7.115\n",
      "Average minibatch loss at step 1602: 5.355\n",
      "Average minibatch loss at step 1604: 5.601\n",
      "Average minibatch loss at step 1606: 5.350\n",
      "Average minibatch loss at step 1608: 5.350\n",
      "Validation loss: 6.776\n",
      "Average minibatch loss at step 1610: 5.440\n",
      "Average minibatch loss at step 1612: 5.279\n",
      "Average minibatch loss at step 1614: 5.279\n",
      "Average minibatch loss at step 1616: 5.136\n",
      "Validation loss: 6.889\n",
      "Average minibatch loss at step 1618: 5.332\n",
      "Average minibatch loss at step 1620: 5.519\n",
      "Average minibatch loss at step 1622: 5.362\n",
      "Average minibatch loss at step 1624: 5.367\n",
      "Validation loss: 6.820\n",
      "Average minibatch loss at step 1626: 5.446\n",
      "Average minibatch loss at step 1628: 5.244\n",
      "Average minibatch loss at step 1630: 5.308\n",
      "Average minibatch loss at step 1632: 5.453\n",
      "Validation loss: 7.247\n",
      "Average minibatch loss at step 1634: 5.363\n",
      "Average minibatch loss at step 1636: 5.520\n",
      "Average minibatch loss at step 1638: 5.370\n",
      "Average minibatch loss at step 1640: 5.359\n",
      "Validation loss: 6.666\n",
      "Average minibatch loss at step 1642: 5.421\n",
      "Average minibatch loss at step 1644: 5.252\n",
      "Average minibatch loss at step 1646: 5.242\n",
      "Average minibatch loss at step 1648: 5.082\n",
      "Validation loss: 6.701\n",
      "Average minibatch loss at step 1650: 5.295\n",
      "Average minibatch loss at step 1652: 5.479\n",
      "Average minibatch loss at step 1654: 5.362\n",
      "Average minibatch loss at step 1656: 5.352\n",
      "Validation loss: 6.727\n",
      "Average minibatch loss at step 1658: 5.394\n",
      "Average minibatch loss at step 1660: 5.218\n",
      "Average minibatch loss at step 1662: 5.196\n",
      "Average minibatch loss at step 1664: 5.094\n",
      "Validation loss: 6.721\n",
      "Average minibatch loss at step 1666: 5.381\n",
      "Average minibatch loss at step 1668: 5.555\n",
      "Average minibatch loss at step 1670: 5.324\n",
      "Average minibatch loss at step 1672: 5.330\n",
      "Validation loss: 6.830\n",
      "Average minibatch loss at step 1674: 5.388\n",
      "Average minibatch loss at step 1676: 5.237\n",
      "Average minibatch loss at step 1678: 5.189\n",
      "Average minibatch loss at step 1680: 5.179\n",
      "Validation loss: 7.026\n",
      "Average minibatch loss at step 1682: 5.299\n",
      "Average minibatch loss at step 1684: 5.437\n",
      "Average minibatch loss at step 1686: 5.379\n",
      "Average minibatch loss at step 1688: 5.342\n",
      "Validation loss: 6.944\n",
      "Average minibatch loss at step 1690: 5.390\n",
      "Average minibatch loss at step 1692: 5.244\n",
      "Average minibatch loss at step 1694: 5.221\n",
      "Average minibatch loss at step 1696: 5.120\n",
      "Validation loss: 6.732\n",
      "Average minibatch loss at step 1698: 5.389\n",
      "Average minibatch loss at step 1700: 5.582\n",
      "Average minibatch loss at step 1702: 5.333\n",
      "Average minibatch loss at step 1704: 5.318\n",
      "Validation loss: 6.963\n",
      "Average minibatch loss at step 1706: 5.378\n",
      "Average minibatch loss at step 1708: 5.257\n",
      "Average minibatch loss at step 1710: 5.185\n",
      "Average minibatch loss at step 1712: 5.170\n",
      "Validation loss: 6.811\n",
      "Average minibatch loss at step 1714: 5.279\n",
      "Average minibatch loss at step 1716: 5.430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1718: 5.318\n",
      "Average minibatch loss at step 1720: 5.319\n",
      "Validation loss: 6.828\n",
      "Average minibatch loss at step 1722: 5.399\n",
      "Average minibatch loss at step 1724: 5.188\n",
      "Average minibatch loss at step 1726: 5.159\n",
      "Average minibatch loss at step 1728: 5.046\n",
      "Validation loss: 6.589\n",
      "Average minibatch loss at step 1730: 5.323\n",
      "Average minibatch loss at step 1732: 5.451\n",
      "Average minibatch loss at step 1734: 5.349\n",
      "Average minibatch loss at step 1736: 5.319\n",
      "Validation loss: 6.806\n",
      "Average minibatch loss at step 1738: 5.505\n",
      "Average minibatch loss at step 1740: 5.304\n",
      "Average minibatch loss at step 1742: 5.276\n",
      "Average minibatch loss at step 1744: 5.186\n",
      "Validation loss: 7.075\n",
      "Average minibatch loss at step 1746: 5.315\n",
      "Average minibatch loss at step 1748: 5.515\n",
      "Average minibatch loss at step 1750: 5.304\n",
      "Average minibatch loss at step 1752: 5.326\n",
      "Validation loss: 6.827\n",
      "Average minibatch loss at step 1754: 5.393\n",
      "Average minibatch loss at step 1756: 5.263\n",
      "Average minibatch loss at step 1758: 5.490\n",
      "Average minibatch loss at step 1760: 5.147\n",
      "Validation loss: 6.664\n",
      "Average minibatch loss at step 1762: 5.317\n",
      "Average minibatch loss at step 1764: 5.479\n",
      "Average minibatch loss at step 1766: 5.299\n",
      "Average minibatch loss at step 1768: 5.336\n",
      "Validation loss: 6.865\n",
      "Average minibatch loss at step 1770: 5.362\n",
      "Average minibatch loss at step 1772: 5.189\n",
      "Average minibatch loss at step 1774: 5.203\n",
      "Average minibatch loss at step 1776: 5.063\n",
      "Validation loss: 6.822\n",
      "Average minibatch loss at step 1778: 5.481\n",
      "Average minibatch loss at step 1780: 5.483\n",
      "Average minibatch loss at step 1782: 5.320\n",
      "Average minibatch loss at step 1784: 5.308\n",
      "Validation loss: 6.887\n",
      "Average minibatch loss at step 1786: 5.392\n",
      "Average minibatch loss at step 1788: 5.195\n",
      "Average minibatch loss at step 1790: 5.202\n",
      "Average minibatch loss at step 1792: 5.174\n",
      "Validation loss: 6.988\n",
      "Average minibatch loss at step 1794: 5.382\n",
      "Average minibatch loss at step 1796: 5.468\n",
      "Average minibatch loss at step 1798: 5.305\n",
      "Average minibatch loss at step 1800: 5.326\n",
      "Validation loss: 6.881\n",
      "Average minibatch loss at step 1802: 5.357\n",
      "Average minibatch loss at step 1804: 5.212\n",
      "Average minibatch loss at step 1806: 5.279\n",
      "Average minibatch loss at step 1808: 5.117\n",
      "Validation loss: 6.877\n",
      "Average minibatch loss at step 1810: 5.253\n",
      "Average minibatch loss at step 1812: 5.390\n",
      "Average minibatch loss at step 1814: 5.323\n",
      "Average minibatch loss at step 1816: 5.389\n",
      "Validation loss: 6.910\n",
      "Average minibatch loss at step 1818: 5.357\n",
      "Average minibatch loss at step 1820: 5.218\n",
      "Average minibatch loss at step 1822: 5.235\n",
      "Average minibatch loss at step 1824: 5.086\n",
      "Validation loss: 6.835\n",
      "Average minibatch loss at step 1826: 5.314\n",
      "Average minibatch loss at step 1828: 5.499\n",
      "Average minibatch loss at step 1830: 5.330\n",
      "Average minibatch loss at step 1832: 5.336\n",
      "Validation loss: 6.861\n",
      "Average minibatch loss at step 1834: 5.399\n",
      "Average minibatch loss at step 1836: 5.193\n",
      "Average minibatch loss at step 1838: 5.177\n",
      "Average minibatch loss at step 1840: 5.086\n",
      "Validation loss: 6.847\n",
      "Average minibatch loss at step 1842: 5.332\n",
      "Average minibatch loss at step 1844: 5.457\n",
      "Average minibatch loss at step 1846: 5.375\n",
      "Average minibatch loss at step 1848: 5.299\n",
      "Validation loss: 6.754\n",
      "Average minibatch loss at step 1850: 5.350\n",
      "Average minibatch loss at step 1852: 5.193\n",
      "Average minibatch loss at step 1854: 5.144\n",
      "Average minibatch loss at step 1856: 5.057\n",
      "Validation loss: 6.936\n",
      "Average minibatch loss at step 1858: 5.237\n",
      "Average minibatch loss at step 1860: 5.424\n",
      "Average minibatch loss at step 1862: 5.306\n",
      "Average minibatch loss at step 1864: 5.314\n",
      "Validation loss: 7.009\n",
      "Average minibatch loss at step 1866: 5.382\n",
      "Average minibatch loss at step 1868: 5.186\n",
      "Average minibatch loss at step 1870: 5.237\n",
      "Average minibatch loss at step 1872: 5.085\n",
      "Validation loss: 6.964\n",
      "Average minibatch loss at step 1874: 5.337\n",
      "Average minibatch loss at step 1876: 5.496\n",
      "Average minibatch loss at step 1878: 5.445\n",
      "Average minibatch loss at step 1880: 5.269\n",
      "Validation loss: 6.836\n",
      "Average minibatch loss at step 1882: 5.349\n",
      "Average minibatch loss at step 1884: 5.159\n",
      "Average minibatch loss at step 1886: 5.178\n",
      "Average minibatch loss at step 1888: 5.067\n",
      "Validation loss: 6.832\n",
      "Average minibatch loss at step 1890: 5.553\n",
      "Average minibatch loss at step 1892: 5.545\n",
      "Average minibatch loss at step 1894: 5.436\n",
      "Average minibatch loss at step 1896: 5.299\n",
      "Validation loss: 7.008\n",
      "Average minibatch loss at step 1898: 5.345\n",
      "Average minibatch loss at step 1900: 5.187\n",
      "Average minibatch loss at step 1902: 5.279\n",
      "Average minibatch loss at step 1904: 5.035\n",
      "Validation loss: 6.900\n",
      "Average minibatch loss at step 1906: 5.267\n",
      "Average minibatch loss at step 1908: 5.424\n",
      "Average minibatch loss at step 1910: 5.299\n",
      "Average minibatch loss at step 1912: 5.285\n",
      "Validation loss: 6.983\n",
      "Average minibatch loss at step 1914: 5.328\n",
      "Average minibatch loss at step 1916: 5.154\n",
      "Average minibatch loss at step 1918: 5.158\n",
      "Average minibatch loss at step 1920: 5.162\n",
      "Validation loss: 7.096\n",
      "Average minibatch loss at step 1922: 5.337\n",
      "Average minibatch loss at step 1924: 5.483\n",
      "Average minibatch loss at step 1926: 5.312\n",
      "Average minibatch loss at step 1928: 5.338\n",
      "Validation loss: 6.895\n",
      "Average minibatch loss at step 1930: 5.315\n",
      "Average minibatch loss at step 1932: 5.210\n",
      "Average minibatch loss at step 1934: 5.182\n",
      "Average minibatch loss at step 1936: 5.148\n",
      "Validation loss: 6.876\n",
      "Average minibatch loss at step 1938: 5.466\n",
      "Average minibatch loss at step 1940: 5.426\n",
      "Average minibatch loss at step 1942: 5.328\n",
      "Average minibatch loss at step 1944: 5.273\n",
      "Validation loss: 6.983\n",
      "Average minibatch loss at step 1946: 5.374\n",
      "Average minibatch loss at step 1948: 5.169\n",
      "Average minibatch loss at step 1950: 5.188\n",
      "Average minibatch loss at step 1952: 5.049\n",
      "Validation loss: 6.729\n",
      "Average minibatch loss at step 1954: 5.394\n",
      "Average minibatch loss at step 1956: 5.371\n",
      "Average minibatch loss at step 1958: 5.260\n",
      "Average minibatch loss at step 1960: 5.280\n",
      "Validation loss: 6.958\n",
      "Average minibatch loss at step 1962: 5.325\n",
      "Average minibatch loss at step 1964: 5.154\n",
      "Average minibatch loss at step 1966: 5.124\n",
      "Average minibatch loss at step 1968: 5.062\n",
      "Validation loss: 7.160\n",
      "Average minibatch loss at step 1970: 5.344\n",
      "Average minibatch loss at step 1972: 5.369\n",
      "Average minibatch loss at step 1974: 5.277\n",
      "Average minibatch loss at step 1976: 5.290\n",
      "Validation loss: 6.948\n",
      "Average minibatch loss at step 1978: 5.388\n",
      "Average minibatch loss at step 1980: 5.246\n",
      "Average minibatch loss at step 1982: 5.157\n",
      "Average minibatch loss at step 1984: 4.994\n",
      "Validation loss: 6.718\n",
      "Average minibatch loss at step 1986: 5.226\n",
      "Average minibatch loss at step 1988: 5.350\n",
      "Average minibatch loss at step 1990: 5.259\n",
      "Average minibatch loss at step 1992: 5.271\n",
      "Validation loss: 6.866\n",
      "Average minibatch loss at step 1994: 5.317\n",
      "Average minibatch loss at step 1996: 5.165\n",
      "Average minibatch loss at step 1998: 5.098\n",
      "Average minibatch loss at step 2000: 5.041\n",
      "Validation loss: 6.960\n",
      "Average minibatch loss at step 2002: 5.230\n",
      "Average minibatch loss at step 2004: 5.425\n",
      "Average minibatch loss at step 2006: 5.313\n",
      "Average minibatch loss at step 2008: 5.268\n",
      "Validation loss: 6.922\n",
      "Average minibatch loss at step 2010: 5.292\n",
      "Average minibatch loss at step 2012: 5.146\n",
      "Average minibatch loss at step 2014: 5.187\n",
      "Average minibatch loss at step 2016: 5.037\n",
      "Validation loss: 6.771\n",
      "Average minibatch loss at step 2018: 5.388\n",
      "Average minibatch loss at step 2020: 5.422\n",
      "Average minibatch loss at step 2022: 5.291\n",
      "Average minibatch loss at step 2024: 5.251\n",
      "Validation loss: 6.868\n",
      "Average minibatch loss at step 2026: 5.278\n",
      "Average minibatch loss at step 2028: 5.176\n",
      "Average minibatch loss at step 2030: 5.157\n",
      "Average minibatch loss at step 2032: 5.066\n",
      "Validation loss: 6.923\n",
      "Average minibatch loss at step 2034: 5.261\n",
      "Average minibatch loss at step 2036: 5.390\n",
      "Average minibatch loss at step 2038: 5.319\n",
      "Average minibatch loss at step 2040: 5.268\n",
      "Validation loss: 6.952\n",
      "Average minibatch loss at step 2042: 5.355\n",
      "Average minibatch loss at step 2044: 5.145\n",
      "Average minibatch loss at step 2046: 5.115\n",
      "Average minibatch loss at step 2048: 5.068\n",
      "Validation loss: 7.143\n",
      "Average minibatch loss at step 2050: 5.238\n",
      "Average minibatch loss at step 2052: 5.329\n",
      "Average minibatch loss at step 2054: 5.293\n",
      "Average minibatch loss at step 2056: 5.280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 6.836\n",
      "Average minibatch loss at step 2058: 5.293\n",
      "Average minibatch loss at step 2060: 5.214\n",
      "Average minibatch loss at step 2062: 5.191\n",
      "Average minibatch loss at step 2064: 5.048\n",
      "Validation loss: 6.892\n",
      "Average minibatch loss at step 2066: 5.224\n",
      "Average minibatch loss at step 2068: 5.432\n",
      "Average minibatch loss at step 2070: 5.279\n",
      "Average minibatch loss at step 2072: 5.228\n",
      "Validation loss: 6.890\n",
      "Average minibatch loss at step 2074: 5.362\n",
      "Average minibatch loss at step 2076: 5.142\n",
      "Average minibatch loss at step 2078: 5.208\n",
      "Average minibatch loss at step 2080: 4.986\n",
      "Validation loss: 7.011\n",
      "Average minibatch loss at step 2082: 5.234\n",
      "Average minibatch loss at step 2084: 5.383\n",
      "Average minibatch loss at step 2086: 5.254\n",
      "Average minibatch loss at step 2088: 5.278\n",
      "Validation loss: 6.993\n",
      "Average minibatch loss at step 2090: 5.303\n",
      "Average minibatch loss at step 2092: 5.168\n",
      "Average minibatch loss at step 2094: 5.232\n",
      "Average minibatch loss at step 2096: 5.065\n",
      "Validation loss: 7.096\n",
      "Average minibatch loss at step 2098: 5.218\n",
      "Average minibatch loss at step 2100: 5.386\n",
      "Average minibatch loss at step 2102: 5.238\n",
      "Average minibatch loss at step 2104: 5.226\n",
      "Validation loss: 6.820\n",
      "Average minibatch loss at step 2106: 5.297\n",
      "Average minibatch loss at step 2108: 5.130\n",
      "Average minibatch loss at step 2110: 5.093\n",
      "Average minibatch loss at step 2112: 5.023\n",
      "Validation loss: 7.206\n",
      "Average minibatch loss at step 2114: 5.187\n",
      "Average minibatch loss at step 2116: 5.354\n",
      "Average minibatch loss at step 2118: 5.243\n",
      "Average minibatch loss at step 2120: 5.223\n",
      "Validation loss: 7.079\n",
      "Average minibatch loss at step 2122: 5.304\n",
      "Average minibatch loss at step 2124: 5.150\n",
      "Average minibatch loss at step 2126: 5.150\n",
      "Average minibatch loss at step 2128: 5.106\n",
      "Validation loss: 6.817\n",
      "Average minibatch loss at step 2130: 5.214\n",
      "Average minibatch loss at step 2132: 5.335\n",
      "Average minibatch loss at step 2134: 5.223\n",
      "Average minibatch loss at step 2136: 5.252\n",
      "Validation loss: 7.005\n",
      "Average minibatch loss at step 2138: 5.341\n",
      "Average minibatch loss at step 2140: 5.139\n",
      "Average minibatch loss at step 2142: 5.123\n",
      "Average minibatch loss at step 2144: 4.994\n",
      "Validation loss: 6.840\n",
      "Average minibatch loss at step 2146: 5.189\n",
      "Average minibatch loss at step 2148: 5.377\n",
      "Average minibatch loss at step 2150: 5.322\n",
      "Average minibatch loss at step 2152: 5.220\n",
      "Validation loss: 7.003\n",
      "Average minibatch loss at step 2154: 5.290\n",
      "Average minibatch loss at step 2156: 5.140\n",
      "Average minibatch loss at step 2158: 5.100\n",
      "Average minibatch loss at step 2160: 5.050\n",
      "Validation loss: 6.984\n",
      "Average minibatch loss at step 2162: 5.169\n",
      "Average minibatch loss at step 2164: 5.310\n",
      "Average minibatch loss at step 2166: 5.230\n",
      "Average minibatch loss at step 2168: 5.247\n",
      "Validation loss: 6.923\n",
      "Average minibatch loss at step 2170: 5.285\n",
      "Average minibatch loss at step 2172: 5.089\n",
      "Average minibatch loss at step 2174: 5.094\n",
      "Average minibatch loss at step 2176: 5.172\n",
      "Validation loss: 7.707\n",
      "Average minibatch loss at step 2178: 5.462\n",
      "Average minibatch loss at step 2180: 5.374\n",
      "Average minibatch loss at step 2182: 5.255\n",
      "Average minibatch loss at step 2184: 5.226\n",
      "Validation loss: 7.037\n",
      "Average minibatch loss at step 2186: 5.290\n",
      "Average minibatch loss at step 2188: 5.142\n",
      "Average minibatch loss at step 2190: 5.097\n",
      "Average minibatch loss at step 2192: 4.983\n",
      "Validation loss: 6.873\n",
      "Average minibatch loss at step 2194: 5.254\n",
      "Average minibatch loss at step 2196: 5.428\n",
      "Average minibatch loss at step 2198: 5.230\n",
      "Average minibatch loss at step 2200: 5.250\n",
      "Validation loss: 6.939\n",
      "Average minibatch loss at step 2202: 5.267\n",
      "Average minibatch loss at step 2204: 5.106\n",
      "Average minibatch loss at step 2206: 5.095\n",
      "Average minibatch loss at step 2208: 4.973\n",
      "Validation loss: 7.074\n",
      "Average minibatch loss at step 2210: 5.370\n",
      "Average minibatch loss at step 2212: 5.303\n",
      "Average minibatch loss at step 2214: 5.231\n",
      "Average minibatch loss at step 2216: 5.314\n",
      "Validation loss: 6.949\n",
      "Average minibatch loss at step 2218: 5.257\n",
      "Average minibatch loss at step 2220: 5.119\n",
      "Average minibatch loss at step 2222: 5.099\n",
      "Average minibatch loss at step 2224: 4.959\n",
      "Validation loss: 6.940\n",
      "Average minibatch loss at step 2226: 5.283\n",
      "Average minibatch loss at step 2228: 5.354\n",
      "Average minibatch loss at step 2230: 5.241\n",
      "Average minibatch loss at step 2232: 5.223\n",
      "Validation loss: 6.964\n",
      "Average minibatch loss at step 2234: 5.280\n",
      "Average minibatch loss at step 2236: 5.202\n",
      "Average minibatch loss at step 2238: 5.105\n",
      "Average minibatch loss at step 2240: 4.984\n",
      "Validation loss: 7.005\n",
      "Average minibatch loss at step 2242: 5.390\n",
      "Average minibatch loss at step 2244: 5.345\n",
      "Average minibatch loss at step 2246: 5.211\n",
      "Average minibatch loss at step 2248: 5.224\n",
      "Validation loss: 7.018\n",
      "Average minibatch loss at step 2250: 5.289\n",
      "Average minibatch loss at step 2252: 5.123\n",
      "Average minibatch loss at step 2254: 5.209\n",
      "Average minibatch loss at step 2256: 5.021\n",
      "Validation loss: 6.832\n",
      "Average minibatch loss at step 2258: 5.241\n",
      "Average minibatch loss at step 2260: 5.366\n",
      "Average minibatch loss at step 2262: 5.224\n",
      "Average minibatch loss at step 2264: 5.222\n",
      "Validation loss: 6.951\n",
      "Average minibatch loss at step 2266: 5.258\n",
      "Average minibatch loss at step 2268: 5.178\n",
      "Average minibatch loss at step 2270: 5.133\n",
      "Average minibatch loss at step 2272: 4.969\n",
      "Validation loss: 6.744\n",
      "Average minibatch loss at step 2274: 5.191\n",
      "Average minibatch loss at step 2276: 5.399\n",
      "Average minibatch loss at step 2278: 5.330\n",
      "Average minibatch loss at step 2280: 5.219\n",
      "Validation loss: 6.996\n",
      "Average minibatch loss at step 2282: 5.258\n",
      "Average minibatch loss at step 2284: 5.099\n",
      "Average minibatch loss at step 2286: 5.083\n",
      "Average minibatch loss at step 2288: 4.960\n",
      "Validation loss: 6.845\n",
      "Average minibatch loss at step 2290: 5.161\n",
      "Average minibatch loss at step 2292: 5.372\n",
      "Average minibatch loss at step 2294: 5.217\n",
      "Average minibatch loss at step 2296: 5.212\n",
      "Validation loss: 7.032\n",
      "Average minibatch loss at step 2298: 5.301\n",
      "Average minibatch loss at step 2300: 5.125\n",
      "Average minibatch loss at step 2302: 5.079\n",
      "Average minibatch loss at step 2304: 4.981\n",
      "Validation loss: 7.142\n",
      "Average minibatch loss at step 2306: 5.153\n",
      "Average minibatch loss at step 2308: 5.357\n",
      "Average minibatch loss at step 2310: 5.220\n",
      "Average minibatch loss at step 2312: 5.208\n",
      "Validation loss: 6.958\n",
      "Average minibatch loss at step 2314: 5.246\n",
      "Average minibatch loss at step 2316: 5.118\n",
      "Average minibatch loss at step 2318: 5.151\n",
      "Average minibatch loss at step 2320: 4.927\n",
      "Validation loss: 6.922\n",
      "Average minibatch loss at step 2322: 5.131\n",
      "Average minibatch loss at step 2324: 5.289\n",
      "Average minibatch loss at step 2326: 5.290\n",
      "Average minibatch loss at step 2328: 5.222\n",
      "Validation loss: 6.900\n",
      "Average minibatch loss at step 2330: 5.257\n",
      "Average minibatch loss at step 2332: 5.075\n",
      "Average minibatch loss at step 2334: 5.036\n",
      "Average minibatch loss at step 2336: 4.908\n",
      "Validation loss: 6.819\n",
      "Average minibatch loss at step 2338: 5.175\n",
      "Average minibatch loss at step 2340: 5.331\n",
      "Average minibatch loss at step 2342: 5.223\n",
      "Average minibatch loss at step 2344: 5.234\n",
      "Validation loss: 6.948\n",
      "Average minibatch loss at step 2346: 5.260\n",
      "Average minibatch loss at step 2348: 5.093\n",
      "Average minibatch loss at step 2350: 5.121\n",
      "Average minibatch loss at step 2352: 4.989\n",
      "Validation loss: 6.993\n",
      "Average minibatch loss at step 2354: 5.147\n",
      "Average minibatch loss at step 2356: 5.349\n",
      "Average minibatch loss at step 2358: 5.225\n",
      "Average minibatch loss at step 2360: 5.212\n",
      "Validation loss: 7.178\n",
      "Average minibatch loss at step 2362: 5.282\n",
      "Average minibatch loss at step 2364: 5.107\n",
      "Average minibatch loss at step 2366: 5.111\n",
      "Average minibatch loss at step 2368: 4.897\n",
      "Validation loss: 6.831\n",
      "Average minibatch loss at step 2370: 5.193\n",
      "Average minibatch loss at step 2372: 5.315\n",
      "Average minibatch loss at step 2374: 5.208\n",
      "Average minibatch loss at step 2376: 5.207\n",
      "Validation loss: 7.294\n",
      "Average minibatch loss at step 2378: 5.279\n",
      "Average minibatch loss at step 2380: 5.157\n",
      "Average minibatch loss at step 2382: 5.077\n",
      "Average minibatch loss at step 2384: 5.015\n",
      "Validation loss: 7.098\n",
      "Average minibatch loss at step 2386: 5.245\n",
      "Average minibatch loss at step 2388: 5.302\n",
      "Average minibatch loss at step 2390: 5.265\n",
      "Average minibatch loss at step 2392: 5.202\n",
      "Validation loss: 6.913\n",
      "Average minibatch loss at step 2394: 5.270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 2396: 5.107\n",
      "Average minibatch loss at step 2398: 5.050\n",
      "Average minibatch loss at step 2400: 4.886\n",
      "Validation loss: 6.912\n",
      "Average minibatch loss at step 2402: 5.163\n",
      "Average minibatch loss at step 2404: 5.329\n",
      "Average minibatch loss at step 2406: 5.325\n",
      "Average minibatch loss at step 2408: 5.189\n",
      "Validation loss: 6.995\n",
      "Average minibatch loss at step 2410: 5.273\n",
      "Average minibatch loss at step 2412: 5.093\n",
      "Average minibatch loss at step 2414: 5.028\n",
      "Average minibatch loss at step 2416: 4.863\n",
      "Validation loss: 6.918\n",
      "Average minibatch loss at step 2418: 5.137\n",
      "Average minibatch loss at step 2420: 5.319\n",
      "Average minibatch loss at step 2422: 5.182\n",
      "Average minibatch loss at step 2424: 5.193\n",
      "Validation loss: 6.982\n",
      "Average minibatch loss at step 2426: 5.283\n",
      "Average minibatch loss at step 2428: 5.115\n",
      "Average minibatch loss at step 2430: 5.122\n",
      "Average minibatch loss at step 2432: 4.887\n",
      "Validation loss: 6.970\n",
      "Average minibatch loss at step 2434: 5.135\n",
      "Average minibatch loss at step 2436: 5.403\n",
      "Average minibatch loss at step 2438: 5.208\n",
      "Average minibatch loss at step 2440: 5.210\n",
      "Validation loss: 6.865\n",
      "Average minibatch loss at step 2442: 5.238\n",
      "Average minibatch loss at step 2444: 5.088\n",
      "Average minibatch loss at step 2446: 5.113\n",
      "Average minibatch loss at step 2448: 4.935\n",
      "Validation loss: 6.843\n",
      "Average minibatch loss at step 2450: 5.180\n",
      "Average minibatch loss at step 2452: 5.290\n",
      "Average minibatch loss at step 2454: 5.204\n",
      "Average minibatch loss at step 2456: 5.272\n",
      "Validation loss: 7.012\n",
      "Average minibatch loss at step 2458: 5.237\n",
      "Average minibatch loss at step 2460: 5.054\n",
      "Average minibatch loss at step 2462: 5.026\n",
      "Average minibatch loss at step 2464: 4.891\n",
      "Validation loss: 6.960\n",
      "Average minibatch loss at step 2466: 5.149\n",
      "Average minibatch loss at step 2468: 5.330\n",
      "Average minibatch loss at step 2470: 5.223\n",
      "Average minibatch loss at step 2472: 5.185\n",
      "Validation loss: 7.045\n",
      "Average minibatch loss at step 2474: 5.222\n",
      "Average minibatch loss at step 2476: 5.067\n",
      "Average minibatch loss at step 2478: 5.079\n",
      "Average minibatch loss at step 2480: 4.902\n",
      "Validation loss: 6.912\n",
      "Average minibatch loss at step 2482: 5.127\n",
      "Average minibatch loss at step 2484: 5.349\n",
      "Average minibatch loss at step 2486: 5.165\n",
      "Average minibatch loss at step 2488: 5.171\n",
      "Validation loss: 7.063\n",
      "Average minibatch loss at step 2490: 5.278\n",
      "Average minibatch loss at step 2492: 5.081\n",
      "Average minibatch loss at step 2494: 5.036\n",
      "Average minibatch loss at step 2496: 4.866\n",
      "Validation loss: 6.854\n",
      "Average minibatch loss at step 2498: 5.127\n",
      "Average minibatch loss at step 2500: 5.370\n",
      "Average minibatch loss at step 2502: 5.218\n",
      "Average minibatch loss at step 2504: 5.191\n",
      "Validation loss: 7.039\n",
      "Average minibatch loss at step 2506: 5.259\n",
      "Average minibatch loss at step 2508: 5.062\n",
      "Average minibatch loss at step 2510: 5.046\n",
      "Average minibatch loss at step 2512: 4.885\n",
      "Validation loss: 6.889\n",
      "Average minibatch loss at step 2514: 5.128\n",
      "Average minibatch loss at step 2516: 5.259\n",
      "Average minibatch loss at step 2518: 5.185\n",
      "Average minibatch loss at step 2520: 5.189\n",
      "Validation loss: 6.965\n",
      "Average minibatch loss at step 2522: 5.635\n",
      "Average minibatch loss at step 2524: 5.234\n",
      "Average minibatch loss at step 2526: 5.069\n",
      "Average minibatch loss at step 2528: 4.880\n",
      "Validation loss: 7.086\n",
      "Average minibatch loss at step 2530: 5.366\n",
      "Average minibatch loss at step 2532: 5.328\n",
      "Average minibatch loss at step 2534: 5.188\n",
      "Average minibatch loss at step 2536: 5.166\n",
      "Validation loss: 7.012\n",
      "Average minibatch loss at step 2538: 5.235\n",
      "Average minibatch loss at step 2540: 5.070\n",
      "Average minibatch loss at step 2542: 5.045\n",
      "Average minibatch loss at step 2544: 4.900\n",
      "Validation loss: 7.056\n",
      "Average minibatch loss at step 2546: 5.139\n",
      "Average minibatch loss at step 2548: 5.282\n",
      "Average minibatch loss at step 2550: 5.178\n",
      "Average minibatch loss at step 2552: 5.150\n",
      "Validation loss: 6.972\n",
      "Average minibatch loss at step 2554: 5.208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d110c9552f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6be907b580b0>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6be907b580b0>\u001b[0m in \u001b[0;36mbuild_batch\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# + 1 is for the <eos> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         abstract_lengths = torch.cuda.LongTensor(\n\u001b[0;32m---> 24\u001b[0;31m             [len(row.abstract.split()[:max_abstract_size]) for row in rows]) + 1\n\u001b[0m\u001b[1;32m     25\u001b[0m         article_lengths = torch.cuda.LongTensor(\n\u001b[1;32m     26\u001b[0m             [len(row.article.split()[:max_article_size]) for row in rows]) + 1 \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "epochs = 7000 #7000\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "val_df = pd.read_csv('datasets/val.csv')\n",
    "iteration = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch % 1500 == 0: #1500\n",
    "        learning_rate = learning_rate / 2 #2\n",
    "        # Filter parameters that do not require gradients\n",
    "        encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "        # Optimizers\n",
    "        encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "        print('')\n",
    "        print('learning rate: %f' % learning_rate)\n",
    "        print('')\n",
    "        \n",
    "    generator = BatchGenerator(batch_size, train_df[:1000]) \n",
    "\n",
    "    while True:\n",
    "        try: \n",
    "            batch = generator.get_batch()\n",
    "        except StopIteration: break\n",
    "        loss = train_model(batch)\n",
    "        \n",
    "        if iteration % 2 == 0:\n",
    "            print('Average minibatch loss at step %d: %.3f' % (iteration, loss))\n",
    "            writer.add_scalar('train_loss', loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "        \n",
    "        if iteration % 8 == 0:    \n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            val_loss = validation_loss(val_df[:8]) # truncating validation dataframe\n",
    "            print('Validation loss: %.3f' % val_loss)\n",
    "            \n",
    "            writer.add_scalar('valid_loss', val_loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(encoder.state_dict(), 'encoder')\n",
    "torch.save(decoder.state_dict(), 'decoder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
