{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "from fastText import load_model\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext embeddings trained on train and val sets\n",
    "# ./fasttext skipgram -input input_text_file -output output_model -dim 128 (fastText-0.1.0)\n",
    "fasttext_model = load_model('word_vectors/fasttext_model.bin')\n",
    "num_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab contains frequent words apperaing in the text along with their frequencies\n",
    "# minimum frequency = 6\n",
    "vocab_file = open('finished_files/vocab')\n",
    "# Store appearing words\n",
    "vocab_words = {}\n",
    "for line in vocab_file:\n",
    "    li = line.split()\n",
    "    if len(li) == 2:\n",
    "        word, freq = li\n",
    "        vocab_words[word] = freq\n",
    "# Final word to id dictionary    \n",
    "word2id = {}\n",
    "tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for token in tokens:\n",
    "    word2id[token] = len(word2id)\n",
    "# Retrieve words from fasttext model and keep only those which are also present in 'vocab'\n",
    "fasttext_words = fasttext_model.get_words()\n",
    "for word in fasttext_words:\n",
    "    if word in vocab_words:\n",
    "        word2id[word] = len(word2id)        \n",
    "vocab_size = len(word2id)\n",
    "# Reverse dictionary\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "# Embeddings\n",
    "embeddings = np.zeros((vocab_size, num_dims))\n",
    "# <pad> token vector contains all zeros. Rest sampled from a normal distribution\n",
    "mu, sigma = 0, 0.05\n",
    "for i in range(1, len(tokens)):\n",
    "    embeddings[i] = np.random.normal(mu, sigma, num_dims)\n",
    "# Get word vectors from fasttext model and store in embeddings matrix\n",
    "for i in range(len(tokens), vocab_size):\n",
    "    embeddings[i] = fasttext_model.get_word_vector(id2word[i])\n",
    "    \n",
    "del fasttext_model, vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "for i in range(1000):\n",
    "    temp[i] = id2word[i]\n",
    "id2word = temp\n",
    "embeddings = embeddings[:1000]\n",
    "word2id = dict(zip(id2word.values(), id2word.keys()))\n",
    "\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_points = 500\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "words = [id2word[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_article_size = 25 #400\n",
    "max_abstract_size = 10 #100\n",
    "hidden_size = 512\n",
    "hidden_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self):\n",
    "        self.abstract = (None, None)\n",
    "        self.article = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator: \n",
    "    \n",
    "    def __init__(self, batch_size, dataframe):\n",
    "        self.batch_size = batch_size\n",
    "        # train, valid, or test dataframe imported from csv\n",
    "        self.df = dataframe\n",
    "        self.generator = self.row_generator()\n",
    "        \n",
    "        \n",
    "    def row_generator(self):\n",
    "        for row in self.df.itertuples(index=False):\n",
    "            yield row\n",
    "            \n",
    "    def build_batch(self, rows):\n",
    "        # If number of rows less than batch size, get extra rows from the beginning of the dataframe\n",
    "        if len(rows) < self.batch_size:\n",
    "            temp_generator = self.row_generator()\n",
    "            for i in range(self.batch_size - len(rows)):\n",
    "                rows.append(self.get_row(temp_generator))\n",
    "                \n",
    "        # Get lengths of all the sequences in the batch upto max number of tokens\n",
    "        # + 1 is for the <eos> token\n",
    "        abstract_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.abstract.split()[:max_abstract_size]) for row in rows]) + 1\n",
    "        article_lengths = torch.cuda.LongTensor(\n",
    "            [len(row.article.split()[:max_article_size]) for row in rows]) + 1 \n",
    "        abs_len = torch.max(abstract_lengths)\n",
    "        art_len = torch.max(article_lengths) \n",
    "        \n",
    "        # Variables containing abstracts and articles of the batch\n",
    "        abstracts = torch.cuda.LongTensor(abs_len, self.batch_size).fill_(0) # zero padding\n",
    "        articles = torch.cuda.LongTensor(art_len, self.batch_size).fill_(0) # zero padding\n",
    "        \n",
    "        # Sort rows in descending order of sequence (article) lengths\n",
    "        article_lengths, indices = torch.sort(article_lengths, descending=True)\n",
    "        rows = [rows[i] for i in indices]\n",
    "        abstract_lengths = torch.cuda.LongTensor([abstract_lengths[i] for i in indices])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Tokenize abstract and take max_abstract_size number of tokens\n",
    "            tokens = rows[i].abstract.split()[:max_abstract_size]\n",
    "            tokens.append('<eos>')\n",
    "            # Convert each token to word index\n",
    "            # <unk> token index for unknown words\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            # Store as column in abstracts variable with zero padding\n",
    "            abstracts[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "            # Same for articles\n",
    "            tokens = rows[i].article.split()[:max_article_size]\n",
    "            tokens.append('<eos>')\n",
    "            token_list = torch.LongTensor([word2id[token] if token in word2id \n",
    "                                           else word2id['<unk>'] for token in tokens])\n",
    "            articles[:,i][:len(token_list)] = token_list\n",
    "            \n",
    "        batch = Batch()\n",
    "        batch.article = (Variable(articles), article_lengths)\n",
    "        batch.abstract = (Variable(abstracts), abstract_lengths)\n",
    "        return batch\n",
    "            \n",
    "    def get_row(self, generator):\n",
    "        row = generator.__next__()\n",
    "        while not isinstance(row.article, str):\n",
    "            row = generator.__next__()\n",
    "        return row\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        rows = []\n",
    "        for b in range(self.batch_size):\n",
    "            try: rows.append(self.get_row(self.generator))\n",
    "            except StopIteration: break\n",
    "        if rows: return self.build_batch(rows)\n",
    "        else: raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embed): Embedding(1000, 128)\n",
       "  (lstm): LSTM(128, 512, num_layers=3)\n",
       "  (linear_transform): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Hidden layer and cell state of model\n",
    "        # Initialize before calling model\n",
    "        self.hidden = None\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm module\n",
    "        self.lstm = nn.LSTM(num_dims, hidden_size, hidden_layers)\n",
    "        self.lstm.cuda()\n",
    "        \n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    # Funtion to initialize hidden layers\n",
    "    def init_hidden(self, batch_size, volatile=False):\n",
    "        tensor1 = torch.cuda.FloatTensor(hidden_layers, batch_size, hidden_size).fill_(0)\n",
    "        tensor2 = torch.cuda.FloatTensor(hidden_layers, batch_size, hidden_size).fill_(0)\n",
    "        return (Variable(tensor1, volatile=volatile), Variable(tensor2, volatile=volatile))\n",
    "    \n",
    "    def forward(self, articles, article_lengths):\n",
    "        # Embedding lookup\n",
    "        input = self.embed(articles)\n",
    "        # input to pack_padded_sequence can be of Txbx*\n",
    "        # where T is the length of longest sequence\n",
    "        # b is batch size\n",
    "        # batch is sorted in descending order of sequence lengths\n",
    "        #packed_input = pack_padded_sequence(input, list(article_lengths))\n",
    "        #packed_output, self.hidden = self.lstm(packed_input, self.hidden)\n",
    "        _, self.hidden = self.lstm(input, self.hidden)\n",
    "        \n",
    "        output = self.linear_transform(self.hidden[0][hidden_layers - 1])\n",
    "        \n",
    "        # Final hidden state\n",
    "        return self.hidden, output\n",
    "    \n",
    "encoder = Encoder(batch_size)\n",
    "encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embed): Embedding(1000, 128)\n",
       "  (cell_list): ModuleList(\n",
       "    (0): LSTMCell(128, 512)\n",
       "    (1): LSTMCell(512, 512)\n",
       "    (2): LSTMCell(512, 512)\n",
       "  )\n",
       "  (linear_transform): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "    \n",
    "        # Cell and hidden states\n",
    "        self.cell_list = []\n",
    "        self.hidden_list = []\n",
    "    \n",
    "        # First cell takes word embeddings as input\n",
    "        self.cell_list.append(nn.LSTMCell(num_dims, hidden_size).cuda())\n",
    "        for cell in range(1, hidden_layers):\n",
    "            self.cell_list.append(nn.LSTMCell(hidden_size, hidden_size).cuda())\n",
    "        # ModlueList Holds submodules in a list. \n",
    "        # ModuleList can be indexed like a regular Python list, \n",
    "        # but modules it contains are properly registered, \n",
    "        # and will be visible by all Module methods.\n",
    "        self.cell_list=nn.ModuleList(self.cell_list) \n",
    "        \n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is a LongTensor of size batch_size\n",
    "        input = self.embed(input) \n",
    "        # Each item in hidden list is a tuple of previous cell and hidden states\n",
    "        for layer in range(hidden_layers):\n",
    "            self.hidden_list[layer] = self.cell_list[layer](input, self.hidden_list[layer])\n",
    "            input = self.hidden_list[layer][0]\n",
    "        # output has shape (batch_size, vocab_size)\n",
    "        output = self.linear_transform(self.hidden_list[hidden_layers - 1][0])\n",
    "        return output\n",
    "    \n",
    "decoder = Decoder()\n",
    "decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.85 #5.0\n",
    "\n",
    "# Filter parameters that do not require gradients\n",
    "encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "# Optimizers\n",
    "encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "# Loss function\n",
    "# Way to accumulate loss on sequences with variable lengths in batches :\n",
    "# size_average: By default, the losses are averaged over observations for each minibatch.\n",
    "# However, if the field size_average is set to False, the losses are instead summed for each minibatch. \n",
    "# Ignored if reduce is False.\n",
    "# Set size_average to False and divide the loss by the number of non-padding tokens.\n",
    "# ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient. \n",
    "# When size_average is True, the loss is averaged over non-ignored targets.\n",
    "# Set ignore_index to the padding value\n",
    "loss_function = nn.CrossEntropyLoss(size_average=False, ignore_index=0).cuda() # 0 is the index of <pad>###\n",
    "\n",
    "def train_model(batch):\n",
    "    loss = 0\n",
    "    # Clear optimizer gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Clear hidden state of LSTM\n",
    "    encoder.hidden = encoder.init_hidden(batch_size)\n",
    "    # articles, abstracts are LongTensor vairables of shape (max_sequence_length, batch_size)\n",
    "    # containig word indices from the respective vocabs\n",
    "    # lengths are LongTensor varibles of shape batch_size containing\n",
    "    # lengths of all the sequences in the batch\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    hiddenT, output = encoder(articles, article_lengths)\n",
    "    \n",
    "    # Seperate hidden states corresponding to the the two layers of the encoder\n",
    "    # and append to hidden state list of decoder as tuples for each layer.\n",
    "    for layer in range(hidden_layers):\n",
    "        decoder.hidden_list.append((hiddenT[0][layer], hiddenT[1][layer]))\n",
    "    #input = Variable(torch.cuda.LongTensor(batch_size).fill_(2)) # 2 is the index of <sos>\n",
    "    input = most_likely(output, batch_size)\n",
    "\n",
    "    # Looping over all the sequences\n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input)\n",
    "        input = most_likely(output, batch_size)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "        \n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    loss.backward()\n",
    "    \n",
    "    nn.utils.clip_grad_norm(encoder.parameters(), 5)\n",
    "    nn.utils.clip_grad_norm(decoder.parameters(), 5)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    # Initialize hidden_list for next batch of inputs\n",
    "    decoder.hidden_list = []\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(df):\n",
    "    batch_size = 1\n",
    "    generator = BatchGenerator(batch_size, df)\n",
    "    loss = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch = generator.get_batch()\n",
    "            step += 1\n",
    "        except StopIteration: break\n",
    "        loss += calc_loss(batch, batch_size)\n",
    "    loss = loss/step\n",
    "    return loss\n",
    "\n",
    "def calc_loss(batch, batch_size):\n",
    "    loss = 0\n",
    "    encoder.hidden = encoder.init_hidden(batch_size, volatile=True)\n",
    "    articles, article_lengths = batch.article\n",
    "    abstracts, abstract_lengths = batch.abstract\n",
    "    \n",
    "    articles.volatile = True\n",
    "    abstracts.volatile = True\n",
    "        \n",
    "    hiddenT, output = encoder(articles, article_lengths) ###\n",
    "    for layer in range(hidden_layers):\n",
    "        decoder.hidden_list.append((hiddenT[0][layer], hiddenT[1][layer])) \n",
    "    #input = Variable(torch.cuda.LongTensor(batch_size).fill_(2), volatile=True)\n",
    "    input = most_likely(output, batch_size)\n",
    "    \n",
    "    for t in range(torch.max(abstract_lengths)):\n",
    "        output = decoder(input)\n",
    "        input = most_likely(output, batch_size)\n",
    "        loss += loss_function(output, abstracts[t])\n",
    "    loss = loss/torch.sum(abstract_lengths)\n",
    "    decoder.hidden_list = []\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely(output, batch_size):\n",
    "    if batch_size > 1:\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        output = softmax(output)\n",
    "        _, next_input = torch.topk(output, 1, dim=1)\n",
    "    else: \n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        output = softmax(output)\n",
    "        _, next_input = torch.topk(output, 1)\n",
    "    return next_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate: 0.085000\n",
      "\n",
      "Average minibatch loss at step 2: 6.886\n",
      "Average minibatch loss at step 4: 6.853\n",
      "Average minibatch loss at step 6: 6.821\n",
      "Average minibatch loss at step 8: 6.788\n",
      "Validation loss: 6.768\n",
      "Average minibatch loss at step 10: 6.755\n",
      "Average minibatch loss at step 12: 6.721\n",
      "Average minibatch loss at step 14: 6.687\n",
      "Average minibatch loss at step 16: 6.652\n",
      "Validation loss: 6.624\n",
      "Average minibatch loss at step 18: 6.616\n",
      "Average minibatch loss at step 20: 6.580\n",
      "Average minibatch loss at step 22: 6.543\n",
      "Average minibatch loss at step 24: 6.504\n",
      "Validation loss: 6.467\n",
      "Average minibatch loss at step 26: 6.464\n",
      "Average minibatch loss at step 28: 6.422\n",
      "Average minibatch loss at step 30: 6.379\n",
      "Average minibatch loss at step 32: 6.333\n",
      "Validation loss: 6.285\n",
      "Average minibatch loss at step 34: 6.285\n",
      "Average minibatch loss at step 36: 6.234\n",
      "Average minibatch loss at step 38: 6.180\n",
      "Average minibatch loss at step 40: 6.122\n",
      "Validation loss: 6.057\n",
      "Average minibatch loss at step 42: 6.059\n",
      "Average minibatch loss at step 44: 5.991\n",
      "Average minibatch loss at step 46: 5.917\n",
      "Average minibatch loss at step 48: 5.836\n",
      "Validation loss: 5.746\n",
      "Average minibatch loss at step 50: 5.747\n",
      "Average minibatch loss at step 52: 5.649\n",
      "Average minibatch loss at step 54: 5.542\n",
      "Average minibatch loss at step 56: 5.428\n",
      "Validation loss: 5.307\n",
      "Average minibatch loss at step 58: 5.312\n",
      "Average minibatch loss at step 60: 5.203\n",
      "Average minibatch loss at step 62: 5.115\n",
      "Average minibatch loss at step 64: 5.053\n",
      "Validation loss: 4.950\n",
      "Average minibatch loss at step 66: 5.012\n",
      "Average minibatch loss at step 68: 4.983\n",
      "Average minibatch loss at step 70: 4.957\n",
      "Average minibatch loss at step 72: 4.933\n",
      "Validation loss: 4.837\n",
      "Average minibatch loss at step 74: 4.909\n",
      "Average minibatch loss at step 76: 4.886\n",
      "Average minibatch loss at step 78: 4.862\n",
      "Average minibatch loss at step 80: 4.838\n",
      "Validation loss: 4.744\n",
      "Average minibatch loss at step 82: 4.815\n",
      "Average minibatch loss at step 84: 4.791\n",
      "Average minibatch loss at step 86: 4.768\n",
      "Average minibatch loss at step 88: 4.745\n",
      "Validation loss: 4.653\n",
      "Average minibatch loss at step 90: 4.722\n",
      "Average minibatch loss at step 92: 4.699\n",
      "Average minibatch loss at step 94: 4.676\n",
      "Average minibatch loss at step 96: 4.654\n",
      "Validation loss: 4.568\n",
      "Average minibatch loss at step 98: 4.633\n",
      "Average minibatch loss at step 100: 4.613\n",
      "Average minibatch loss at step 102: 4.593\n",
      "Average minibatch loss at step 104: 4.575\n",
      "Validation loss: 4.495\n",
      "Average minibatch loss at step 106: 4.557\n",
      "Average minibatch loss at step 108: 4.541\n",
      "Average minibatch loss at step 110: 4.526\n",
      "Average minibatch loss at step 112: 4.512\n",
      "Validation loss: 4.439\n",
      "Average minibatch loss at step 114: 4.498\n",
      "Average minibatch loss at step 116: 4.485\n",
      "Average minibatch loss at step 118: 4.472\n",
      "Average minibatch loss at step 120: 4.459\n",
      "Validation loss: 4.392\n",
      "Average minibatch loss at step 122: 4.447\n",
      "Average minibatch loss at step 124: 4.434\n",
      "Average minibatch loss at step 126: 4.421\n",
      "Average minibatch loss at step 128: 4.409\n",
      "Validation loss: 4.346\n",
      "Average minibatch loss at step 130: 4.396\n",
      "Average minibatch loss at step 132: 4.383\n",
      "Average minibatch loss at step 134: 4.369\n",
      "Average minibatch loss at step 136: 4.356\n",
      "Validation loss: 4.299\n",
      "Average minibatch loss at step 138: 4.342\n",
      "Average minibatch loss at step 140: 4.329\n",
      "Average minibatch loss at step 142: 4.315\n",
      "Average minibatch loss at step 144: 4.301\n",
      "Validation loss: 4.249\n",
      "Average minibatch loss at step 146: 4.287\n",
      "Average minibatch loss at step 148: 4.272\n",
      "Average minibatch loss at step 150: 4.258\n",
      "Average minibatch loss at step 152: 4.243\n",
      "Validation loss: 4.198\n",
      "Average minibatch loss at step 154: 4.229\n",
      "Average minibatch loss at step 156: 4.214\n",
      "Average minibatch loss at step 158: 4.199\n",
      "Average minibatch loss at step 160: 4.184\n",
      "Validation loss: 4.146\n",
      "Average minibatch loss at step 162: 4.169\n",
      "Average minibatch loss at step 164: 4.154\n",
      "Average minibatch loss at step 166: 4.140\n",
      "Average minibatch loss at step 168: 4.125\n",
      "Validation loss: 4.094\n",
      "Average minibatch loss at step 170: 4.111\n",
      "Average minibatch loss at step 172: 4.096\n",
      "Average minibatch loss at step 174: 4.082\n",
      "Average minibatch loss at step 176: 4.068\n",
      "Validation loss: 4.044\n",
      "Average minibatch loss at step 178: 4.055\n",
      "Average minibatch loss at step 180: 4.042\n",
      "Average minibatch loss at step 182: 4.029\n",
      "Average minibatch loss at step 184: 4.016\n",
      "Validation loss: 3.998\n",
      "Average minibatch loss at step 186: 4.004\n",
      "Average minibatch loss at step 188: 3.992\n",
      "Average minibatch loss at step 190: 3.980\n",
      "Average minibatch loss at step 192: 3.969\n",
      "Validation loss: 3.957\n",
      "Average minibatch loss at step 194: 3.958\n",
      "Average minibatch loss at step 196: 3.948\n",
      "Average minibatch loss at step 198: 3.937\n",
      "Average minibatch loss at step 200: 3.928\n",
      "Validation loss: 3.920\n",
      "Average minibatch loss at step 202: 3.918\n",
      "Average minibatch loss at step 204: 3.909\n",
      "Average minibatch loss at step 206: 3.900\n",
      "Average minibatch loss at step 208: 3.891\n",
      "Validation loss: 3.889\n",
      "Average minibatch loss at step 210: 3.883\n",
      "Average minibatch loss at step 212: 3.875\n",
      "Average minibatch loss at step 214: 3.867\n",
      "Average minibatch loss at step 216: 3.860\n",
      "Validation loss: 3.864\n",
      "Average minibatch loss at step 218: 3.852\n",
      "Average minibatch loss at step 220: 3.845\n",
      "Average minibatch loss at step 222: 3.839\n",
      "Average minibatch loss at step 224: 3.832\n",
      "Validation loss: 3.843\n",
      "Average minibatch loss at step 226: 3.826\n",
      "Average minibatch loss at step 228: 3.819\n",
      "Average minibatch loss at step 230: 3.813\n",
      "Average minibatch loss at step 232: 3.808\n",
      "Validation loss: 3.827\n",
      "Average minibatch loss at step 234: 3.802\n",
      "Average minibatch loss at step 236: 3.796\n",
      "Average minibatch loss at step 238: 3.791\n",
      "Average minibatch loss at step 240: 3.786\n",
      "Validation loss: 3.815\n",
      "Average minibatch loss at step 242: 3.780\n",
      "Average minibatch loss at step 244: 3.775\n",
      "Average minibatch loss at step 246: 3.770\n",
      "Average minibatch loss at step 248: 3.765\n",
      "Validation loss: 3.807\n",
      "Average minibatch loss at step 250: 3.761\n",
      "Average minibatch loss at step 252: 3.756\n",
      "Average minibatch loss at step 254: 3.751\n",
      "Average minibatch loss at step 256: 3.746\n",
      "Validation loss: 3.800\n",
      "Average minibatch loss at step 258: 3.742\n",
      "Average minibatch loss at step 260: 3.737\n",
      "Average minibatch loss at step 262: 3.733\n",
      "Average minibatch loss at step 264: 3.728\n",
      "Validation loss: 3.796\n",
      "Average minibatch loss at step 266: 3.724\n",
      "Average minibatch loss at step 268: 3.720\n",
      "Average minibatch loss at step 270: 3.715\n",
      "Average minibatch loss at step 272: 3.711\n",
      "Validation loss: 3.792\n",
      "Average minibatch loss at step 274: 3.707\n",
      "Average minibatch loss at step 276: 3.702\n",
      "Average minibatch loss at step 278: 3.698\n",
      "Average minibatch loss at step 280: 3.694\n",
      "Validation loss: 3.790\n",
      "Average minibatch loss at step 282: 3.689\n",
      "Average minibatch loss at step 284: 3.685\n",
      "Average minibatch loss at step 286: 3.681\n",
      "Average minibatch loss at step 288: 3.677\n",
      "Validation loss: 3.788\n",
      "Average minibatch loss at step 290: 3.673\n",
      "Average minibatch loss at step 292: 3.669\n",
      "Average minibatch loss at step 294: 3.664\n",
      "Average minibatch loss at step 296: 3.660\n",
      "Validation loss: 3.786\n",
      "Average minibatch loss at step 298: 3.656\n",
      "Average minibatch loss at step 300: 3.652\n",
      "Average minibatch loss at step 302: 3.648\n",
      "Average minibatch loss at step 304: 3.644\n",
      "Validation loss: 3.785\n",
      "Average minibatch loss at step 306: 3.640\n",
      "Average minibatch loss at step 308: 3.635\n",
      "Average minibatch loss at step 310: 3.631\n",
      "Average minibatch loss at step 312: 3.627\n",
      "Validation loss: 3.785\n",
      "Average minibatch loss at step 314: 3.623\n",
      "Average minibatch loss at step 316: 3.619\n",
      "Average minibatch loss at step 318: 3.615\n",
      "Average minibatch loss at step 320: 3.611\n",
      "Validation loss: 3.784\n",
      "Average minibatch loss at step 322: 3.607\n",
      "Average minibatch loss at step 324: 3.603\n",
      "Average minibatch loss at step 326: 3.599\n",
      "Average minibatch loss at step 328: 3.594\n",
      "Validation loss: 3.784\n",
      "Average minibatch loss at step 330: 3.590\n",
      "Average minibatch loss at step 332: 3.586\n",
      "Average minibatch loss at step 334: 3.582\n",
      "Average minibatch loss at step 336: 3.578\n",
      "Validation loss: 3.784\n",
      "Average minibatch loss at step 338: 3.574\n",
      "Average minibatch loss at step 340: 3.569\n",
      "Average minibatch loss at step 342: 3.565\n",
      "Average minibatch loss at step 344: 3.561\n",
      "Validation loss: 3.783\n",
      "Average minibatch loss at step 346: 3.557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 348: 3.553\n",
      "Average minibatch loss at step 350: 3.548\n",
      "Average minibatch loss at step 352: 3.544\n",
      "Validation loss: 3.783\n",
      "Average minibatch loss at step 354: 3.540\n",
      "Average minibatch loss at step 356: 3.535\n",
      "Average minibatch loss at step 358: 3.531\n",
      "Average minibatch loss at step 360: 3.527\n",
      "Validation loss: 3.782\n",
      "Average minibatch loss at step 362: 3.522\n",
      "Average minibatch loss at step 364: 3.518\n",
      "Average minibatch loss at step 366: 3.513\n",
      "Average minibatch loss at step 368: 3.509\n",
      "Validation loss: 3.780\n",
      "Average minibatch loss at step 370: 3.504\n",
      "Average minibatch loss at step 372: 3.499\n",
      "Average minibatch loss at step 374: 3.495\n",
      "Average minibatch loss at step 376: 3.490\n",
      "Validation loss: 3.778\n",
      "Average minibatch loss at step 378: 3.485\n",
      "Average minibatch loss at step 380: 3.480\n",
      "Average minibatch loss at step 382: 3.476\n",
      "Average minibatch loss at step 384: 3.471\n",
      "Validation loss: 3.776\n",
      "Average minibatch loss at step 386: 3.466\n",
      "Average minibatch loss at step 388: 3.461\n",
      "Average minibatch loss at step 390: 3.456\n",
      "Average minibatch loss at step 392: 3.451\n",
      "Validation loss: 3.772\n",
      "Average minibatch loss at step 394: 3.445\n",
      "Average minibatch loss at step 396: 3.440\n",
      "Average minibatch loss at step 398: 3.435\n",
      "Average minibatch loss at step 400: 3.430\n",
      "Validation loss: 3.768\n",
      "Average minibatch loss at step 402: 3.424\n",
      "Average minibatch loss at step 404: 3.419\n",
      "Average minibatch loss at step 406: 3.414\n",
      "Average minibatch loss at step 408: 3.409\n",
      "Validation loss: 3.767\n",
      "Average minibatch loss at step 410: 3.431\n",
      "Average minibatch loss at step 412: 3.431\n",
      "Average minibatch loss at step 414: 3.448\n",
      "Average minibatch loss at step 416: 3.416\n",
      "Validation loss: 3.774\n",
      "Average minibatch loss at step 418: 3.409\n",
      "Average minibatch loss at step 420: 3.431\n",
      "Average minibatch loss at step 422: 3.403\n",
      "Average minibatch loss at step 424: 3.400\n",
      "Validation loss: 3.779\n",
      "Average minibatch loss at step 426: 3.421\n",
      "Average minibatch loss at step 428: 3.394\n",
      "Average minibatch loss at step 430: 3.396\n",
      "Average minibatch loss at step 432: 3.402\n",
      "Validation loss: 3.783\n",
      "Average minibatch loss at step 434: 3.391\n",
      "Average minibatch loss at step 436: 3.397\n",
      "Average minibatch loss at step 438: 3.381\n",
      "Average minibatch loss at step 440: 3.392\n",
      "Validation loss: 3.784\n",
      "Average minibatch loss at step 442: 3.374\n",
      "Average minibatch loss at step 444: 3.385\n",
      "Average minibatch loss at step 446: 3.370\n",
      "Average minibatch loss at step 448: 3.379\n",
      "Validation loss: 3.786\n",
      "Average minibatch loss at step 450: 3.366\n",
      "Average minibatch loss at step 452: 3.373\n",
      "Average minibatch loss at step 454: 3.361\n",
      "Average minibatch loss at step 456: 3.366\n",
      "Validation loss: 3.786\n",
      "Average minibatch loss at step 458: 3.357\n",
      "Average minibatch loss at step 460: 3.360\n",
      "Average minibatch loss at step 462: 3.353\n",
      "Average minibatch loss at step 464: 3.354\n",
      "Validation loss: 3.787\n",
      "Average minibatch loss at step 466: 3.349\n",
      "Average minibatch loss at step 468: 3.348\n",
      "Average minibatch loss at step 470: 3.345\n",
      "Average minibatch loss at step 472: 3.343\n",
      "Validation loss: 3.788\n",
      "Average minibatch loss at step 474: 3.340\n",
      "Average minibatch loss at step 476: 3.338\n",
      "Average minibatch loss at step 478: 3.336\n",
      "Average minibatch loss at step 480: 3.333\n",
      "Validation loss: 3.790\n",
      "Average minibatch loss at step 482: 3.331\n",
      "Average minibatch loss at step 484: 3.329\n",
      "Average minibatch loss at step 486: 3.327\n",
      "Average minibatch loss at step 488: 3.324\n",
      "Validation loss: 3.792\n",
      "Average minibatch loss at step 490: 3.322\n",
      "Average minibatch loss at step 492: 3.320\n",
      "Average minibatch loss at step 494: 3.318\n",
      "Average minibatch loss at step 496: 3.316\n",
      "Validation loss: 3.795\n",
      "Average minibatch loss at step 498: 3.314\n",
      "Average minibatch loss at step 500: 3.312\n",
      "Average minibatch loss at step 502: 3.310\n",
      "Average minibatch loss at step 504: 3.308\n",
      "Validation loss: 3.797\n",
      "Average minibatch loss at step 506: 3.306\n",
      "Average minibatch loss at step 508: 3.304\n",
      "Average minibatch loss at step 510: 3.302\n",
      "Average minibatch loss at step 512: 3.300\n",
      "Validation loss: 3.800\n",
      "Average minibatch loss at step 514: 3.299\n",
      "Average minibatch loss at step 516: 3.297\n",
      "Average minibatch loss at step 518: 3.295\n",
      "Average minibatch loss at step 520: 3.293\n",
      "Validation loss: 3.804\n",
      "Average minibatch loss at step 522: 3.292\n",
      "Average minibatch loss at step 524: 3.290\n",
      "Average minibatch loss at step 526: 3.288\n",
      "Average minibatch loss at step 528: 3.287\n",
      "Validation loss: 3.807\n",
      "Average minibatch loss at step 530: 3.285\n",
      "Average minibatch loss at step 532: 3.283\n",
      "Average minibatch loss at step 534: 3.282\n",
      "Average minibatch loss at step 536: 3.280\n",
      "Validation loss: 3.810\n",
      "Average minibatch loss at step 538: 3.279\n",
      "Average minibatch loss at step 540: 3.277\n",
      "Average minibatch loss at step 542: 3.276\n",
      "Average minibatch loss at step 544: 3.274\n",
      "Validation loss: 3.814\n",
      "Average minibatch loss at step 546: 3.273\n",
      "Average minibatch loss at step 548: 3.271\n",
      "Average minibatch loss at step 550: 3.270\n",
      "Average minibatch loss at step 552: 3.268\n",
      "Validation loss: 3.817\n",
      "Average minibatch loss at step 554: 3.267\n",
      "Average minibatch loss at step 556: 3.265\n",
      "Average minibatch loss at step 558: 3.264\n",
      "Average minibatch loss at step 560: 3.262\n",
      "Validation loss: 3.820\n",
      "Average minibatch loss at step 562: 3.261\n",
      "Average minibatch loss at step 564: 3.259\n",
      "Average minibatch loss at step 566: 3.258\n",
      "Average minibatch loss at step 568: 3.257\n",
      "Validation loss: 3.824\n",
      "Average minibatch loss at step 570: 3.255\n",
      "Average minibatch loss at step 572: 3.254\n",
      "Average minibatch loss at step 574: 3.252\n",
      "Average minibatch loss at step 576: 3.251\n",
      "Validation loss: 3.827\n",
      "Average minibatch loss at step 578: 3.250\n",
      "Average minibatch loss at step 580: 3.248\n",
      "Average minibatch loss at step 582: 3.247\n",
      "Average minibatch loss at step 584: 3.245\n",
      "Validation loss: 3.830\n",
      "Average minibatch loss at step 586: 3.244\n",
      "Average minibatch loss at step 588: 3.243\n",
      "Average minibatch loss at step 590: 3.241\n",
      "Average minibatch loss at step 592: 3.240\n",
      "Validation loss: 3.833\n",
      "Average minibatch loss at step 594: 3.238\n",
      "Average minibatch loss at step 596: 3.237\n",
      "Average minibatch loss at step 598: 3.236\n",
      "Average minibatch loss at step 600: 3.234\n",
      "Validation loss: 3.836\n",
      "Average minibatch loss at step 602: 3.233\n",
      "Average minibatch loss at step 604: 3.232\n",
      "Average minibatch loss at step 606: 3.230\n",
      "Average minibatch loss at step 608: 3.229\n",
      "Validation loss: 3.839\n",
      "Average minibatch loss at step 610: 3.227\n",
      "Average minibatch loss at step 612: 3.226\n",
      "Average minibatch loss at step 614: 3.225\n",
      "Average minibatch loss at step 616: 3.223\n",
      "Validation loss: 3.841\n",
      "Average minibatch loss at step 618: 3.222\n",
      "Average minibatch loss at step 620: 3.221\n",
      "Average minibatch loss at step 622: 3.219\n",
      "Average minibatch loss at step 624: 3.218\n",
      "Validation loss: 3.844\n",
      "Average minibatch loss at step 626: 3.216\n",
      "Average minibatch loss at step 628: 3.215\n",
      "Average minibatch loss at step 630: 3.214\n",
      "Average minibatch loss at step 632: 3.212\n",
      "Validation loss: 3.847\n",
      "Average minibatch loss at step 634: 3.211\n",
      "Average minibatch loss at step 636: 3.209\n",
      "Average minibatch loss at step 638: 3.208\n",
      "Average minibatch loss at step 640: 3.207\n",
      "Validation loss: 3.849\n",
      "Average minibatch loss at step 642: 3.205\n",
      "Average minibatch loss at step 644: 3.204\n",
      "Average minibatch loss at step 646: 3.203\n",
      "Average minibatch loss at step 648: 3.201\n",
      "Validation loss: 3.852\n",
      "Average minibatch loss at step 650: 3.200\n",
      "Average minibatch loss at step 652: 3.198\n",
      "Average minibatch loss at step 654: 3.197\n",
      "Average minibatch loss at step 656: 3.196\n",
      "Validation loss: 3.854\n",
      "Average minibatch loss at step 658: 3.194\n",
      "Average minibatch loss at step 660: 3.193\n",
      "Average minibatch loss at step 662: 3.191\n",
      "Average minibatch loss at step 664: 3.190\n",
      "Validation loss: 3.857\n",
      "Average minibatch loss at step 666: 3.189\n",
      "Average minibatch loss at step 668: 3.187\n",
      "Average minibatch loss at step 670: 3.186\n",
      "Average minibatch loss at step 672: 3.185\n",
      "Validation loss: 3.859\n",
      "Average minibatch loss at step 674: 3.183\n",
      "Average minibatch loss at step 676: 3.182\n",
      "Average minibatch loss at step 678: 3.180\n",
      "Average minibatch loss at step 680: 3.179\n",
      "Validation loss: 3.862\n",
      "Average minibatch loss at step 682: 3.178\n",
      "Average minibatch loss at step 684: 3.176\n",
      "Average minibatch loss at step 686: 3.175\n",
      "Average minibatch loss at step 688: 3.173\n",
      "Validation loss: 3.864\n",
      "Average minibatch loss at step 690: 3.172\n",
      "Average minibatch loss at step 692: 3.171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 694: 3.169\n",
      "Average minibatch loss at step 696: 3.168\n",
      "Validation loss: 3.866\n",
      "Average minibatch loss at step 698: 3.166\n",
      "Average minibatch loss at step 700: 3.165\n",
      "Average minibatch loss at step 702: 3.164\n",
      "Average minibatch loss at step 704: 3.162\n",
      "Validation loss: 3.869\n",
      "Average minibatch loss at step 706: 3.161\n",
      "Average minibatch loss at step 708: 3.159\n",
      "Average minibatch loss at step 710: 3.158\n",
      "Average minibatch loss at step 712: 3.157\n",
      "Validation loss: 3.871\n",
      "Average minibatch loss at step 714: 3.155\n",
      "Average minibatch loss at step 716: 3.154\n",
      "Average minibatch loss at step 718: 3.152\n",
      "Average minibatch loss at step 720: 3.151\n",
      "Validation loss: 3.873\n",
      "Average minibatch loss at step 722: 3.150\n",
      "Average minibatch loss at step 724: 3.148\n",
      "Average minibatch loss at step 726: 3.147\n",
      "Average minibatch loss at step 728: 3.146\n",
      "Validation loss: 3.876\n",
      "Average minibatch loss at step 730: 3.144\n",
      "Average minibatch loss at step 732: 3.143\n",
      "Average minibatch loss at step 734: 3.141\n",
      "Average minibatch loss at step 736: 3.140\n",
      "Validation loss: 3.878\n",
      "Average minibatch loss at step 738: 3.139\n",
      "Average minibatch loss at step 740: 3.137\n",
      "Average minibatch loss at step 742: 3.136\n",
      "Average minibatch loss at step 744: 3.134\n",
      "Validation loss: 3.881\n",
      "Average minibatch loss at step 746: 3.133\n",
      "Average minibatch loss at step 748: 3.132\n",
      "Average minibatch loss at step 750: 3.130\n",
      "Average minibatch loss at step 752: 3.129\n",
      "Validation loss: 3.884\n",
      "Average minibatch loss at step 754: 3.128\n",
      "Average minibatch loss at step 756: 3.126\n",
      "Average minibatch loss at step 758: 3.125\n",
      "Average minibatch loss at step 760: 3.123\n",
      "Validation loss: 3.886\n",
      "Average minibatch loss at step 762: 3.122\n",
      "Average minibatch loss at step 764: 3.121\n",
      "Average minibatch loss at step 766: 3.119\n",
      "Average minibatch loss at step 768: 3.118\n",
      "Validation loss: 3.889\n",
      "Average minibatch loss at step 770: 3.117\n",
      "Average minibatch loss at step 772: 3.115\n",
      "Average minibatch loss at step 774: 3.114\n",
      "Average minibatch loss at step 776: 3.113\n",
      "Validation loss: 3.892\n",
      "Average minibatch loss at step 778: 3.111\n",
      "Average minibatch loss at step 780: 3.110\n",
      "Average minibatch loss at step 782: 3.109\n",
      "Average minibatch loss at step 784: 3.107\n",
      "Validation loss: 3.895\n",
      "Average minibatch loss at step 786: 3.106\n",
      "Average minibatch loss at step 788: 3.105\n",
      "Average minibatch loss at step 790: 3.103\n",
      "Average minibatch loss at step 792: 3.102\n",
      "Validation loss: 3.898\n",
      "Average minibatch loss at step 794: 3.101\n",
      "Average minibatch loss at step 796: 3.099\n",
      "Average minibatch loss at step 798: 3.098\n",
      "Average minibatch loss at step 800: 3.097\n",
      "Validation loss: 3.902\n",
      "Average minibatch loss at step 802: 3.095\n",
      "Average minibatch loss at step 804: 3.094\n",
      "Average minibatch loss at step 806: 3.093\n",
      "Average minibatch loss at step 808: 3.091\n",
      "Validation loss: 3.905\n",
      "Average minibatch loss at step 810: 3.090\n",
      "Average minibatch loss at step 812: 3.089\n",
      "Average minibatch loss at step 814: 3.088\n",
      "Average minibatch loss at step 816: 3.086\n",
      "Validation loss: 3.909\n",
      "Average minibatch loss at step 818: 3.085\n",
      "Average minibatch loss at step 820: 3.084\n",
      "Average minibatch loss at step 822: 3.082\n",
      "Average minibatch loss at step 824: 3.081\n",
      "Validation loss: 3.912\n",
      "Average minibatch loss at step 826: 3.080\n",
      "Average minibatch loss at step 828: 3.078\n",
      "Average minibatch loss at step 830: 3.077\n",
      "Average minibatch loss at step 832: 3.076\n",
      "Validation loss: 3.916\n",
      "Average minibatch loss at step 834: 3.075\n",
      "Average minibatch loss at step 836: 3.073\n",
      "Average minibatch loss at step 838: 3.072\n",
      "Average minibatch loss at step 840: 3.071\n",
      "Validation loss: 3.919\n",
      "Average minibatch loss at step 842: 3.070\n",
      "Average minibatch loss at step 844: 3.068\n",
      "Average minibatch loss at step 846: 3.067\n",
      "Average minibatch loss at step 848: 3.066\n",
      "Validation loss: 3.923\n",
      "Average minibatch loss at step 850: 3.065\n",
      "Average minibatch loss at step 852: 3.063\n",
      "Average minibatch loss at step 854: 3.062\n",
      "Average minibatch loss at step 856: 3.061\n",
      "Validation loss: 3.927\n",
      "Average minibatch loss at step 858: 3.060\n",
      "Average minibatch loss at step 860: 3.058\n",
      "Average minibatch loss at step 862: 3.057\n",
      "Average minibatch loss at step 864: 3.056\n",
      "Validation loss: 3.931\n",
      "Average minibatch loss at step 866: 3.055\n",
      "Average minibatch loss at step 868: 3.053\n",
      "Average minibatch loss at step 870: 3.052\n",
      "Average minibatch loss at step 872: 3.051\n",
      "Validation loss: 3.935\n",
      "Average minibatch loss at step 874: 3.050\n",
      "Average minibatch loss at step 876: 3.048\n",
      "Average minibatch loss at step 878: 3.047\n",
      "Average minibatch loss at step 880: 3.046\n",
      "Validation loss: 3.939\n",
      "Average minibatch loss at step 882: 3.045\n",
      "Average minibatch loss at step 884: 3.044\n",
      "Average minibatch loss at step 886: 3.042\n",
      "Average minibatch loss at step 888: 3.041\n",
      "Validation loss: 3.943\n",
      "Average minibatch loss at step 890: 3.040\n",
      "Average minibatch loss at step 892: 3.039\n",
      "Average minibatch loss at step 894: 3.038\n",
      "Average minibatch loss at step 896: 3.036\n",
      "Validation loss: 3.947\n",
      "Average minibatch loss at step 898: 3.035\n",
      "Average minibatch loss at step 900: 3.034\n",
      "Average minibatch loss at step 902: 3.033\n",
      "Average minibatch loss at step 904: 3.032\n",
      "Validation loss: 3.951\n",
      "Average minibatch loss at step 906: 3.030\n",
      "Average minibatch loss at step 908: 3.029\n",
      "Average minibatch loss at step 910: 3.028\n",
      "Average minibatch loss at step 912: 3.027\n",
      "Validation loss: 3.955\n",
      "Average minibatch loss at step 914: 3.026\n",
      "Average minibatch loss at step 916: 3.024\n",
      "Average minibatch loss at step 918: 3.023\n",
      "Average minibatch loss at step 920: 3.022\n",
      "Validation loss: 3.960\n",
      "Average minibatch loss at step 922: 3.021\n",
      "Average minibatch loss at step 924: 3.020\n",
      "Average minibatch loss at step 926: 3.019\n",
      "Average minibatch loss at step 928: 3.017\n",
      "Validation loss: 3.964\n",
      "Average minibatch loss at step 930: 3.016\n",
      "Average minibatch loss at step 932: 3.015\n",
      "Average minibatch loss at step 934: 3.014\n",
      "Average minibatch loss at step 936: 3.013\n",
      "Validation loss: 3.968\n",
      "Average minibatch loss at step 938: 3.012\n",
      "Average minibatch loss at step 940: 3.010\n",
      "Average minibatch loss at step 942: 3.009\n",
      "Average minibatch loss at step 944: 3.008\n",
      "Validation loss: 3.972\n",
      "Average minibatch loss at step 946: 3.007\n",
      "Average minibatch loss at step 948: 3.006\n",
      "Average minibatch loss at step 950: 3.005\n",
      "Average minibatch loss at step 952: 3.004\n",
      "Validation loss: 3.977\n",
      "Average minibatch loss at step 954: 3.003\n",
      "Average minibatch loss at step 956: 3.001\n",
      "Average minibatch loss at step 958: 3.000\n",
      "Average minibatch loss at step 960: 2.999\n",
      "Validation loss: 3.981\n",
      "Average minibatch loss at step 962: 2.998\n",
      "Average minibatch loss at step 964: 2.997\n",
      "Average minibatch loss at step 966: 2.996\n",
      "Average minibatch loss at step 968: 2.995\n",
      "Validation loss: 3.986\n",
      "Average minibatch loss at step 970: 2.994\n",
      "Average minibatch loss at step 972: 2.993\n",
      "Average minibatch loss at step 974: 2.992\n",
      "Average minibatch loss at step 976: 2.991\n",
      "Validation loss: 3.990\n",
      "Average minibatch loss at step 978: 2.990\n",
      "Average minibatch loss at step 980: 2.989\n",
      "Average minibatch loss at step 982: 2.987\n",
      "Average minibatch loss at step 984: 2.986\n",
      "Validation loss: 3.995\n",
      "Average minibatch loss at step 986: 2.985\n",
      "Average minibatch loss at step 988: 2.984\n",
      "Average minibatch loss at step 990: 2.983\n",
      "Average minibatch loss at step 992: 2.982\n",
      "Validation loss: 3.999\n",
      "Average minibatch loss at step 994: 2.981\n",
      "Average minibatch loss at step 996: 2.980\n",
      "Average minibatch loss at step 998: 2.979\n",
      "Average minibatch loss at step 1000: 2.978\n",
      "Validation loss: 4.004\n",
      "Average minibatch loss at step 1002: 2.977\n",
      "Average minibatch loss at step 1004: 2.976\n",
      "Average minibatch loss at step 1006: 2.975\n",
      "Average minibatch loss at step 1008: 2.974\n",
      "Validation loss: 4.008\n",
      "Average minibatch loss at step 1010: 2.973\n",
      "Average minibatch loss at step 1012: 2.972\n",
      "Average minibatch loss at step 1014: 2.971\n",
      "Average minibatch loss at step 1016: 2.970\n",
      "Validation loss: 4.012\n",
      "Average minibatch loss at step 1018: 2.969\n",
      "Average minibatch loss at step 1020: 2.968\n",
      "Average minibatch loss at step 1022: 2.966\n",
      "Average minibatch loss at step 1024: 2.965\n",
      "Validation loss: 4.017\n",
      "Average minibatch loss at step 1026: 2.964\n",
      "Average minibatch loss at step 1028: 2.963\n",
      "Average minibatch loss at step 1030: 2.962\n",
      "Average minibatch loss at step 1032: 2.961\n",
      "Validation loss: 4.021\n",
      "Average minibatch loss at step 1034: 2.960\n",
      "Average minibatch loss at step 1036: 2.959\n",
      "Average minibatch loss at step 1038: 2.958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1040: 2.957\n",
      "Validation loss: 4.026\n",
      "Average minibatch loss at step 1042: 2.956\n",
      "Average minibatch loss at step 1044: 2.955\n",
      "Average minibatch loss at step 1046: 2.954\n",
      "Average minibatch loss at step 1048: 2.953\n",
      "Validation loss: 4.030\n",
      "Average minibatch loss at step 1050: 2.952\n",
      "Average minibatch loss at step 1052: 2.951\n",
      "Average minibatch loss at step 1054: 2.950\n",
      "Average minibatch loss at step 1056: 2.949\n",
      "Validation loss: 4.034\n",
      "Average minibatch loss at step 1058: 2.948\n",
      "Average minibatch loss at step 1060: 2.947\n",
      "Average minibatch loss at step 1062: 2.946\n",
      "Average minibatch loss at step 1064: 2.944\n",
      "Validation loss: 4.038\n",
      "Average minibatch loss at step 1066: 2.943\n",
      "Average minibatch loss at step 1068: 2.942\n",
      "Average minibatch loss at step 1070: 2.941\n",
      "Average minibatch loss at step 1072: 2.940\n",
      "Validation loss: 4.042\n",
      "Average minibatch loss at step 1074: 2.939\n",
      "Average minibatch loss at step 1076: 2.938\n",
      "Average minibatch loss at step 1078: 2.937\n",
      "Average minibatch loss at step 1080: 2.936\n",
      "Validation loss: 4.046\n",
      "Average minibatch loss at step 1082: 2.935\n",
      "Average minibatch loss at step 1084: 2.934\n",
      "Average minibatch loss at step 1086: 2.933\n",
      "Average minibatch loss at step 1088: 2.932\n",
      "Validation loss: 4.050\n",
      "Average minibatch loss at step 1090: 2.931\n",
      "Average minibatch loss at step 1092: 2.930\n",
      "Average minibatch loss at step 1094: 2.928\n",
      "Average minibatch loss at step 1096: 2.927\n",
      "Validation loss: 4.054\n",
      "Average minibatch loss at step 1098: 2.926\n",
      "Average minibatch loss at step 1100: 2.925\n",
      "Average minibatch loss at step 1102: 2.924\n",
      "Average minibatch loss at step 1104: 2.923\n",
      "Validation loss: 4.058\n",
      "Average minibatch loss at step 1106: 2.922\n",
      "Average minibatch loss at step 1108: 2.921\n",
      "Average minibatch loss at step 1110: 2.920\n",
      "Average minibatch loss at step 1112: 2.919\n",
      "Validation loss: 4.062\n",
      "Average minibatch loss at step 1114: 2.918\n",
      "Average minibatch loss at step 1116: 2.917\n",
      "Average minibatch loss at step 1118: 2.916\n",
      "Average minibatch loss at step 1120: 2.914\n",
      "Validation loss: 4.066\n",
      "Average minibatch loss at step 1122: 2.913\n",
      "Average minibatch loss at step 1124: 2.912\n",
      "Average minibatch loss at step 1126: 2.911\n",
      "Average minibatch loss at step 1128: 2.910\n",
      "Validation loss: 4.070\n",
      "Average minibatch loss at step 1130: 2.909\n",
      "Average minibatch loss at step 1132: 2.908\n",
      "Average minibatch loss at step 1134: 2.907\n",
      "Average minibatch loss at step 1136: 2.906\n",
      "Validation loss: 4.073\n",
      "Average minibatch loss at step 1138: 2.905\n",
      "Average minibatch loss at step 1140: 2.904\n",
      "Average minibatch loss at step 1142: 2.903\n",
      "Average minibatch loss at step 1144: 2.901\n",
      "Validation loss: 4.077\n",
      "Average minibatch loss at step 1146: 2.900\n",
      "Average minibatch loss at step 1148: 2.899\n",
      "Average minibatch loss at step 1150: 2.898\n",
      "Average minibatch loss at step 1152: 2.897\n",
      "Validation loss: 4.080\n",
      "Average minibatch loss at step 1154: 2.896\n",
      "Average minibatch loss at step 1156: 2.895\n",
      "Average minibatch loss at step 1158: 2.894\n",
      "Average minibatch loss at step 1160: 2.893\n",
      "Validation loss: 4.084\n",
      "Average minibatch loss at step 1162: 2.892\n",
      "Average minibatch loss at step 1164: 2.890\n",
      "Average minibatch loss at step 1166: 2.889\n",
      "Average minibatch loss at step 1168: 2.888\n",
      "Validation loss: 4.087\n",
      "Average minibatch loss at step 1170: 2.887\n",
      "Average minibatch loss at step 1172: 2.886\n",
      "Average minibatch loss at step 1174: 2.885\n",
      "Average minibatch loss at step 1176: 2.884\n",
      "Validation loss: 4.091\n",
      "Average minibatch loss at step 1178: 2.883\n",
      "Average minibatch loss at step 1180: 2.881\n",
      "Average minibatch loss at step 1182: 2.880\n",
      "Average minibatch loss at step 1184: 2.879\n",
      "Validation loss: 4.093\n",
      "Average minibatch loss at step 1186: 2.877\n",
      "Average minibatch loss at step 1188: 2.875\n",
      "Average minibatch loss at step 1190: 2.873\n",
      "Average minibatch loss at step 1192: 2.871\n",
      "Validation loss: 4.094\n",
      "Average minibatch loss at step 1194: 2.867\n",
      "Average minibatch loss at step 1196: 2.862\n",
      "Average minibatch loss at step 1198: 2.856\n",
      "Average minibatch loss at step 1200: 2.847\n",
      "Validation loss: 4.079\n",
      "Average minibatch loss at step 1202: 2.837\n",
      "Average minibatch loss at step 1204: 2.830\n",
      "Average minibatch loss at step 1206: 2.826\n",
      "Average minibatch loss at step 1208: 2.824\n",
      "Validation loss: 4.073\n",
      "Average minibatch loss at step 1210: 2.823\n",
      "Average minibatch loss at step 1212: 2.823\n",
      "Average minibatch loss at step 1214: 2.824\n",
      "Average minibatch loss at step 1216: 2.825\n",
      "Validation loss: 4.082\n",
      "Average minibatch loss at step 1218: 2.826\n",
      "Average minibatch loss at step 1220: 2.827\n",
      "Average minibatch loss at step 1222: 2.828\n",
      "Average minibatch loss at step 1224: 2.829\n",
      "Validation loss: 4.091\n",
      "Average minibatch loss at step 1226: 2.829\n",
      "Average minibatch loss at step 1228: 2.830\n",
      "Average minibatch loss at step 1230: 2.831\n",
      "Average minibatch loss at step 1232: 2.831\n",
      "Validation loss: 4.098\n",
      "Average minibatch loss at step 1234: 2.831\n",
      "Average minibatch loss at step 1236: 2.831\n",
      "Average minibatch loss at step 1238: 2.831\n",
      "Average minibatch loss at step 1240: 2.831\n",
      "Validation loss: 4.104\n",
      "Average minibatch loss at step 1242: 2.831\n",
      "Average minibatch loss at step 1244: 2.831\n",
      "Average minibatch loss at step 1246: 2.830\n",
      "Average minibatch loss at step 1248: 2.830\n",
      "Validation loss: 4.109\n",
      "Average minibatch loss at step 1250: 2.830\n",
      "Average minibatch loss at step 1252: 2.829\n",
      "Average minibatch loss at step 1254: 2.829\n",
      "Average minibatch loss at step 1256: 2.829\n",
      "Validation loss: 4.114\n",
      "Average minibatch loss at step 1258: 2.828\n",
      "Average minibatch loss at step 1260: 2.828\n",
      "Average minibatch loss at step 1262: 2.827\n",
      "Average minibatch loss at step 1264: 2.827\n",
      "Validation loss: 4.119\n",
      "Average minibatch loss at step 1266: 2.826\n",
      "Average minibatch loss at step 1268: 2.826\n",
      "Average minibatch loss at step 1270: 2.825\n",
      "Average minibatch loss at step 1272: 2.825\n",
      "Validation loss: 4.123\n",
      "Average minibatch loss at step 1274: 2.824\n",
      "Average minibatch loss at step 1276: 2.823\n",
      "Average minibatch loss at step 1278: 2.823\n",
      "Average minibatch loss at step 1280: 2.822\n",
      "Validation loss: 4.127\n",
      "Average minibatch loss at step 1282: 2.822\n",
      "Average minibatch loss at step 1284: 2.821\n",
      "Average minibatch loss at step 1286: 2.821\n",
      "Average minibatch loss at step 1288: 2.820\n",
      "Validation loss: 4.131\n",
      "Average minibatch loss at step 1290: 2.819\n",
      "Average minibatch loss at step 1292: 2.819\n",
      "Average minibatch loss at step 1294: 2.818\n",
      "Average minibatch loss at step 1296: 2.818\n",
      "Validation loss: 4.135\n",
      "Average minibatch loss at step 1298: 2.817\n",
      "Average minibatch loss at step 1300: 2.816\n",
      "Average minibatch loss at step 1302: 2.816\n",
      "Average minibatch loss at step 1304: 2.815\n",
      "Validation loss: 4.139\n",
      "Average minibatch loss at step 1306: 2.815\n",
      "Average minibatch loss at step 1308: 2.814\n",
      "Average minibatch loss at step 1310: 2.813\n",
      "Average minibatch loss at step 1312: 2.813\n",
      "Validation loss: 4.143\n",
      "Average minibatch loss at step 1314: 2.812\n",
      "Average minibatch loss at step 1316: 2.812\n",
      "Average minibatch loss at step 1318: 2.811\n",
      "Average minibatch loss at step 1320: 2.810\n",
      "Validation loss: 4.147\n",
      "Average minibatch loss at step 1322: 2.810\n",
      "Average minibatch loss at step 1324: 2.809\n",
      "Average minibatch loss at step 1326: 2.809\n",
      "Average minibatch loss at step 1328: 2.808\n",
      "Validation loss: 4.150\n",
      "Average minibatch loss at step 1330: 2.807\n",
      "Average minibatch loss at step 1332: 2.807\n",
      "Average minibatch loss at step 1334: 2.806\n",
      "Average minibatch loss at step 1336: 2.806\n",
      "Validation loss: 4.154\n",
      "Average minibatch loss at step 1338: 2.805\n",
      "Average minibatch loss at step 1340: 2.804\n",
      "Average minibatch loss at step 1342: 2.804\n",
      "Average minibatch loss at step 1344: 2.803\n",
      "Validation loss: 4.158\n",
      "Average minibatch loss at step 1346: 2.803\n",
      "Average minibatch loss at step 1348: 2.802\n",
      "Average minibatch loss at step 1350: 2.802\n",
      "Average minibatch loss at step 1352: 2.801\n",
      "Validation loss: 4.162\n",
      "Average minibatch loss at step 1354: 2.800\n",
      "Average minibatch loss at step 1356: 2.800\n",
      "Average minibatch loss at step 1358: 2.799\n",
      "Average minibatch loss at step 1360: 2.799\n",
      "Validation loss: 4.165\n",
      "Average minibatch loss at step 1362: 2.798\n",
      "Average minibatch loss at step 1364: 2.797\n",
      "Average minibatch loss at step 1366: 2.797\n",
      "Average minibatch loss at step 1368: 2.796\n",
      "Validation loss: 4.169\n",
      "Average minibatch loss at step 1370: 2.796\n",
      "Average minibatch loss at step 1372: 2.795\n",
      "Average minibatch loss at step 1374: 2.794\n",
      "Average minibatch loss at step 1376: 2.794\n",
      "Validation loss: 4.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1378: 2.793\n",
      "Average minibatch loss at step 1380: 2.793\n",
      "Average minibatch loss at step 1382: 2.792\n",
      "Average minibatch loss at step 1384: 2.792\n",
      "Validation loss: 4.177\n",
      "Average minibatch loss at step 1386: 2.791\n",
      "Average minibatch loss at step 1388: 2.790\n",
      "Average minibatch loss at step 1390: 2.790\n",
      "Average minibatch loss at step 1392: 2.789\n",
      "Validation loss: 4.180\n",
      "Average minibatch loss at step 1394: 2.789\n",
      "Average minibatch loss at step 1396: 2.788\n",
      "Average minibatch loss at step 1398: 2.788\n",
      "Average minibatch loss at step 1400: 2.787\n",
      "Validation loss: 4.184\n",
      "Average minibatch loss at step 1402: 2.786\n",
      "Average minibatch loss at step 1404: 2.786\n",
      "Average minibatch loss at step 1406: 2.785\n",
      "Average minibatch loss at step 1408: 2.785\n",
      "Validation loss: 4.188\n",
      "Average minibatch loss at step 1410: 2.784\n",
      "Average minibatch loss at step 1412: 2.784\n",
      "Average minibatch loss at step 1414: 2.783\n",
      "Average minibatch loss at step 1416: 2.783\n",
      "Validation loss: 4.191\n",
      "Average minibatch loss at step 1418: 2.782\n",
      "Average minibatch loss at step 1420: 2.781\n",
      "Average minibatch loss at step 1422: 2.781\n",
      "Average minibatch loss at step 1424: 2.780\n",
      "Validation loss: 4.195\n",
      "Average minibatch loss at step 1426: 2.780\n",
      "Average minibatch loss at step 1428: 2.779\n",
      "Average minibatch loss at step 1430: 2.779\n",
      "Average minibatch loss at step 1432: 2.778\n",
      "Validation loss: 4.199\n",
      "Average minibatch loss at step 1434: 2.778\n",
      "Average minibatch loss at step 1436: 2.777\n",
      "Average minibatch loss at step 1438: 2.776\n",
      "Average minibatch loss at step 1440: 2.776\n",
      "Validation loss: 4.202\n",
      "Average minibatch loss at step 1442: 2.775\n",
      "Average minibatch loss at step 1444: 2.775\n",
      "Average minibatch loss at step 1446: 2.774\n",
      "Average minibatch loss at step 1448: 2.774\n",
      "Validation loss: 4.206\n",
      "Average minibatch loss at step 1450: 2.773\n",
      "Average minibatch loss at step 1452: 2.773\n",
      "Average minibatch loss at step 1454: 2.772\n",
      "Average minibatch loss at step 1456: 2.772\n",
      "Validation loss: 4.210\n",
      "Average minibatch loss at step 1458: 2.771\n",
      "Average minibatch loss at step 1460: 2.770\n",
      "Average minibatch loss at step 1462: 2.770\n",
      "Average minibatch loss at step 1464: 2.769\n",
      "Validation loss: 4.213\n",
      "Average minibatch loss at step 1466: 2.769\n",
      "Average minibatch loss at step 1468: 2.768\n",
      "Average minibatch loss at step 1470: 2.768\n",
      "Average minibatch loss at step 1472: 2.767\n",
      "Validation loss: 4.217\n",
      "Average minibatch loss at step 1474: 2.767\n",
      "Average minibatch loss at step 1476: 2.766\n",
      "Average minibatch loss at step 1478: 2.766\n",
      "Average minibatch loss at step 1480: 2.765\n",
      "Validation loss: 4.220\n",
      "Average minibatch loss at step 1482: 2.765\n",
      "Average minibatch loss at step 1484: 2.764\n",
      "Average minibatch loss at step 1486: 2.763\n",
      "Average minibatch loss at step 1488: 2.763\n",
      "Validation loss: 4.224\n",
      "Average minibatch loss at step 1490: 2.762\n",
      "Average minibatch loss at step 1492: 2.762\n",
      "Average minibatch loss at step 1494: 2.761\n",
      "Average minibatch loss at step 1496: 2.761\n",
      "Validation loss: 4.228\n",
      "Average minibatch loss at step 1498: 2.760\n",
      "Average minibatch loss at step 1500: 2.760\n",
      "Average minibatch loss at step 1502: 2.759\n",
      "Average minibatch loss at step 1504: 2.759\n",
      "Validation loss: 4.231\n",
      "Average minibatch loss at step 1506: 2.758\n",
      "Average minibatch loss at step 1508: 2.758\n",
      "Average minibatch loss at step 1510: 2.757\n",
      "Average minibatch loss at step 1512: 2.757\n",
      "Validation loss: 4.235\n",
      "Average minibatch loss at step 1514: 2.756\n",
      "Average minibatch loss at step 1516: 2.756\n",
      "Average minibatch loss at step 1518: 2.755\n",
      "Average minibatch loss at step 1520: 2.755\n",
      "Validation loss: 4.239\n",
      "Average minibatch loss at step 1522: 2.754\n",
      "Average minibatch loss at step 1524: 2.754\n",
      "Average minibatch loss at step 1526: 2.753\n",
      "Average minibatch loss at step 1528: 2.753\n",
      "Validation loss: 4.242\n",
      "Average minibatch loss at step 1530: 2.752\n",
      "Average minibatch loss at step 1532: 2.752\n",
      "Average minibatch loss at step 1534: 2.751\n",
      "Average minibatch loss at step 1536: 2.751\n",
      "Validation loss: 4.246\n",
      "Average minibatch loss at step 1538: 2.750\n",
      "Average minibatch loss at step 1540: 2.750\n",
      "Average minibatch loss at step 1542: 2.749\n",
      "Average minibatch loss at step 1544: 2.749\n",
      "Validation loss: 4.249\n",
      "Average minibatch loss at step 1546: 2.748\n",
      "Average minibatch loss at step 1548: 2.747\n",
      "Average minibatch loss at step 1550: 2.747\n",
      "Average minibatch loss at step 1552: 2.746\n",
      "Validation loss: 4.253\n",
      "Average minibatch loss at step 1554: 2.746\n",
      "Average minibatch loss at step 1556: 2.745\n",
      "Average minibatch loss at step 1558: 2.745\n",
      "Average minibatch loss at step 1560: 2.744\n",
      "Validation loss: 4.257\n",
      "Average minibatch loss at step 1562: 2.744\n",
      "Average minibatch loss at step 1564: 2.743\n",
      "Average minibatch loss at step 1566: 2.743\n",
      "Average minibatch loss at step 1568: 2.742\n",
      "Validation loss: 4.260\n",
      "Average minibatch loss at step 1570: 2.742\n",
      "Average minibatch loss at step 1572: 2.741\n",
      "Average minibatch loss at step 1574: 2.741\n",
      "Average minibatch loss at step 1576: 2.741\n",
      "Validation loss: 4.264\n",
      "Average minibatch loss at step 1578: 2.740\n",
      "Average minibatch loss at step 1580: 2.740\n",
      "Average minibatch loss at step 1582: 2.739\n",
      "Average minibatch loss at step 1584: 2.739\n",
      "Validation loss: 4.267\n",
      "Average minibatch loss at step 1586: 2.738\n",
      "Average minibatch loss at step 1588: 2.738\n",
      "Average minibatch loss at step 1590: 2.737\n",
      "Average minibatch loss at step 1592: 2.737\n",
      "Validation loss: 4.271\n",
      "Average minibatch loss at step 1594: 2.736\n",
      "Average minibatch loss at step 1596: 2.736\n",
      "Average minibatch loss at step 1598: 2.735\n",
      "Average minibatch loss at step 1600: 2.735\n",
      "Validation loss: 4.275\n",
      "Average minibatch loss at step 1602: 2.734\n",
      "Average minibatch loss at step 1604: 2.734\n",
      "Average minibatch loss at step 1606: 2.733\n",
      "Average minibatch loss at step 1608: 2.733\n",
      "Validation loss: 4.278\n",
      "Average minibatch loss at step 1610: 2.732\n",
      "Average minibatch loss at step 1612: 2.732\n",
      "Average minibatch loss at step 1614: 2.731\n",
      "Average minibatch loss at step 1616: 2.731\n",
      "Validation loss: 4.282\n",
      "Average minibatch loss at step 1618: 2.730\n",
      "Average minibatch loss at step 1620: 2.730\n",
      "Average minibatch loss at step 1622: 2.729\n",
      "Average minibatch loss at step 1624: 2.729\n",
      "Validation loss: 4.285\n",
      "Average minibatch loss at step 1626: 2.728\n",
      "Average minibatch loss at step 1628: 2.728\n",
      "Average minibatch loss at step 1630: 2.727\n",
      "Average minibatch loss at step 1632: 2.727\n",
      "Validation loss: 4.289\n",
      "Average minibatch loss at step 1634: 2.726\n",
      "Average minibatch loss at step 1636: 2.726\n",
      "Average minibatch loss at step 1638: 2.725\n",
      "Average minibatch loss at step 1640: 2.725\n",
      "Validation loss: 4.292\n",
      "Average minibatch loss at step 1642: 2.725\n",
      "Average minibatch loss at step 1644: 2.724\n",
      "Average minibatch loss at step 1646: 2.724\n",
      "Average minibatch loss at step 1648: 2.723\n",
      "Validation loss: 4.296\n",
      "Average minibatch loss at step 1650: 2.723\n",
      "Average minibatch loss at step 1652: 2.722\n",
      "Average minibatch loss at step 1654: 2.722\n",
      "Average minibatch loss at step 1656: 2.721\n",
      "Validation loss: 4.300\n",
      "Average minibatch loss at step 1658: 2.721\n",
      "Average minibatch loss at step 1660: 2.720\n",
      "Average minibatch loss at step 1662: 2.720\n",
      "Average minibatch loss at step 1664: 2.719\n",
      "Validation loss: 4.303\n",
      "Average minibatch loss at step 1666: 2.719\n",
      "Average minibatch loss at step 1668: 2.718\n",
      "Average minibatch loss at step 1670: 2.718\n",
      "Average minibatch loss at step 1672: 2.718\n",
      "Validation loss: 4.307\n",
      "Average minibatch loss at step 1674: 2.717\n",
      "Average minibatch loss at step 1676: 2.717\n",
      "Average minibatch loss at step 1678: 2.716\n",
      "Average minibatch loss at step 1680: 2.716\n",
      "Validation loss: 4.310\n",
      "Average minibatch loss at step 1682: 2.715\n",
      "Average minibatch loss at step 1684: 2.715\n",
      "Average minibatch loss at step 1686: 2.714\n",
      "Average minibatch loss at step 1688: 2.714\n",
      "Validation loss: 4.314\n",
      "Average minibatch loss at step 1690: 2.713\n",
      "Average minibatch loss at step 1692: 2.713\n",
      "Average minibatch loss at step 1694: 2.712\n",
      "Average minibatch loss at step 1696: 2.712\n",
      "Validation loss: 4.318\n",
      "Average minibatch loss at step 1698: 2.712\n",
      "Average minibatch loss at step 1700: 2.711\n",
      "Average minibatch loss at step 1702: 2.711\n",
      "Average minibatch loss at step 1704: 2.710\n",
      "Validation loss: 4.321\n",
      "Average minibatch loss at step 1706: 2.710\n",
      "Average minibatch loss at step 1708: 2.709\n",
      "Average minibatch loss at step 1710: 2.709\n",
      "Average minibatch loss at step 1712: 2.708\n",
      "Validation loss: 4.325\n",
      "Average minibatch loss at step 1714: 2.708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 1716: 2.707\n",
      "Average minibatch loss at step 1718: 2.707\n",
      "Average minibatch loss at step 1720: 2.707\n",
      "Validation loss: 4.328\n",
      "Average minibatch loss at step 1722: 2.706\n",
      "Average minibatch loss at step 1724: 2.706\n",
      "Average minibatch loss at step 1726: 2.705\n",
      "Average minibatch loss at step 1728: 2.705\n",
      "Validation loss: 4.332\n",
      "Average minibatch loss at step 1730: 2.704\n",
      "Average minibatch loss at step 1732: 2.704\n",
      "Average minibatch loss at step 1734: 2.703\n",
      "Average minibatch loss at step 1736: 2.703\n",
      "Validation loss: 4.335\n",
      "Average minibatch loss at step 1738: 2.703\n",
      "Average minibatch loss at step 1740: 2.702\n",
      "Average minibatch loss at step 1742: 2.702\n",
      "Average minibatch loss at step 1744: 2.701\n",
      "Validation loss: 4.339\n",
      "Average minibatch loss at step 1746: 2.701\n",
      "Average minibatch loss at step 1748: 2.700\n",
      "Average minibatch loss at step 1750: 2.700\n",
      "Average minibatch loss at step 1752: 2.699\n",
      "Validation loss: 4.343\n",
      "Average minibatch loss at step 1754: 2.699\n",
      "Average minibatch loss at step 1756: 2.699\n",
      "Average minibatch loss at step 1758: 2.698\n",
      "Average minibatch loss at step 1760: 2.698\n",
      "Validation loss: 4.346\n",
      "Average minibatch loss at step 1762: 2.697\n",
      "Average minibatch loss at step 1764: 2.697\n",
      "Average minibatch loss at step 1766: 2.696\n",
      "Average minibatch loss at step 1768: 2.696\n",
      "Validation loss: 4.350\n",
      "Average minibatch loss at step 1770: 2.695\n",
      "Average minibatch loss at step 1772: 2.695\n",
      "Average minibatch loss at step 1774: 2.695\n",
      "Average minibatch loss at step 1776: 2.694\n",
      "Validation loss: 4.353\n",
      "Average minibatch loss at step 1778: 2.694\n",
      "Average minibatch loss at step 1780: 2.693\n",
      "Average minibatch loss at step 1782: 2.693\n",
      "Average minibatch loss at step 1784: 2.692\n",
      "Validation loss: 4.357\n",
      "Average minibatch loss at step 1786: 2.692\n",
      "Average minibatch loss at step 1788: 2.692\n",
      "Average minibatch loss at step 1790: 2.691\n",
      "Average minibatch loss at step 1792: 2.691\n",
      "Validation loss: 4.360\n",
      "Average minibatch loss at step 1794: 2.690\n",
      "Average minibatch loss at step 1796: 2.690\n",
      "Average minibatch loss at step 1798: 2.689\n",
      "Average minibatch loss at step 1800: 2.689\n",
      "Validation loss: 4.364\n",
      "Average minibatch loss at step 1802: 2.689\n",
      "Average minibatch loss at step 1804: 2.688\n",
      "Average minibatch loss at step 1806: 2.688\n",
      "Average minibatch loss at step 1808: 2.687\n",
      "Validation loss: 4.368\n",
      "Average minibatch loss at step 1810: 2.687\n",
      "Average minibatch loss at step 1812: 2.686\n",
      "Average minibatch loss at step 1814: 2.686\n",
      "Average minibatch loss at step 1816: 2.686\n",
      "Validation loss: 4.371\n",
      "Average minibatch loss at step 1818: 2.685\n",
      "Average minibatch loss at step 1820: 2.685\n",
      "Average minibatch loss at step 1822: 2.684\n",
      "Average minibatch loss at step 1824: 2.684\n",
      "Validation loss: 4.375\n",
      "Average minibatch loss at step 1826: 2.683\n",
      "Average minibatch loss at step 1828: 2.683\n",
      "Average minibatch loss at step 1830: 2.683\n",
      "Average minibatch loss at step 1832: 2.682\n",
      "Validation loss: 4.378\n",
      "Average minibatch loss at step 1834: 2.682\n",
      "Average minibatch loss at step 1836: 2.681\n",
      "Average minibatch loss at step 1838: 2.681\n",
      "Average minibatch loss at step 1840: 2.681\n",
      "Validation loss: 4.382\n",
      "Average minibatch loss at step 1842: 2.680\n",
      "Average minibatch loss at step 1844: 2.680\n",
      "Average minibatch loss at step 1846: 2.679\n",
      "Average minibatch loss at step 1848: 2.679\n",
      "Validation loss: 4.385\n",
      "Average minibatch loss at step 1850: 2.678\n",
      "Average minibatch loss at step 1852: 2.678\n",
      "Average minibatch loss at step 1854: 2.678\n",
      "Average minibatch loss at step 1856: 2.677\n",
      "Validation loss: 4.389\n",
      "Average minibatch loss at step 1858: 2.677\n",
      "Average minibatch loss at step 1860: 2.676\n",
      "Average minibatch loss at step 1862: 2.676\n",
      "Average minibatch loss at step 1864: 2.676\n",
      "Validation loss: 4.392\n",
      "Average minibatch loss at step 1866: 2.675\n",
      "Average minibatch loss at step 1868: 2.675\n",
      "Average minibatch loss at step 1870: 2.674\n",
      "Average minibatch loss at step 1872: 2.674\n",
      "Validation loss: 4.396\n",
      "Average minibatch loss at step 1874: 2.674\n",
      "Average minibatch loss at step 1876: 2.673\n",
      "Average minibatch loss at step 1878: 2.673\n",
      "Average minibatch loss at step 1880: 2.672\n",
      "Validation loss: 4.400\n",
      "Average minibatch loss at step 1882: 2.672\n",
      "Average minibatch loss at step 1884: 2.672\n",
      "Average minibatch loss at step 1886: 2.671\n",
      "Average minibatch loss at step 1888: 2.671\n",
      "Validation loss: 4.403\n",
      "Average minibatch loss at step 1890: 2.670\n",
      "Average minibatch loss at step 1892: 2.670\n",
      "Average minibatch loss at step 1894: 2.669\n",
      "Average minibatch loss at step 1896: 2.669\n",
      "Validation loss: 4.407\n",
      "Average minibatch loss at step 1898: 2.669\n",
      "Average minibatch loss at step 1900: 2.668\n",
      "Average minibatch loss at step 1902: 2.668\n",
      "Average minibatch loss at step 1904: 2.667\n",
      "Validation loss: 4.410\n",
      "Average minibatch loss at step 1906: 2.667\n",
      "Average minibatch loss at step 1908: 2.667\n",
      "Average minibatch loss at step 1910: 2.666\n",
      "Average minibatch loss at step 1912: 2.666\n",
      "Validation loss: 4.414\n",
      "Average minibatch loss at step 1914: 2.665\n",
      "Average minibatch loss at step 1916: 2.665\n",
      "Average minibatch loss at step 1918: 2.665\n",
      "Average minibatch loss at step 1920: 2.664\n",
      "Validation loss: 4.417\n",
      "Average minibatch loss at step 1922: 2.664\n",
      "Average minibatch loss at step 1924: 2.663\n",
      "Average minibatch loss at step 1926: 2.663\n",
      "Average minibatch loss at step 1928: 2.663\n",
      "Validation loss: 4.421\n",
      "Average minibatch loss at step 1930: 2.662\n",
      "Average minibatch loss at step 1932: 2.662\n",
      "Average minibatch loss at step 1934: 2.662\n",
      "Average minibatch loss at step 1936: 2.661\n",
      "Validation loss: 4.424\n",
      "Average minibatch loss at step 1938: 2.661\n",
      "Average minibatch loss at step 1940: 2.660\n",
      "Average minibatch loss at step 1942: 2.660\n",
      "Average minibatch loss at step 1944: 2.660\n",
      "Validation loss: 4.428\n",
      "Average minibatch loss at step 1946: 2.659\n",
      "Average minibatch loss at step 1948: 2.659\n",
      "Average minibatch loss at step 1950: 2.658\n",
      "Average minibatch loss at step 1952: 2.658\n",
      "Validation loss: 4.431\n",
      "Average minibatch loss at step 1954: 2.658\n",
      "Average minibatch loss at step 1956: 2.657\n",
      "Average minibatch loss at step 1958: 2.657\n",
      "Average minibatch loss at step 1960: 2.656\n",
      "Validation loss: 4.435\n",
      "Average minibatch loss at step 1962: 2.656\n",
      "Average minibatch loss at step 1964: 2.656\n",
      "Average minibatch loss at step 1966: 2.655\n",
      "Average minibatch loss at step 1968: 2.655\n",
      "Validation loss: 4.438\n",
      "Average minibatch loss at step 1970: 2.655\n",
      "Average minibatch loss at step 1972: 2.654\n",
      "Average minibatch loss at step 1974: 2.654\n",
      "Average minibatch loss at step 1976: 2.653\n",
      "Validation loss: 4.442\n",
      "Average minibatch loss at step 1978: 2.653\n",
      "Average minibatch loss at step 1980: 2.653\n",
      "Average minibatch loss at step 1982: 2.652\n",
      "Average minibatch loss at step 1984: 2.652\n",
      "Validation loss: 4.445\n",
      "Average minibatch loss at step 1986: 2.651\n",
      "Average minibatch loss at step 1988: 2.651\n",
      "Average minibatch loss at step 1990: 2.651\n",
      "Average minibatch loss at step 1992: 2.650\n",
      "Validation loss: 4.449\n",
      "Average minibatch loss at step 1994: 2.650\n",
      "Average minibatch loss at step 1996: 2.650\n",
      "Average minibatch loss at step 1998: 2.649\n",
      "Average minibatch loss at step 2000: 2.649\n",
      "Validation loss: 4.452\n",
      "Average minibatch loss at step 2002: 2.648\n",
      "Average minibatch loss at step 2004: 2.648\n",
      "Average minibatch loss at step 2006: 2.648\n",
      "Average minibatch loss at step 2008: 2.647\n",
      "Validation loss: 4.455\n",
      "Average minibatch loss at step 2010: 2.647\n",
      "Average minibatch loss at step 2012: 2.647\n",
      "Average minibatch loss at step 2014: 2.646\n",
      "Average minibatch loss at step 2016: 2.646\n",
      "Validation loss: 4.459\n",
      "Average minibatch loss at step 2018: 2.645\n",
      "Average minibatch loss at step 2020: 2.645\n",
      "Average minibatch loss at step 2022: 2.645\n",
      "Average minibatch loss at step 2024: 2.644\n",
      "Validation loss: 4.462\n",
      "Average minibatch loss at step 2026: 2.644\n",
      "Average minibatch loss at step 2028: 2.644\n",
      "Average minibatch loss at step 2030: 2.643\n",
      "Average minibatch loss at step 2032: 2.643\n",
      "Validation loss: 4.466\n",
      "Average minibatch loss at step 2034: 2.642\n",
      "Average minibatch loss at step 2036: 2.642\n",
      "Average minibatch loss at step 2038: 2.642\n",
      "Average minibatch loss at step 2040: 2.641\n",
      "Validation loss: 4.469\n",
      "Average minibatch loss at step 2042: 2.641\n",
      "Average minibatch loss at step 2044: 2.641\n",
      "Average minibatch loss at step 2046: 2.640\n",
      "Average minibatch loss at step 2048: 2.640\n",
      "Validation loss: 4.473\n",
      "Average minibatch loss at step 2050: 2.640\n",
      "Average minibatch loss at step 2052: 2.639\n",
      "Average minibatch loss at step 2054: 2.639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 2056: 2.638\n",
      "Validation loss: 4.476\n",
      "Average minibatch loss at step 2058: 2.638\n",
      "Average minibatch loss at step 2060: 2.638\n",
      "Average minibatch loss at step 2062: 2.637\n",
      "Average minibatch loss at step 2064: 2.637\n",
      "Validation loss: 4.479\n",
      "Average minibatch loss at step 2066: 2.637\n",
      "Average minibatch loss at step 2068: 2.636\n",
      "Average minibatch loss at step 2070: 2.636\n",
      "Average minibatch loss at step 2072: 2.636\n",
      "Validation loss: 4.483\n",
      "Average minibatch loss at step 2074: 2.635\n",
      "Average minibatch loss at step 2076: 2.635\n",
      "Average minibatch loss at step 2078: 2.634\n",
      "Average minibatch loss at step 2080: 2.634\n",
      "Validation loss: 4.486\n",
      "Average minibatch loss at step 2082: 2.634\n",
      "Average minibatch loss at step 2084: 2.633\n",
      "Average minibatch loss at step 2086: 2.633\n",
      "Average minibatch loss at step 2088: 2.633\n",
      "Validation loss: 4.490\n",
      "Average minibatch loss at step 2090: 2.632\n",
      "Average minibatch loss at step 2092: 2.632\n",
      "Average minibatch loss at step 2094: 2.632\n",
      "Average minibatch loss at step 2096: 2.631\n",
      "Validation loss: 4.493\n",
      "Average minibatch loss at step 2098: 2.631\n",
      "Average minibatch loss at step 2100: 2.630\n",
      "Average minibatch loss at step 2102: 2.630\n",
      "Average minibatch loss at step 2104: 2.630\n",
      "Validation loss: 4.496\n",
      "Average minibatch loss at step 2106: 2.629\n",
      "Average minibatch loss at step 2108: 2.629\n",
      "Average minibatch loss at step 2110: 2.629\n",
      "Average minibatch loss at step 2112: 2.628\n",
      "Validation loss: 4.500\n",
      "Average minibatch loss at step 2114: 2.628\n",
      "Average minibatch loss at step 2116: 2.628\n",
      "Average minibatch loss at step 2118: 2.627\n",
      "Average minibatch loss at step 2120: 2.627\n",
      "Validation loss: 4.503\n",
      "Average minibatch loss at step 2122: 2.627\n",
      "Average minibatch loss at step 2124: 2.626\n",
      "Average minibatch loss at step 2126: 2.626\n",
      "Average minibatch loss at step 2128: 2.626\n",
      "Validation loss: 4.507\n",
      "Average minibatch loss at step 2130: 2.625\n",
      "Average minibatch loss at step 2132: 2.625\n",
      "Average minibatch loss at step 2134: 2.625\n",
      "Average minibatch loss at step 2136: 2.624\n",
      "Validation loss: 4.510\n",
      "Average minibatch loss at step 2138: 2.624\n",
      "Average minibatch loss at step 2140: 2.623\n",
      "Average minibatch loss at step 2142: 2.623\n",
      "Average minibatch loss at step 2144: 2.623\n",
      "Validation loss: 4.513\n",
      "Average minibatch loss at step 2146: 2.622\n",
      "Average minibatch loss at step 2148: 2.622\n",
      "Average minibatch loss at step 2150: 2.622\n",
      "Average minibatch loss at step 2152: 2.621\n",
      "Validation loss: 4.517\n",
      "Average minibatch loss at step 2154: 2.621\n",
      "Average minibatch loss at step 2156: 2.621\n",
      "Average minibatch loss at step 2158: 2.620\n",
      "Average minibatch loss at step 2160: 2.620\n",
      "Validation loss: 4.520\n",
      "Average minibatch loss at step 2162: 2.620\n",
      "Average minibatch loss at step 2164: 2.619\n",
      "Average minibatch loss at step 2166: 2.619\n",
      "Average minibatch loss at step 2168: 2.619\n",
      "Validation loss: 4.523\n",
      "Average minibatch loss at step 2170: 2.618\n",
      "Average minibatch loss at step 2172: 2.618\n",
      "Average minibatch loss at step 2174: 2.618\n",
      "Average minibatch loss at step 2176: 2.617\n",
      "Validation loss: 4.526\n",
      "Average minibatch loss at step 2178: 2.617\n",
      "Average minibatch loss at step 2180: 2.617\n",
      "Average minibatch loss at step 2182: 2.616\n",
      "Average minibatch loss at step 2184: 2.616\n",
      "Validation loss: 4.530\n",
      "Average minibatch loss at step 2186: 2.616\n",
      "Average minibatch loss at step 2188: 2.615\n",
      "Average minibatch loss at step 2190: 2.615\n",
      "Average minibatch loss at step 2192: 2.615\n",
      "Validation loss: 4.533\n",
      "Average minibatch loss at step 2194: 2.614\n",
      "Average minibatch loss at step 2196: 2.614\n",
      "Average minibatch loss at step 2198: 2.614\n",
      "Average minibatch loss at step 2200: 2.613\n",
      "Validation loss: 4.536\n",
      "Average minibatch loss at step 2202: 2.613\n",
      "Average minibatch loss at step 2204: 2.613\n",
      "Average minibatch loss at step 2206: 2.612\n",
      "Average minibatch loss at step 2208: 2.612\n",
      "Validation loss: 4.540\n",
      "Average minibatch loss at step 2210: 2.612\n",
      "Average minibatch loss at step 2212: 2.611\n",
      "Average minibatch loss at step 2214: 2.611\n",
      "Average minibatch loss at step 2216: 2.611\n",
      "Validation loss: 4.543\n",
      "Average minibatch loss at step 2218: 2.610\n",
      "Average minibatch loss at step 2220: 2.610\n",
      "Average minibatch loss at step 2222: 2.610\n",
      "Average minibatch loss at step 2224: 2.609\n",
      "Validation loss: 4.546\n",
      "Average minibatch loss at step 2226: 2.609\n",
      "Average minibatch loss at step 2228: 2.609\n",
      "Average minibatch loss at step 2230: 2.608\n",
      "Average minibatch loss at step 2232: 2.608\n",
      "Validation loss: 4.549\n",
      "Average minibatch loss at step 2234: 2.608\n",
      "Average minibatch loss at step 2236: 2.607\n",
      "Average minibatch loss at step 2238: 2.607\n",
      "Average minibatch loss at step 2240: 2.607\n",
      "Validation loss: 4.553\n",
      "Average minibatch loss at step 2242: 2.606\n",
      "Average minibatch loss at step 2244: 2.606\n",
      "Average minibatch loss at step 2246: 2.606\n",
      "Average minibatch loss at step 2248: 2.605\n",
      "Validation loss: 4.556\n",
      "Average minibatch loss at step 2250: 2.605\n",
      "Average minibatch loss at step 2252: 2.605\n",
      "Average minibatch loss at step 2254: 2.604\n",
      "Average minibatch loss at step 2256: 2.604\n",
      "Validation loss: 4.559\n",
      "Average minibatch loss at step 2258: 2.604\n",
      "Average minibatch loss at step 2260: 2.603\n",
      "Average minibatch loss at step 2262: 2.603\n",
      "Average minibatch loss at step 2264: 2.603\n",
      "Validation loss: 4.562\n",
      "Average minibatch loss at step 2266: 2.602\n",
      "Average minibatch loss at step 2268: 2.602\n",
      "Average minibatch loss at step 2270: 2.602\n",
      "Average minibatch loss at step 2272: 2.601\n",
      "Validation loss: 4.566\n",
      "Average minibatch loss at step 2274: 2.601\n",
      "Average minibatch loss at step 2276: 2.601\n",
      "Average minibatch loss at step 2278: 2.600\n",
      "Average minibatch loss at step 2280: 2.600\n",
      "Validation loss: 4.569\n",
      "Average minibatch loss at step 2282: 2.600\n",
      "Average minibatch loss at step 2284: 2.599\n",
      "Average minibatch loss at step 2286: 2.599\n",
      "Average minibatch loss at step 2288: 2.599\n",
      "Validation loss: 4.572\n",
      "Average minibatch loss at step 2290: 2.598\n",
      "Average minibatch loss at step 2292: 2.598\n",
      "Average minibatch loss at step 2294: 2.598\n",
      "Average minibatch loss at step 2296: 2.597\n",
      "Validation loss: 4.575\n",
      "Average minibatch loss at step 2298: 2.597\n",
      "Average minibatch loss at step 2300: 2.597\n",
      "Average minibatch loss at step 2302: 2.596\n",
      "Average minibatch loss at step 2304: 2.596\n",
      "Validation loss: 4.578\n",
      "Average minibatch loss at step 2306: 2.596\n",
      "Average minibatch loss at step 2308: 2.596\n",
      "Average minibatch loss at step 2310: 2.595\n",
      "Average minibatch loss at step 2312: 2.595\n",
      "Validation loss: 4.582\n",
      "Average minibatch loss at step 2314: 2.595\n",
      "Average minibatch loss at step 2316: 2.594\n",
      "Average minibatch loss at step 2318: 2.594\n",
      "Average minibatch loss at step 2320: 2.594\n",
      "Validation loss: 4.585\n",
      "Average minibatch loss at step 2322: 2.593\n",
      "Average minibatch loss at step 2324: 2.593\n",
      "Average minibatch loss at step 2326: 2.593\n",
      "Average minibatch loss at step 2328: 2.592\n",
      "Validation loss: 4.588\n",
      "Average minibatch loss at step 2330: 2.592\n",
      "Average minibatch loss at step 2332: 2.592\n",
      "Average minibatch loss at step 2334: 2.591\n",
      "Average minibatch loss at step 2336: 2.591\n",
      "Validation loss: 4.591\n",
      "Average minibatch loss at step 2338: 2.591\n",
      "Average minibatch loss at step 2340: 2.591\n",
      "Average minibatch loss at step 2342: 2.590\n",
      "Average minibatch loss at step 2344: 2.590\n",
      "Validation loss: 4.594\n",
      "Average minibatch loss at step 2346: 2.590\n",
      "Average minibatch loss at step 2348: 2.589\n",
      "Average minibatch loss at step 2350: 2.589\n",
      "Average minibatch loss at step 2352: 2.589\n",
      "Validation loss: 4.597\n",
      "Average minibatch loss at step 2354: 2.588\n",
      "Average minibatch loss at step 2356: 2.588\n",
      "Average minibatch loss at step 2358: 2.588\n",
      "Average minibatch loss at step 2360: 2.587\n",
      "Validation loss: 4.600\n",
      "Average minibatch loss at step 2362: 2.587\n",
      "Average minibatch loss at step 2364: 2.587\n",
      "Average minibatch loss at step 2366: 2.586\n",
      "Average minibatch loss at step 2368: 2.586\n",
      "Validation loss: 4.604\n",
      "Average minibatch loss at step 2370: 2.586\n",
      "Average minibatch loss at step 2372: 2.586\n",
      "Average minibatch loss at step 2374: 2.585\n",
      "Average minibatch loss at step 2376: 2.585\n",
      "Validation loss: 4.607\n",
      "Average minibatch loss at step 2378: 2.585\n",
      "Average minibatch loss at step 2380: 2.584\n",
      "Average minibatch loss at step 2382: 2.584\n",
      "Average minibatch loss at step 2384: 2.584\n",
      "Validation loss: 4.610\n",
      "Average minibatch loss at step 2386: 2.583\n",
      "Average minibatch loss at step 2388: 2.583\n",
      "Average minibatch loss at step 2390: 2.583\n",
      "Average minibatch loss at step 2392: 2.582\n",
      "Validation loss: 4.613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 2394: 2.582\n",
      "Average minibatch loss at step 2396: 2.582\n",
      "Average minibatch loss at step 2398: 2.582\n",
      "Average minibatch loss at step 2400: 2.581\n",
      "Validation loss: 4.616\n",
      "Average minibatch loss at step 2402: 2.581\n",
      "Average minibatch loss at step 2404: 2.581\n",
      "Average minibatch loss at step 2406: 2.580\n",
      "Average minibatch loss at step 2408: 2.580\n",
      "Validation loss: 4.619\n",
      "Average minibatch loss at step 2410: 2.580\n",
      "Average minibatch loss at step 2412: 2.579\n",
      "Average minibatch loss at step 2414: 2.579\n",
      "Average minibatch loss at step 2416: 2.579\n",
      "Validation loss: 4.622\n",
      "Average minibatch loss at step 2418: 2.579\n",
      "Average minibatch loss at step 2420: 2.578\n",
      "Average minibatch loss at step 2422: 2.578\n",
      "Average minibatch loss at step 2424: 2.578\n",
      "Validation loss: 4.625\n",
      "Average minibatch loss at step 2426: 2.577\n",
      "Average minibatch loss at step 2428: 2.577\n",
      "Average minibatch loss at step 2430: 2.577\n",
      "Average minibatch loss at step 2432: 2.576\n",
      "Validation loss: 4.628\n",
      "Average minibatch loss at step 2434: 2.576\n",
      "Average minibatch loss at step 2436: 2.576\n",
      "Average minibatch loss at step 2438: 2.576\n",
      "Average minibatch loss at step 2440: 2.575\n",
      "Validation loss: 4.631\n",
      "Average minibatch loss at step 2442: 2.575\n",
      "Average minibatch loss at step 2444: 2.575\n",
      "Average minibatch loss at step 2446: 2.574\n",
      "Average minibatch loss at step 2448: 2.574\n",
      "Validation loss: 4.634\n",
      "Average minibatch loss at step 2450: 2.574\n",
      "Average minibatch loss at step 2452: 2.574\n",
      "Average minibatch loss at step 2454: 2.573\n",
      "Average minibatch loss at step 2456: 2.573\n",
      "Validation loss: 4.637\n",
      "Average minibatch loss at step 2458: 2.573\n",
      "Average minibatch loss at step 2460: 2.572\n",
      "Average minibatch loss at step 2462: 2.572\n",
      "Average minibatch loss at step 2464: 2.572\n",
      "Validation loss: 4.641\n",
      "Average minibatch loss at step 2466: 2.571\n",
      "Average minibatch loss at step 2468: 2.571\n",
      "Average minibatch loss at step 2470: 2.571\n",
      "Average minibatch loss at step 2472: 2.571\n",
      "Validation loss: 4.644\n",
      "Average minibatch loss at step 2474: 2.570\n",
      "Average minibatch loss at step 2476: 2.570\n",
      "Average minibatch loss at step 2478: 2.570\n",
      "Average minibatch loss at step 2480: 2.569\n",
      "Validation loss: 4.647\n",
      "Average minibatch loss at step 2482: 2.569\n",
      "Average minibatch loss at step 2484: 2.569\n",
      "Average minibatch loss at step 2486: 2.569\n",
      "Average minibatch loss at step 2488: 2.568\n",
      "Validation loss: 4.650\n",
      "Average minibatch loss at step 2490: 2.568\n",
      "Average minibatch loss at step 2492: 2.568\n",
      "Average minibatch loss at step 2494: 2.567\n",
      "Average minibatch loss at step 2496: 2.567\n",
      "Validation loss: 4.653\n",
      "Average minibatch loss at step 2498: 2.567\n",
      "Average minibatch loss at step 2500: 2.567\n",
      "Average minibatch loss at step 2502: 2.566\n",
      "Average minibatch loss at step 2504: 2.566\n",
      "Validation loss: 4.656\n",
      "Average minibatch loss at step 2506: 2.566\n",
      "Average minibatch loss at step 2508: 2.565\n",
      "Average minibatch loss at step 2510: 2.565\n",
      "Average minibatch loss at step 2512: 2.565\n",
      "Validation loss: 4.659\n",
      "Average minibatch loss at step 2514: 2.565\n",
      "Average minibatch loss at step 2516: 2.564\n",
      "Average minibatch loss at step 2518: 2.564\n",
      "Average minibatch loss at step 2520: 2.564\n",
      "Validation loss: 4.662\n",
      "Average minibatch loss at step 2522: 2.563\n",
      "Average minibatch loss at step 2524: 2.563\n",
      "Average minibatch loss at step 2526: 2.563\n",
      "Average minibatch loss at step 2528: 2.563\n",
      "Validation loss: 4.665\n",
      "Average minibatch loss at step 2530: 2.562\n",
      "Average minibatch loss at step 2532: 2.562\n",
      "Average minibatch loss at step 2534: 2.562\n",
      "Average minibatch loss at step 2536: 2.561\n",
      "Validation loss: 4.668\n",
      "Average minibatch loss at step 2538: 2.561\n",
      "Average minibatch loss at step 2540: 2.561\n",
      "Average minibatch loss at step 2542: 2.561\n",
      "Average minibatch loss at step 2544: 2.560\n",
      "Validation loss: 4.671\n",
      "Average minibatch loss at step 2546: 2.560\n",
      "Average minibatch loss at step 2548: 2.560\n",
      "Average minibatch loss at step 2550: 2.559\n",
      "Average minibatch loss at step 2552: 2.559\n",
      "Validation loss: 4.673\n",
      "Average minibatch loss at step 2554: 2.559\n",
      "Average minibatch loss at step 2556: 2.559\n",
      "Average minibatch loss at step 2558: 2.558\n",
      "Average minibatch loss at step 2560: 2.558\n",
      "Validation loss: 4.676\n",
      "Average minibatch loss at step 2562: 2.558\n",
      "Average minibatch loss at step 2564: 2.558\n",
      "Average minibatch loss at step 2566: 2.557\n",
      "Average minibatch loss at step 2568: 2.557\n",
      "Validation loss: 4.679\n",
      "Average minibatch loss at step 2570: 2.557\n",
      "Average minibatch loss at step 2572: 2.556\n",
      "Average minibatch loss at step 2574: 2.556\n",
      "Average minibatch loss at step 2576: 2.556\n",
      "Validation loss: 4.682\n",
      "Average minibatch loss at step 2578: 2.556\n",
      "Average minibatch loss at step 2580: 2.555\n",
      "Average minibatch loss at step 2582: 2.555\n",
      "Average minibatch loss at step 2584: 2.555\n",
      "Validation loss: 4.685\n",
      "Average minibatch loss at step 2586: 2.555\n",
      "Average minibatch loss at step 2588: 2.554\n",
      "Average minibatch loss at step 2590: 2.554\n",
      "Average minibatch loss at step 2592: 2.554\n",
      "Validation loss: 4.688\n",
      "Average minibatch loss at step 2594: 2.553\n",
      "Average minibatch loss at step 2596: 2.553\n",
      "Average minibatch loss at step 2598: 2.553\n",
      "Average minibatch loss at step 2600: 2.553\n",
      "Validation loss: 4.691\n",
      "Average minibatch loss at step 2602: 2.552\n",
      "Average minibatch loss at step 2604: 2.552\n",
      "Average minibatch loss at step 2606: 2.552\n",
      "Average minibatch loss at step 2608: 2.552\n",
      "Validation loss: 4.694\n",
      "Average minibatch loss at step 2610: 2.551\n",
      "Average minibatch loss at step 2612: 2.551\n",
      "Average minibatch loss at step 2614: 2.551\n",
      "Average minibatch loss at step 2616: 2.550\n",
      "Validation loss: 4.697\n",
      "Average minibatch loss at step 2618: 2.550\n",
      "Average minibatch loss at step 2620: 2.550\n",
      "Average minibatch loss at step 2622: 2.550\n",
      "Average minibatch loss at step 2624: 2.549\n",
      "Validation loss: 4.700\n",
      "Average minibatch loss at step 2626: 2.549\n",
      "Average minibatch loss at step 2628: 2.549\n",
      "Average minibatch loss at step 2630: 2.549\n",
      "Average minibatch loss at step 2632: 2.548\n",
      "Validation loss: 4.703\n",
      "Average minibatch loss at step 2634: 2.548\n",
      "Average minibatch loss at step 2636: 2.548\n",
      "Average minibatch loss at step 2638: 2.547\n",
      "Average minibatch loss at step 2640: 2.547\n",
      "Validation loss: 4.706\n",
      "Average minibatch loss at step 2642: 2.547\n",
      "Average minibatch loss at step 2644: 2.547\n",
      "Average minibatch loss at step 2646: 2.546\n",
      "Average minibatch loss at step 2648: 2.546\n",
      "Validation loss: 4.709\n",
      "Average minibatch loss at step 2650: 2.546\n",
      "Average minibatch loss at step 2652: 2.546\n",
      "Average minibatch loss at step 2654: 2.545\n",
      "Average minibatch loss at step 2656: 2.545\n",
      "Validation loss: 4.711\n",
      "Average minibatch loss at step 2658: 2.545\n",
      "Average minibatch loss at step 2660: 2.545\n",
      "Average minibatch loss at step 2662: 2.544\n",
      "Average minibatch loss at step 2664: 2.544\n",
      "Validation loss: 4.714\n",
      "Average minibatch loss at step 2666: 2.544\n",
      "Average minibatch loss at step 2668: 2.544\n",
      "Average minibatch loss at step 2670: 2.543\n",
      "Average minibatch loss at step 2672: 2.543\n",
      "Validation loss: 4.717\n",
      "Average minibatch loss at step 2674: 2.543\n",
      "Average minibatch loss at step 2676: 2.542\n",
      "Average minibatch loss at step 2678: 2.542\n",
      "Average minibatch loss at step 2680: 2.542\n",
      "Validation loss: 4.720\n",
      "Average minibatch loss at step 2682: 2.542\n",
      "Average minibatch loss at step 2684: 2.541\n",
      "Average minibatch loss at step 2686: 2.541\n",
      "Average minibatch loss at step 2688: 2.541\n",
      "Validation loss: 4.723\n",
      "Average minibatch loss at step 2690: 2.541\n",
      "Average minibatch loss at step 2692: 2.540\n",
      "Average minibatch loss at step 2694: 2.540\n",
      "Average minibatch loss at step 2696: 2.540\n",
      "Validation loss: 4.726\n",
      "Average minibatch loss at step 2698: 2.540\n",
      "Average minibatch loss at step 2700: 2.539\n",
      "Average minibatch loss at step 2702: 2.539\n",
      "Average minibatch loss at step 2704: 2.539\n",
      "Validation loss: 4.729\n",
      "Average minibatch loss at step 2706: 2.539\n",
      "Average minibatch loss at step 2708: 2.538\n",
      "Average minibatch loss at step 2710: 2.538\n",
      "Average minibatch loss at step 2712: 2.538\n",
      "Validation loss: 4.732\n",
      "Average minibatch loss at step 2714: 2.538\n",
      "Average minibatch loss at step 2716: 2.537\n",
      "Average minibatch loss at step 2718: 2.537\n",
      "Average minibatch loss at step 2720: 2.537\n",
      "Validation loss: 4.734\n",
      "Average minibatch loss at step 2722: 2.537\n",
      "Average minibatch loss at step 2724: 2.536\n",
      "Average minibatch loss at step 2726: 2.536\n",
      "Average minibatch loss at step 2728: 2.536\n",
      "Validation loss: 4.737\n",
      "Average minibatch loss at step 2730: 2.536\n",
      "Average minibatch loss at step 2732: 2.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 2734: 2.535\n",
      "Average minibatch loss at step 2736: 2.535\n",
      "Validation loss: 4.740\n",
      "Average minibatch loss at step 2738: 2.535\n",
      "Average minibatch loss at step 2740: 2.534\n",
      "Average minibatch loss at step 2742: 2.534\n",
      "Average minibatch loss at step 2744: 2.534\n",
      "Validation loss: 4.743\n",
      "Average minibatch loss at step 2746: 2.534\n",
      "Average minibatch loss at step 2748: 2.533\n",
      "Average minibatch loss at step 2750: 2.533\n",
      "Average minibatch loss at step 2752: 2.533\n",
      "Validation loss: 4.746\n",
      "Average minibatch loss at step 2754: 2.533\n",
      "Average minibatch loss at step 2756: 2.532\n",
      "Average minibatch loss at step 2758: 2.532\n",
      "Average minibatch loss at step 2760: 2.532\n",
      "Validation loss: 4.749\n",
      "Average minibatch loss at step 2762: 2.532\n",
      "Average minibatch loss at step 2764: 2.531\n",
      "Average minibatch loss at step 2766: 2.531\n",
      "Average minibatch loss at step 2768: 2.531\n",
      "Validation loss: 4.751\n",
      "Average minibatch loss at step 2770: 2.531\n",
      "Average minibatch loss at step 2772: 2.530\n",
      "Average minibatch loss at step 2774: 2.530\n",
      "Average minibatch loss at step 2776: 2.530\n",
      "Validation loss: 4.754\n",
      "Average minibatch loss at step 2778: 2.530\n",
      "Average minibatch loss at step 2780: 2.529\n",
      "Average minibatch loss at step 2782: 2.529\n",
      "Average minibatch loss at step 2784: 2.529\n",
      "Validation loss: 4.757\n",
      "Average minibatch loss at step 2786: 2.529\n",
      "Average minibatch loss at step 2788: 2.528\n",
      "Average minibatch loss at step 2790: 2.528\n",
      "Average minibatch loss at step 2792: 2.528\n",
      "Validation loss: 4.760\n",
      "Average minibatch loss at step 2794: 2.528\n",
      "Average minibatch loss at step 2796: 2.527\n",
      "Average minibatch loss at step 2798: 2.527\n",
      "Average minibatch loss at step 2800: 2.527\n",
      "Validation loss: 4.763\n",
      "Average minibatch loss at step 2802: 2.527\n",
      "Average minibatch loss at step 2804: 2.526\n",
      "Average minibatch loss at step 2806: 2.526\n",
      "Average minibatch loss at step 2808: 2.526\n",
      "Validation loss: 4.765\n",
      "Average minibatch loss at step 2810: 2.526\n",
      "Average minibatch loss at step 2812: 2.525\n",
      "Average minibatch loss at step 2814: 2.525\n",
      "Average minibatch loss at step 2816: 2.525\n",
      "Validation loss: 4.768\n",
      "Average minibatch loss at step 2818: 2.525\n",
      "Average minibatch loss at step 2820: 2.524\n",
      "Average minibatch loss at step 2822: 2.524\n",
      "Average minibatch loss at step 2824: 2.524\n",
      "Validation loss: 4.771\n",
      "Average minibatch loss at step 2826: 2.524\n",
      "Average minibatch loss at step 2828: 2.523\n",
      "Average minibatch loss at step 2830: 2.523\n",
      "Average minibatch loss at step 2832: 2.523\n",
      "Validation loss: 4.774\n",
      "Average minibatch loss at step 2834: 2.523\n",
      "Average minibatch loss at step 2836: 2.523\n",
      "Average minibatch loss at step 2838: 2.522\n",
      "Average minibatch loss at step 2840: 2.522\n",
      "Validation loss: 4.776\n",
      "Average minibatch loss at step 2842: 2.522\n",
      "Average minibatch loss at step 2844: 2.522\n",
      "Average minibatch loss at step 2846: 2.521\n",
      "Average minibatch loss at step 2848: 2.521\n",
      "Validation loss: 4.779\n",
      "Average minibatch loss at step 2850: 2.521\n",
      "Average minibatch loss at step 2852: 2.521\n",
      "Average minibatch loss at step 2854: 2.520\n",
      "Average minibatch loss at step 2856: 2.520\n",
      "Validation loss: 4.782\n",
      "Average minibatch loss at step 2858: 2.520\n",
      "Average minibatch loss at step 2860: 2.520\n",
      "Average minibatch loss at step 2862: 2.519\n",
      "Average minibatch loss at step 2864: 2.519\n",
      "Validation loss: 4.785\n",
      "Average minibatch loss at step 2866: 2.519\n",
      "Average minibatch loss at step 2868: 2.519\n",
      "Average minibatch loss at step 2870: 2.518\n",
      "Average minibatch loss at step 2872: 2.518\n",
      "Validation loss: 4.788\n",
      "Average minibatch loss at step 2874: 2.518\n",
      "Average minibatch loss at step 2876: 2.518\n",
      "Average minibatch loss at step 2878: 2.518\n",
      "Average minibatch loss at step 2880: 2.517\n",
      "Validation loss: 4.790\n",
      "Average minibatch loss at step 2882: 2.517\n",
      "Average minibatch loss at step 2884: 2.517\n",
      "Average minibatch loss at step 2886: 2.517\n",
      "Average minibatch loss at step 2888: 2.516\n",
      "Validation loss: 4.793\n",
      "Average minibatch loss at step 2890: 2.516\n",
      "Average minibatch loss at step 2892: 2.516\n",
      "Average minibatch loss at step 2894: 2.516\n",
      "Average minibatch loss at step 2896: 2.515\n",
      "Validation loss: 4.796\n",
      "Average minibatch loss at step 2898: 2.515\n",
      "Average minibatch loss at step 2900: 2.515\n",
      "Average minibatch loss at step 2902: 2.515\n",
      "Average minibatch loss at step 2904: 2.515\n",
      "Validation loss: 4.799\n",
      "Average minibatch loss at step 2906: 2.514\n",
      "Average minibatch loss at step 2908: 2.514\n",
      "Average minibatch loss at step 2910: 2.514\n",
      "Average minibatch loss at step 2912: 2.514\n",
      "Validation loss: 4.801\n",
      "Average minibatch loss at step 2914: 2.513\n",
      "Average minibatch loss at step 2916: 2.513\n",
      "Average minibatch loss at step 2918: 2.513\n",
      "Average minibatch loss at step 2920: 2.513\n",
      "Validation loss: 4.804\n",
      "Average minibatch loss at step 2922: 2.513\n",
      "Average minibatch loss at step 2924: 2.512\n",
      "Average minibatch loss at step 2926: 2.512\n",
      "Average minibatch loss at step 2928: 2.512\n",
      "Validation loss: 4.807\n",
      "Average minibatch loss at step 2930: 2.512\n",
      "Average minibatch loss at step 2932: 2.511\n",
      "Average minibatch loss at step 2934: 2.511\n",
      "Average minibatch loss at step 2936: 2.511\n",
      "Validation loss: 4.809\n",
      "Average minibatch loss at step 2938: 2.511\n",
      "Average minibatch loss at step 2940: 2.510\n",
      "Average minibatch loss at step 2942: 2.510\n",
      "Average minibatch loss at step 2944: 2.510\n",
      "Validation loss: 4.812\n",
      "Average minibatch loss at step 2946: 2.510\n",
      "Average minibatch loss at step 2948: 2.510\n",
      "Average minibatch loss at step 2950: 2.509\n",
      "Average minibatch loss at step 2952: 2.509\n",
      "Validation loss: 4.815\n",
      "Average minibatch loss at step 2954: 2.509\n",
      "Average minibatch loss at step 2956: 2.509\n",
      "Average minibatch loss at step 2958: 2.508\n",
      "Average minibatch loss at step 2960: 2.508\n",
      "Validation loss: 4.818\n",
      "Average minibatch loss at step 2962: 2.508\n",
      "Average minibatch loss at step 2964: 2.508\n",
      "Average minibatch loss at step 2966: 2.508\n",
      "Average minibatch loss at step 2968: 2.507\n",
      "Validation loss: 4.820\n",
      "Average minibatch loss at step 2970: 2.507\n",
      "Average minibatch loss at step 2972: 2.507\n",
      "Average minibatch loss at step 2974: 2.507\n",
      "Average minibatch loss at step 2976: 2.506\n",
      "Validation loss: 4.823\n",
      "Average minibatch loss at step 2978: 2.506\n",
      "Average minibatch loss at step 2980: 2.506\n",
      "Average minibatch loss at step 2982: 2.506\n",
      "Average minibatch loss at step 2984: 2.506\n",
      "Validation loss: 4.826\n",
      "Average minibatch loss at step 2986: 2.505\n",
      "Average minibatch loss at step 2988: 2.505\n",
      "Average minibatch loss at step 2990: 2.505\n",
      "Average minibatch loss at step 2992: 2.505\n",
      "Validation loss: 4.828\n",
      "Average minibatch loss at step 2994: 2.505\n",
      "Average minibatch loss at step 2996: 2.504\n",
      "Average minibatch loss at step 2998: 2.504\n",
      "Average minibatch loss at step 3000: 2.504\n",
      "Validation loss: 4.831\n",
      "Average minibatch loss at step 3002: 2.504\n",
      "Average minibatch loss at step 3004: 2.503\n",
      "Average minibatch loss at step 3006: 2.503\n",
      "Average minibatch loss at step 3008: 2.503\n",
      "Validation loss: 4.834\n",
      "Average minibatch loss at step 3010: 2.503\n",
      "Average minibatch loss at step 3012: 2.503\n",
      "Average minibatch loss at step 3014: 2.502\n",
      "Average minibatch loss at step 3016: 2.502\n",
      "Validation loss: 4.836\n",
      "Average minibatch loss at step 3018: 2.502\n",
      "Average minibatch loss at step 3020: 2.502\n",
      "Average minibatch loss at step 3022: 2.501\n",
      "Average minibatch loss at step 3024: 2.501\n",
      "Validation loss: 4.839\n",
      "Average minibatch loss at step 3026: 2.501\n",
      "Average minibatch loss at step 3028: 2.501\n",
      "Average minibatch loss at step 3030: 2.501\n",
      "Average minibatch loss at step 3032: 2.500\n",
      "Validation loss: 4.842\n",
      "Average minibatch loss at step 3034: 2.500\n",
      "Average minibatch loss at step 3036: 2.500\n",
      "Average minibatch loss at step 3038: 2.500\n",
      "Average minibatch loss at step 3040: 2.500\n",
      "Validation loss: 4.844\n",
      "Average minibatch loss at step 3042: 2.499\n",
      "Average minibatch loss at step 3044: 2.499\n",
      "Average minibatch loss at step 3046: 2.499\n",
      "Average minibatch loss at step 3048: 2.499\n",
      "Validation loss: 4.847\n",
      "Average minibatch loss at step 3050: 2.499\n",
      "Average minibatch loss at step 3052: 2.498\n",
      "Average minibatch loss at step 3054: 2.498\n",
      "Average minibatch loss at step 3056: 2.498\n",
      "Validation loss: 4.850\n",
      "Average minibatch loss at step 3058: 2.498\n",
      "Average minibatch loss at step 3060: 2.497\n",
      "Average minibatch loss at step 3062: 2.497\n",
      "Average minibatch loss at step 3064: 2.497\n",
      "Validation loss: 4.852\n",
      "Average minibatch loss at step 3066: 2.497\n",
      "Average minibatch loss at step 3068: 2.497\n",
      "Average minibatch loss at step 3070: 2.496\n",
      "Average minibatch loss at step 3072: 2.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.855\n",
      "Average minibatch loss at step 3074: 2.496\n",
      "Average minibatch loss at step 3076: 2.496\n",
      "Average minibatch loss at step 3078: 2.496\n",
      "Average minibatch loss at step 3080: 2.495\n",
      "Validation loss: 4.858\n",
      "Average minibatch loss at step 3082: 2.495\n",
      "Average minibatch loss at step 3084: 2.495\n",
      "Average minibatch loss at step 3086: 2.495\n",
      "Average minibatch loss at step 3088: 2.495\n",
      "Validation loss: 4.860\n",
      "Average minibatch loss at step 3090: 2.494\n",
      "Average minibatch loss at step 3092: 2.494\n",
      "Average minibatch loss at step 3094: 2.494\n",
      "Average minibatch loss at step 3096: 2.494\n",
      "Validation loss: 4.863\n",
      "Average minibatch loss at step 3098: 2.494\n",
      "Average minibatch loss at step 3100: 2.493\n",
      "Average minibatch loss at step 3102: 2.493\n",
      "Average minibatch loss at step 3104: 2.493\n",
      "Validation loss: 4.866\n",
      "Average minibatch loss at step 3106: 2.493\n",
      "Average minibatch loss at step 3108: 2.493\n",
      "Average minibatch loss at step 3110: 2.492\n",
      "Average minibatch loss at step 3112: 2.492\n",
      "Validation loss: 4.868\n",
      "Average minibatch loss at step 3114: 2.492\n",
      "Average minibatch loss at step 3116: 2.492\n",
      "Average minibatch loss at step 3118: 2.492\n",
      "Average minibatch loss at step 3120: 2.491\n",
      "Validation loss: 4.871\n",
      "Average minibatch loss at step 3122: 2.491\n",
      "Average minibatch loss at step 3124: 2.491\n",
      "Average minibatch loss at step 3126: 2.491\n",
      "Average minibatch loss at step 3128: 2.491\n",
      "Validation loss: 4.874\n",
      "Average minibatch loss at step 3130: 2.490\n",
      "Average minibatch loss at step 3132: 2.490\n",
      "Average minibatch loss at step 3134: 2.490\n",
      "Average minibatch loss at step 3136: 2.490\n",
      "Validation loss: 4.876\n",
      "Average minibatch loss at step 3138: 2.490\n",
      "Average minibatch loss at step 3140: 2.489\n",
      "Average minibatch loss at step 3142: 2.489\n",
      "Average minibatch loss at step 3144: 2.489\n",
      "Validation loss: 4.879\n",
      "Average minibatch loss at step 3146: 2.489\n",
      "Average minibatch loss at step 3148: 2.489\n",
      "Average minibatch loss at step 3150: 2.488\n",
      "Average minibatch loss at step 3152: 2.488\n",
      "Validation loss: 4.881\n",
      "Average minibatch loss at step 3154: 2.488\n",
      "Average minibatch loss at step 3156: 2.488\n",
      "Average minibatch loss at step 3158: 2.488\n",
      "Average minibatch loss at step 3160: 2.487\n",
      "Validation loss: 4.884\n",
      "Average minibatch loss at step 3162: 2.487\n",
      "Average minibatch loss at step 3164: 2.487\n",
      "Average minibatch loss at step 3166: 2.487\n",
      "Average minibatch loss at step 3168: 2.487\n",
      "Validation loss: 4.887\n",
      "Average minibatch loss at step 3170: 2.486\n",
      "Average minibatch loss at step 3172: 2.486\n",
      "Average minibatch loss at step 3174: 2.486\n",
      "Average minibatch loss at step 3176: 2.486\n",
      "Validation loss: 4.889\n",
      "Average minibatch loss at step 3178: 2.486\n",
      "Average minibatch loss at step 3180: 2.485\n",
      "Average minibatch loss at step 3182: 2.485\n",
      "Average minibatch loss at step 3184: 2.485\n",
      "Validation loss: 4.892\n",
      "Average minibatch loss at step 3186: 2.485\n",
      "Average minibatch loss at step 3188: 2.485\n",
      "Average minibatch loss at step 3190: 2.484\n",
      "Average minibatch loss at step 3192: 2.484\n",
      "Validation loss: 4.894\n",
      "Average minibatch loss at step 3194: 2.484\n",
      "Average minibatch loss at step 3196: 2.484\n",
      "Average minibatch loss at step 3198: 2.484\n",
      "Average minibatch loss at step 3200: 2.484\n",
      "Validation loss: 4.897\n",
      "Average minibatch loss at step 3202: 2.483\n",
      "Average minibatch loss at step 3204: 2.483\n",
      "Average minibatch loss at step 3206: 2.483\n",
      "Average minibatch loss at step 3208: 2.483\n",
      "Validation loss: 4.900\n",
      "Average minibatch loss at step 3210: 2.483\n",
      "Average minibatch loss at step 3212: 2.482\n",
      "Average minibatch loss at step 3214: 2.482\n",
      "Average minibatch loss at step 3216: 2.482\n",
      "Validation loss: 4.902\n",
      "Average minibatch loss at step 3218: 2.482\n",
      "Average minibatch loss at step 3220: 2.482\n",
      "Average minibatch loss at step 3222: 2.481\n",
      "Average minibatch loss at step 3224: 2.481\n",
      "Validation loss: 4.905\n",
      "Average minibatch loss at step 3226: 2.481\n",
      "Average minibatch loss at step 3228: 2.481\n",
      "Average minibatch loss at step 3230: 2.481\n",
      "Average minibatch loss at step 3232: 2.480\n",
      "Validation loss: 4.907\n",
      "Average minibatch loss at step 3234: 2.480\n",
      "Average minibatch loss at step 3236: 2.480\n",
      "Average minibatch loss at step 3238: 2.480\n",
      "Average minibatch loss at step 3240: 2.480\n",
      "Validation loss: 4.910\n",
      "Average minibatch loss at step 3242: 2.480\n",
      "Average minibatch loss at step 3244: 2.479\n",
      "Average minibatch loss at step 3246: 2.479\n",
      "Average minibatch loss at step 3248: 2.479\n",
      "Validation loss: 4.912\n",
      "Average minibatch loss at step 3250: 2.479\n",
      "Average minibatch loss at step 3252: 2.479\n",
      "Average minibatch loss at step 3254: 2.478\n",
      "Average minibatch loss at step 3256: 2.478\n",
      "Validation loss: 4.915\n",
      "Average minibatch loss at step 3258: 2.478\n",
      "Average minibatch loss at step 3260: 2.478\n",
      "Average minibatch loss at step 3262: 2.478\n",
      "Average minibatch loss at step 3264: 2.478\n",
      "Validation loss: 4.918\n",
      "Average minibatch loss at step 3266: 2.477\n",
      "Average minibatch loss at step 3268: 2.477\n",
      "Average minibatch loss at step 3270: 2.477\n",
      "Average minibatch loss at step 3272: 2.477\n",
      "Validation loss: 4.920\n",
      "Average minibatch loss at step 3274: 2.477\n",
      "Average minibatch loss at step 3276: 2.476\n",
      "Average minibatch loss at step 3278: 2.476\n",
      "Average minibatch loss at step 3280: 2.476\n",
      "Validation loss: 4.923\n",
      "Average minibatch loss at step 3282: 2.476\n",
      "Average minibatch loss at step 3284: 2.476\n",
      "Average minibatch loss at step 3286: 2.476\n",
      "Average minibatch loss at step 3288: 2.475\n",
      "Validation loss: 4.925\n",
      "Average minibatch loss at step 3290: 2.475\n",
      "Average minibatch loss at step 3292: 2.475\n",
      "Average minibatch loss at step 3294: 2.475\n",
      "Average minibatch loss at step 3296: 2.475\n",
      "Validation loss: 4.928\n",
      "Average minibatch loss at step 3298: 2.474\n",
      "Average minibatch loss at step 3300: 2.474\n",
      "Average minibatch loss at step 3302: 2.474\n",
      "Average minibatch loss at step 3304: 2.474\n",
      "Validation loss: 4.930\n",
      "Average minibatch loss at step 3306: 2.474\n",
      "Average minibatch loss at step 3308: 2.474\n",
      "Average minibatch loss at step 3310: 2.473\n",
      "Average minibatch loss at step 3312: 2.473\n",
      "Validation loss: 4.933\n",
      "Average minibatch loss at step 3314: 2.473\n",
      "Average minibatch loss at step 3316: 2.473\n",
      "Average minibatch loss at step 3318: 2.473\n",
      "Average minibatch loss at step 3320: 2.473\n",
      "Validation loss: 4.935\n",
      "Average minibatch loss at step 3322: 2.472\n",
      "Average minibatch loss at step 3324: 2.472\n",
      "Average minibatch loss at step 3326: 2.472\n",
      "Average minibatch loss at step 3328: 2.472\n",
      "Validation loss: 4.938\n",
      "Average minibatch loss at step 3330: 2.472\n",
      "Average minibatch loss at step 3332: 2.471\n",
      "Average minibatch loss at step 3334: 2.471\n",
      "Average minibatch loss at step 3336: 2.471\n",
      "Validation loss: 4.940\n",
      "Average minibatch loss at step 3338: 2.471\n",
      "Average minibatch loss at step 3340: 2.471\n",
      "Average minibatch loss at step 3342: 2.471\n",
      "Average minibatch loss at step 3344: 2.470\n",
      "Validation loss: 4.943\n",
      "Average minibatch loss at step 3346: 2.470\n",
      "Average minibatch loss at step 3348: 2.470\n",
      "Average minibatch loss at step 3350: 2.470\n",
      "Average minibatch loss at step 3352: 2.470\n",
      "Validation loss: 4.945\n",
      "Average minibatch loss at step 3354: 2.470\n",
      "Average minibatch loss at step 3356: 2.469\n",
      "Average minibatch loss at step 3358: 2.469\n",
      "Average minibatch loss at step 3360: 2.469\n",
      "Validation loss: 4.948\n",
      "Average minibatch loss at step 3362: 2.469\n",
      "Average minibatch loss at step 3364: 2.469\n",
      "Average minibatch loss at step 3366: 2.469\n",
      "Average minibatch loss at step 3368: 2.468\n",
      "Validation loss: 4.950\n",
      "Average minibatch loss at step 3370: 2.468\n",
      "Average minibatch loss at step 3372: 2.468\n",
      "Average minibatch loss at step 3374: 2.468\n",
      "Average minibatch loss at step 3376: 2.468\n",
      "Validation loss: 4.953\n",
      "Average minibatch loss at step 3378: 2.468\n",
      "Average minibatch loss at step 3380: 2.467\n",
      "Average minibatch loss at step 3382: 2.467\n",
      "Average minibatch loss at step 3384: 2.467\n",
      "Validation loss: 4.955\n",
      "Average minibatch loss at step 3386: 2.467\n",
      "Average minibatch loss at step 3388: 2.467\n",
      "Average minibatch loss at step 3390: 2.467\n",
      "Average minibatch loss at step 3392: 2.466\n",
      "Validation loss: 4.958\n",
      "Average minibatch loss at step 3394: 2.466\n",
      "Average minibatch loss at step 3396: 2.466\n",
      "Average minibatch loss at step 3398: 2.466\n",
      "Average minibatch loss at step 3400: 2.466\n",
      "Validation loss: 4.960\n",
      "Average minibatch loss at step 3402: 2.466\n",
      "Average minibatch loss at step 3404: 2.465\n",
      "Average minibatch loss at step 3406: 2.465\n",
      "Average minibatch loss at step 3408: 2.465\n",
      "Validation loss: 4.963\n",
      "Average minibatch loss at step 3410: 2.465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 3412: 2.465\n",
      "Average minibatch loss at step 3414: 2.465\n",
      "Average minibatch loss at step 3416: 2.464\n",
      "Validation loss: 4.965\n",
      "Average minibatch loss at step 3418: 2.464\n",
      "Average minibatch loss at step 3420: 2.464\n",
      "Average minibatch loss at step 3422: 2.464\n",
      "Average minibatch loss at step 3424: 2.464\n",
      "Validation loss: 4.968\n",
      "Average minibatch loss at step 3426: 2.464\n",
      "Average minibatch loss at step 3428: 2.463\n",
      "Average minibatch loss at step 3430: 2.463\n",
      "Average minibatch loss at step 3432: 2.463\n",
      "Validation loss: 4.970\n",
      "Average minibatch loss at step 3434: 2.463\n",
      "Average minibatch loss at step 3436: 2.463\n",
      "Average minibatch loss at step 3438: 2.463\n",
      "Average minibatch loss at step 3440: 2.462\n",
      "Validation loss: 4.973\n",
      "Average minibatch loss at step 3442: 2.462\n",
      "Average minibatch loss at step 3444: 2.462\n",
      "Average minibatch loss at step 3446: 2.462\n",
      "Average minibatch loss at step 3448: 2.462\n",
      "Validation loss: 4.975\n",
      "Average minibatch loss at step 3450: 2.462\n",
      "Average minibatch loss at step 3452: 2.461\n",
      "Average minibatch loss at step 3454: 2.461\n",
      "Average minibatch loss at step 3456: 2.461\n",
      "Validation loss: 4.978\n",
      "Average minibatch loss at step 3458: 2.461\n",
      "Average minibatch loss at step 3460: 2.461\n",
      "Average minibatch loss at step 3462: 2.461\n",
      "Average minibatch loss at step 3464: 2.461\n",
      "Validation loss: 4.980\n",
      "Average minibatch loss at step 3466: 2.460\n",
      "Average minibatch loss at step 3468: 2.460\n",
      "Average minibatch loss at step 3470: 2.460\n",
      "Average minibatch loss at step 3472: 2.460\n",
      "Validation loss: 4.983\n",
      "Average minibatch loss at step 3474: 2.460\n",
      "Average minibatch loss at step 3476: 2.460\n",
      "Average minibatch loss at step 3478: 2.459\n",
      "Average minibatch loss at step 3480: 2.459\n",
      "Validation loss: 4.985\n",
      "Average minibatch loss at step 3482: 2.459\n",
      "Average minibatch loss at step 3484: 2.459\n",
      "Average minibatch loss at step 3486: 2.459\n",
      "Average minibatch loss at step 3488: 2.459\n",
      "Validation loss: 4.988\n",
      "Average minibatch loss at step 3490: 2.459\n",
      "Average minibatch loss at step 3492: 2.458\n",
      "Average minibatch loss at step 3494: 2.458\n",
      "Average minibatch loss at step 3496: 2.458\n",
      "Validation loss: 4.990\n",
      "Average minibatch loss at step 3498: 2.458\n",
      "Average minibatch loss at step 3500: 2.458\n",
      "Average minibatch loss at step 3502: 2.458\n",
      "Average minibatch loss at step 3504: 2.457\n",
      "Validation loss: 4.992\n",
      "Average minibatch loss at step 3506: 2.457\n",
      "Average minibatch loss at step 3508: 2.457\n",
      "Average minibatch loss at step 3510: 2.457\n",
      "Average minibatch loss at step 3512: 2.457\n",
      "Validation loss: 4.995\n",
      "Average minibatch loss at step 3514: 2.457\n",
      "Average minibatch loss at step 3516: 2.457\n",
      "Average minibatch loss at step 3518: 2.456\n",
      "Average minibatch loss at step 3520: 2.456\n",
      "Validation loss: 4.997\n",
      "Average minibatch loss at step 3522: 2.456\n",
      "Average minibatch loss at step 3524: 2.456\n",
      "Average minibatch loss at step 3526: 2.456\n",
      "Average minibatch loss at step 3528: 2.456\n",
      "Validation loss: 5.000\n",
      "Average minibatch loss at step 3530: 2.455\n",
      "Average minibatch loss at step 3532: 2.455\n",
      "Average minibatch loss at step 3534: 2.455\n",
      "Average minibatch loss at step 3536: 2.455\n",
      "Validation loss: 5.002\n",
      "Average minibatch loss at step 3538: 2.455\n",
      "Average minibatch loss at step 3540: 2.455\n",
      "Average minibatch loss at step 3542: 2.455\n",
      "Average minibatch loss at step 3544: 2.454\n",
      "Validation loss: 5.004\n",
      "Average minibatch loss at step 3546: 2.454\n",
      "Average minibatch loss at step 3548: 2.454\n",
      "Average minibatch loss at step 3550: 2.454\n",
      "Average minibatch loss at step 3552: 2.454\n",
      "Validation loss: 5.007\n",
      "Average minibatch loss at step 3554: 2.454\n",
      "Average minibatch loss at step 3556: 2.454\n",
      "Average minibatch loss at step 3558: 2.453\n",
      "Average minibatch loss at step 3560: 2.453\n",
      "Validation loss: 5.009\n",
      "Average minibatch loss at step 3562: 2.453\n",
      "Average minibatch loss at step 3564: 2.453\n",
      "Average minibatch loss at step 3566: 2.453\n",
      "Average minibatch loss at step 3568: 2.453\n",
      "Validation loss: 5.012\n",
      "Average minibatch loss at step 3570: 2.452\n",
      "Average minibatch loss at step 3572: 2.452\n",
      "Average minibatch loss at step 3574: 2.452\n",
      "Average minibatch loss at step 3576: 2.452\n",
      "Validation loss: 5.014\n",
      "Average minibatch loss at step 3578: 2.452\n",
      "Average minibatch loss at step 3580: 2.452\n",
      "Average minibatch loss at step 3582: 2.452\n",
      "Average minibatch loss at step 3584: 2.451\n",
      "Validation loss: 5.016\n",
      "Average minibatch loss at step 3586: 2.451\n",
      "Average minibatch loss at step 3588: 2.451\n",
      "Average minibatch loss at step 3590: 2.451\n",
      "Average minibatch loss at step 3592: 2.451\n",
      "Validation loss: 5.019\n",
      "Average minibatch loss at step 3594: 2.451\n",
      "Average minibatch loss at step 3596: 2.451\n",
      "Average minibatch loss at step 3598: 2.450\n",
      "Average minibatch loss at step 3600: 2.450\n",
      "Validation loss: 5.021\n",
      "Average minibatch loss at step 3602: 2.450\n",
      "Average minibatch loss at step 3604: 2.450\n",
      "Average minibatch loss at step 3606: 2.450\n",
      "Average minibatch loss at step 3608: 2.450\n",
      "Validation loss: 5.024\n",
      "Average minibatch loss at step 3610: 2.450\n",
      "Average minibatch loss at step 3612: 2.449\n",
      "Average minibatch loss at step 3614: 2.449\n",
      "Average minibatch loss at step 3616: 2.449\n",
      "Validation loss: 5.026\n",
      "Average minibatch loss at step 3618: 2.449\n",
      "Average minibatch loss at step 3620: 2.449\n",
      "Average minibatch loss at step 3622: 2.449\n",
      "Average minibatch loss at step 3624: 2.449\n",
      "Validation loss: 5.028\n",
      "Average minibatch loss at step 3626: 2.449\n",
      "Average minibatch loss at step 3628: 2.448\n",
      "Average minibatch loss at step 3630: 2.448\n",
      "Average minibatch loss at step 3632: 2.448\n",
      "Validation loss: 5.031\n",
      "Average minibatch loss at step 3634: 2.448\n",
      "Average minibatch loss at step 3636: 2.448\n",
      "Average minibatch loss at step 3638: 2.448\n",
      "Average minibatch loss at step 3640: 2.448\n",
      "Validation loss: 5.033\n",
      "Average minibatch loss at step 3642: 2.447\n",
      "Average minibatch loss at step 3644: 2.447\n",
      "Average minibatch loss at step 3646: 2.447\n",
      "Average minibatch loss at step 3648: 2.447\n",
      "Validation loss: 5.035\n",
      "Average minibatch loss at step 3650: 2.447\n",
      "Average minibatch loss at step 3652: 2.447\n",
      "Average minibatch loss at step 3654: 2.447\n",
      "Average minibatch loss at step 3656: 2.446\n",
      "Validation loss: 5.038\n",
      "Average minibatch loss at step 3658: 2.446\n",
      "Average minibatch loss at step 3660: 2.446\n",
      "Average minibatch loss at step 3662: 2.446\n",
      "Average minibatch loss at step 3664: 2.446\n",
      "Validation loss: 5.040\n",
      "Average minibatch loss at step 3666: 2.446\n",
      "Average minibatch loss at step 3668: 2.446\n",
      "Average minibatch loss at step 3670: 2.445\n",
      "Average minibatch loss at step 3672: 2.445\n",
      "Validation loss: 5.042\n",
      "Average minibatch loss at step 3674: 2.445\n",
      "Average minibatch loss at step 3676: 2.445\n",
      "Average minibatch loss at step 3678: 2.445\n",
      "Average minibatch loss at step 3680: 2.445\n",
      "Validation loss: 5.045\n",
      "Average minibatch loss at step 3682: 2.445\n",
      "Average minibatch loss at step 3684: 2.445\n",
      "Average minibatch loss at step 3686: 2.444\n",
      "Average minibatch loss at step 3688: 2.444\n",
      "Validation loss: 5.047\n",
      "Average minibatch loss at step 3690: 2.444\n",
      "Average minibatch loss at step 3692: 2.444\n",
      "Average minibatch loss at step 3694: 2.444\n",
      "Average minibatch loss at step 3696: 2.444\n",
      "Validation loss: 5.049\n",
      "Average minibatch loss at step 3698: 2.444\n",
      "Average minibatch loss at step 3700: 2.443\n",
      "Average minibatch loss at step 3702: 2.443\n",
      "Average minibatch loss at step 3704: 2.443\n",
      "Validation loss: 5.052\n",
      "Average minibatch loss at step 3706: 2.443\n",
      "Average minibatch loss at step 3708: 2.443\n",
      "Average minibatch loss at step 3710: 2.443\n",
      "Average minibatch loss at step 3712: 2.443\n",
      "Validation loss: 5.054\n",
      "Average minibatch loss at step 3714: 2.443\n",
      "Average minibatch loss at step 3716: 2.442\n",
      "Average minibatch loss at step 3718: 2.442\n",
      "Average minibatch loss at step 3720: 2.442\n",
      "Validation loss: 5.056\n",
      "Average minibatch loss at step 3722: 2.442\n",
      "Average minibatch loss at step 3724: 2.442\n",
      "Average minibatch loss at step 3726: 2.442\n",
      "Average minibatch loss at step 3728: 2.442\n",
      "Validation loss: 5.059\n",
      "Average minibatch loss at step 3730: 2.442\n",
      "Average minibatch loss at step 3732: 2.441\n",
      "Average minibatch loss at step 3734: 2.441\n",
      "Average minibatch loss at step 3736: 2.441\n",
      "Validation loss: 5.061\n",
      "Average minibatch loss at step 3738: 2.441\n",
      "Average minibatch loss at step 3740: 2.441\n",
      "Average minibatch loss at step 3742: 2.441\n",
      "Average minibatch loss at step 3744: 2.441\n",
      "Validation loss: 5.063\n",
      "Average minibatch loss at step 3746: 2.441\n",
      "Average minibatch loss at step 3748: 2.440\n",
      "Average minibatch loss at step 3750: 2.440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 3752: 2.440\n",
      "Validation loss: 5.065\n",
      "Average minibatch loss at step 3754: 2.440\n",
      "Average minibatch loss at step 3756: 2.440\n",
      "Average minibatch loss at step 3758: 2.440\n",
      "Average minibatch loss at step 3760: 2.440\n",
      "Validation loss: 5.068\n",
      "Average minibatch loss at step 3762: 2.439\n",
      "Average minibatch loss at step 3764: 2.439\n",
      "Average minibatch loss at step 3766: 2.439\n",
      "Average minibatch loss at step 3768: 2.439\n",
      "Validation loss: 5.070\n",
      "Average minibatch loss at step 3770: 2.439\n",
      "Average minibatch loss at step 3772: 2.439\n",
      "Average minibatch loss at step 3774: 2.439\n",
      "Average minibatch loss at step 3776: 2.439\n",
      "Validation loss: 5.072\n",
      "Average minibatch loss at step 3778: 2.439\n",
      "Average minibatch loss at step 3780: 2.438\n",
      "Average minibatch loss at step 3782: 2.438\n",
      "Average minibatch loss at step 3784: 2.438\n",
      "Validation loss: 5.075\n",
      "Average minibatch loss at step 3786: 2.438\n",
      "Average minibatch loss at step 3788: 2.438\n",
      "Average minibatch loss at step 3790: 2.438\n",
      "Average minibatch loss at step 3792: 2.438\n",
      "Validation loss: 5.077\n",
      "Average minibatch loss at step 3794: 2.438\n",
      "Average minibatch loss at step 3796: 2.437\n",
      "Average minibatch loss at step 3798: 2.437\n",
      "Average minibatch loss at step 3800: 2.437\n",
      "Validation loss: 5.079\n",
      "Average minibatch loss at step 3802: 2.437\n",
      "Average minibatch loss at step 3804: 2.437\n",
      "Average minibatch loss at step 3806: 2.437\n",
      "Average minibatch loss at step 3808: 2.437\n",
      "Validation loss: 5.081\n",
      "Average minibatch loss at step 3810: 2.437\n",
      "Average minibatch loss at step 3812: 2.436\n",
      "Average minibatch loss at step 3814: 2.436\n",
      "Average minibatch loss at step 3816: 2.436\n",
      "Validation loss: 5.084\n",
      "Average minibatch loss at step 3818: 2.436\n",
      "Average minibatch loss at step 3820: 2.436\n",
      "Average minibatch loss at step 3822: 2.436\n",
      "Average minibatch loss at step 3824: 2.436\n",
      "Validation loss: 5.086\n",
      "Average minibatch loss at step 3826: 2.436\n",
      "Average minibatch loss at step 3828: 2.435\n",
      "Average minibatch loss at step 3830: 2.435\n",
      "Average minibatch loss at step 3832: 2.435\n",
      "Validation loss: 5.088\n",
      "Average minibatch loss at step 3834: 2.435\n",
      "Average minibatch loss at step 3836: 2.435\n",
      "Average minibatch loss at step 3838: 2.435\n",
      "Average minibatch loss at step 3840: 2.435\n",
      "Validation loss: 5.090\n",
      "Average minibatch loss at step 3842: 2.435\n",
      "Average minibatch loss at step 3844: 2.435\n",
      "Average minibatch loss at step 3846: 2.434\n",
      "Average minibatch loss at step 3848: 2.434\n",
      "Validation loss: 5.092\n",
      "Average minibatch loss at step 3850: 2.434\n",
      "Average minibatch loss at step 3852: 2.434\n",
      "Average minibatch loss at step 3854: 2.434\n",
      "Average minibatch loss at step 3856: 2.434\n",
      "Validation loss: 5.095\n",
      "Average minibatch loss at step 3858: 2.434\n",
      "Average minibatch loss at step 3860: 2.434\n",
      "Average minibatch loss at step 3862: 2.433\n",
      "Average minibatch loss at step 3864: 2.433\n",
      "Validation loss: 5.097\n",
      "Average minibatch loss at step 3866: 2.433\n",
      "Average minibatch loss at step 3868: 2.433\n",
      "Average minibatch loss at step 3870: 2.433\n",
      "Average minibatch loss at step 3872: 2.433\n",
      "Validation loss: 5.099\n",
      "Average minibatch loss at step 3874: 2.433\n",
      "Average minibatch loss at step 3876: 2.433\n",
      "Average minibatch loss at step 3878: 2.433\n",
      "Average minibatch loss at step 3880: 2.432\n",
      "Validation loss: 5.101\n",
      "Average minibatch loss at step 3882: 2.432\n",
      "Average minibatch loss at step 3884: 2.432\n",
      "Average minibatch loss at step 3886: 2.432\n",
      "Average minibatch loss at step 3888: 2.432\n",
      "Validation loss: 5.104\n",
      "Average minibatch loss at step 3890: 2.432\n",
      "Average minibatch loss at step 3892: 2.432\n",
      "Average minibatch loss at step 3894: 2.432\n",
      "Average minibatch loss at step 3896: 2.432\n",
      "Validation loss: 5.106\n",
      "Average minibatch loss at step 3898: 2.431\n",
      "Average minibatch loss at step 3900: 2.431\n",
      "Average minibatch loss at step 3902: 2.431\n",
      "Average minibatch loss at step 3904: 2.431\n",
      "Validation loss: 5.108\n",
      "Average minibatch loss at step 3906: 2.431\n",
      "Average minibatch loss at step 3908: 2.431\n",
      "Average minibatch loss at step 3910: 2.431\n",
      "Average minibatch loss at step 3912: 2.431\n",
      "Validation loss: 5.110\n",
      "Average minibatch loss at step 3914: 2.431\n",
      "Average minibatch loss at step 3916: 2.430\n",
      "Average minibatch loss at step 3918: 2.430\n",
      "Average minibatch loss at step 3920: 2.430\n",
      "Validation loss: 5.112\n",
      "Average minibatch loss at step 3922: 2.430\n",
      "Average minibatch loss at step 3924: 2.430\n",
      "Average minibatch loss at step 3926: 2.430\n",
      "Average minibatch loss at step 3928: 2.430\n",
      "Validation loss: 5.114\n",
      "Average minibatch loss at step 3930: 2.430\n",
      "Average minibatch loss at step 3932: 2.430\n",
      "Average minibatch loss at step 3934: 2.429\n",
      "Average minibatch loss at step 3936: 2.429\n",
      "Validation loss: 5.117\n",
      "Average minibatch loss at step 3938: 2.429\n",
      "Average minibatch loss at step 3940: 2.429\n",
      "Average minibatch loss at step 3942: 2.429\n",
      "Average minibatch loss at step 3944: 2.429\n",
      "Validation loss: 5.119\n",
      "Average minibatch loss at step 3946: 2.429\n",
      "Average minibatch loss at step 3948: 2.429\n",
      "Average minibatch loss at step 3950: 2.429\n",
      "Average minibatch loss at step 3952: 2.428\n",
      "Validation loss: 5.121\n",
      "Average minibatch loss at step 3954: 2.428\n",
      "Average minibatch loss at step 3956: 2.428\n",
      "Average minibatch loss at step 3958: 2.428\n",
      "Average minibatch loss at step 3960: 2.428\n",
      "Validation loss: 5.123\n",
      "Average minibatch loss at step 3962: 2.428\n",
      "Average minibatch loss at step 3964: 2.428\n",
      "Average minibatch loss at step 3966: 2.428\n",
      "Average minibatch loss at step 3968: 2.428\n",
      "Validation loss: 5.125\n",
      "Average minibatch loss at step 3970: 2.427\n",
      "Average minibatch loss at step 3972: 2.427\n",
      "Average minibatch loss at step 3974: 2.427\n",
      "Average minibatch loss at step 3976: 2.427\n",
      "Validation loss: 5.127\n",
      "Average minibatch loss at step 3978: 2.427\n",
      "Average minibatch loss at step 3980: 2.427\n",
      "Average minibatch loss at step 3982: 2.427\n",
      "Average minibatch loss at step 3984: 2.427\n",
      "Validation loss: 5.130\n",
      "Average minibatch loss at step 3986: 2.427\n",
      "Average minibatch loss at step 3988: 2.427\n",
      "Average minibatch loss at step 3990: 2.426\n",
      "Average minibatch loss at step 3992: 2.426\n",
      "Validation loss: 5.132\n",
      "Average minibatch loss at step 3994: 2.426\n",
      "Average minibatch loss at step 3996: 2.426\n",
      "Average minibatch loss at step 3998: 2.426\n",
      "Average minibatch loss at step 4000: 2.426\n",
      "Validation loss: 5.134\n",
      "\n",
      "learning rate: 0.008500\n",
      "\n",
      "Average minibatch loss at step 4002: 2.418\n",
      "Average minibatch loss at step 4004: 2.413\n",
      "Average minibatch loss at step 4006: 2.410\n",
      "Average minibatch loss at step 4008: 2.410\n",
      "Validation loss: 5.125\n",
      "Average minibatch loss at step 4010: 2.409\n",
      "Average minibatch loss at step 4012: 2.409\n",
      "Average minibatch loss at step 4014: 2.409\n",
      "Average minibatch loss at step 4016: 2.409\n",
      "Validation loss: 5.126\n",
      "Average minibatch loss at step 4018: 2.409\n",
      "Average minibatch loss at step 4020: 2.409\n",
      "Average minibatch loss at step 4022: 2.409\n",
      "Average minibatch loss at step 4024: 2.409\n",
      "Validation loss: 5.127\n",
      "Average minibatch loss at step 4026: 2.409\n",
      "Average minibatch loss at step 4028: 2.409\n",
      "Average minibatch loss at step 4030: 2.409\n",
      "Average minibatch loss at step 4032: 2.409\n",
      "Validation loss: 5.127\n",
      "Average minibatch loss at step 4034: 2.409\n",
      "Average minibatch loss at step 4036: 2.409\n",
      "Average minibatch loss at step 4038: 2.409\n",
      "Average minibatch loss at step 4040: 2.409\n",
      "Validation loss: 5.127\n",
      "Average minibatch loss at step 4042: 2.409\n",
      "Average minibatch loss at step 4044: 2.409\n",
      "Average minibatch loss at step 4046: 2.409\n",
      "Average minibatch loss at step 4048: 2.409\n",
      "Validation loss: 5.128\n",
      "Average minibatch loss at step 4050: 2.409\n",
      "Average minibatch loss at step 4052: 2.409\n",
      "Average minibatch loss at step 4054: 2.409\n",
      "Average minibatch loss at step 4056: 2.409\n",
      "Validation loss: 5.128\n",
      "Average minibatch loss at step 4058: 2.409\n",
      "Average minibatch loss at step 4060: 2.409\n",
      "Average minibatch loss at step 4062: 2.409\n",
      "Average minibatch loss at step 4064: 2.409\n",
      "Validation loss: 5.129\n",
      "Average minibatch loss at step 4066: 2.409\n",
      "Average minibatch loss at step 4068: 2.409\n",
      "Average minibatch loss at step 4070: 2.409\n",
      "Average minibatch loss at step 4072: 2.409\n",
      "Validation loss: 5.129\n",
      "Average minibatch loss at step 4074: 2.409\n",
      "Average minibatch loss at step 4076: 2.409\n",
      "Average minibatch loss at step 4078: 2.409\n",
      "Average minibatch loss at step 4080: 2.409\n",
      "Validation loss: 5.129\n",
      "Average minibatch loss at step 4082: 2.409\n",
      "Average minibatch loss at step 4084: 2.409\n",
      "Average minibatch loss at step 4086: 2.409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 4088: 2.409\n",
      "Validation loss: 5.130\n",
      "Average minibatch loss at step 4090: 2.409\n",
      "Average minibatch loss at step 4092: 2.409\n",
      "Average minibatch loss at step 4094: 2.409\n",
      "Average minibatch loss at step 4096: 2.409\n",
      "Validation loss: 5.130\n",
      "Average minibatch loss at step 4098: 2.409\n",
      "Average minibatch loss at step 4100: 2.409\n",
      "Average minibatch loss at step 4102: 2.409\n",
      "Average minibatch loss at step 4104: 2.409\n",
      "Validation loss: 5.131\n",
      "Average minibatch loss at step 4106: 2.408\n",
      "Average minibatch loss at step 4108: 2.408\n",
      "Average minibatch loss at step 4110: 2.408\n",
      "Average minibatch loss at step 4112: 2.408\n",
      "Validation loss: 5.131\n",
      "Average minibatch loss at step 4114: 2.408\n",
      "Average minibatch loss at step 4116: 2.408\n",
      "Average minibatch loss at step 4118: 2.408\n",
      "Average minibatch loss at step 4120: 2.408\n",
      "Validation loss: 5.131\n",
      "Average minibatch loss at step 4122: 2.408\n",
      "Average minibatch loss at step 4124: 2.408\n",
      "Average minibatch loss at step 4126: 2.408\n",
      "Average minibatch loss at step 4128: 2.408\n",
      "Validation loss: 5.132\n",
      "Average minibatch loss at step 4130: 2.408\n",
      "Average minibatch loss at step 4132: 2.408\n",
      "Average minibatch loss at step 4134: 2.408\n",
      "Average minibatch loss at step 4136: 2.408\n",
      "Validation loss: 5.132\n",
      "Average minibatch loss at step 4138: 2.408\n",
      "Average minibatch loss at step 4140: 2.408\n",
      "Average minibatch loss at step 4142: 2.408\n",
      "Average minibatch loss at step 4144: 2.408\n",
      "Validation loss: 5.132\n",
      "Average minibatch loss at step 4146: 2.408\n",
      "Average minibatch loss at step 4148: 2.408\n",
      "Average minibatch loss at step 4150: 2.408\n",
      "Average minibatch loss at step 4152: 2.408\n",
      "Validation loss: 5.133\n",
      "Average minibatch loss at step 4154: 2.408\n",
      "Average minibatch loss at step 4156: 2.408\n",
      "Average minibatch loss at step 4158: 2.408\n",
      "Average minibatch loss at step 4160: 2.408\n",
      "Validation loss: 5.133\n",
      "Average minibatch loss at step 4162: 2.408\n",
      "Average minibatch loss at step 4164: 2.408\n",
      "Average minibatch loss at step 4166: 2.408\n",
      "Average minibatch loss at step 4168: 2.408\n",
      "Validation loss: 5.134\n",
      "Average minibatch loss at step 4170: 2.408\n",
      "Average minibatch loss at step 4172: 2.408\n",
      "Average minibatch loss at step 4174: 2.408\n",
      "Average minibatch loss at step 4176: 2.408\n",
      "Validation loss: 5.134\n",
      "Average minibatch loss at step 4178: 2.408\n",
      "Average minibatch loss at step 4180: 2.408\n",
      "Average minibatch loss at step 4182: 2.408\n",
      "Average minibatch loss at step 4184: 2.408\n",
      "Validation loss: 5.134\n",
      "Average minibatch loss at step 4186: 2.408\n",
      "Average minibatch loss at step 4188: 2.408\n",
      "Average minibatch loss at step 4190: 2.408\n",
      "Average minibatch loss at step 4192: 2.408\n",
      "Validation loss: 5.135\n",
      "Average minibatch loss at step 4194: 2.408\n",
      "Average minibatch loss at step 4196: 2.408\n",
      "Average minibatch loss at step 4198: 2.408\n",
      "Average minibatch loss at step 4200: 2.408\n",
      "Validation loss: 5.135\n",
      "Average minibatch loss at step 4202: 2.408\n",
      "Average minibatch loss at step 4204: 2.408\n",
      "Average minibatch loss at step 4206: 2.408\n",
      "Average minibatch loss at step 4208: 2.408\n",
      "Validation loss: 5.136\n",
      "Average minibatch loss at step 4210: 2.408\n",
      "Average minibatch loss at step 4212: 2.408\n",
      "Average minibatch loss at step 4214: 2.408\n",
      "Average minibatch loss at step 4216: 2.408\n",
      "Validation loss: 5.136\n",
      "Average minibatch loss at step 4218: 2.408\n",
      "Average minibatch loss at step 4220: 2.408\n",
      "Average minibatch loss at step 4222: 2.408\n",
      "Average minibatch loss at step 4224: 2.408\n",
      "Validation loss: 5.136\n",
      "Average minibatch loss at step 4226: 2.408\n",
      "Average minibatch loss at step 4228: 2.408\n",
      "Average minibatch loss at step 4230: 2.408\n",
      "Average minibatch loss at step 4232: 2.408\n",
      "Validation loss: 5.137\n",
      "Average minibatch loss at step 4234: 2.408\n",
      "Average minibatch loss at step 4236: 2.408\n",
      "Average minibatch loss at step 4238: 2.408\n",
      "Average minibatch loss at step 4240: 2.408\n",
      "Validation loss: 5.137\n",
      "Average minibatch loss at step 4242: 2.408\n",
      "Average minibatch loss at step 4244: 2.408\n",
      "Average minibatch loss at step 4246: 2.408\n",
      "Average minibatch loss at step 4248: 2.408\n",
      "Validation loss: 5.137\n",
      "Average minibatch loss at step 4250: 2.408\n",
      "Average minibatch loss at step 4252: 2.408\n",
      "Average minibatch loss at step 4254: 2.408\n",
      "Average minibatch loss at step 4256: 2.408\n",
      "Validation loss: 5.138\n",
      "Average minibatch loss at step 4258: 2.408\n",
      "Average minibatch loss at step 4260: 2.408\n",
      "Average minibatch loss at step 4262: 2.408\n",
      "Average minibatch loss at step 4264: 2.408\n",
      "Validation loss: 5.138\n",
      "Average minibatch loss at step 4266: 2.408\n",
      "Average minibatch loss at step 4268: 2.408\n",
      "Average minibatch loss at step 4270: 2.408\n",
      "Average minibatch loss at step 4272: 2.408\n",
      "Validation loss: 5.139\n",
      "Average minibatch loss at step 4274: 2.408\n",
      "Average minibatch loss at step 4276: 2.408\n",
      "Average minibatch loss at step 4278: 2.408\n",
      "Average minibatch loss at step 4280: 2.408\n",
      "Validation loss: 5.139\n",
      "Average minibatch loss at step 4282: 2.408\n",
      "Average minibatch loss at step 4284: 2.408\n",
      "Average minibatch loss at step 4286: 2.408\n",
      "Average minibatch loss at step 4288: 2.408\n",
      "Validation loss: 5.139\n",
      "Average minibatch loss at step 4290: 2.408\n",
      "Average minibatch loss at step 4292: 2.408\n",
      "Average minibatch loss at step 4294: 2.408\n",
      "Average minibatch loss at step 4296: 2.408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3f11ff471d25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average minibatch loss at step %d: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_scalars_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./all_scalars.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorboardX/writer.py\u001b[0m in \u001b[0;36mexport_scalars_to_json\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0mwriter_id\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_setlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnl_langinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCODESET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "epochs = 7000\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "val_df = pd.read_csv('datasets/val.csv')\n",
    "iteration = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch % 4000 == 0:\n",
    "        learning_rate = learning_rate / 10\n",
    "        # Filter parameters that do not require gradients\n",
    "        encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "        decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "        # Optimizers\n",
    "        encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "        print('')\n",
    "        print('learning rate: %f' % learning_rate)\n",
    "        print('')\n",
    "        \n",
    "    generator = BatchGenerator(batch_size, train_df[:64]) \n",
    "\n",
    "    while True:\n",
    "        try: \n",
    "            batch = generator.get_batch()\n",
    "        except StopIteration: break\n",
    "        loss = train_model(batch)\n",
    "        \n",
    "        if iteration % 2 == 0:\n",
    "            print('Average minibatch loss at step %d: %.3f' % (iteration, loss))\n",
    "            writer.add_scalar('train_loss', loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "        \n",
    "        if iteration % 8 == 0:    \n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            val_loss = validation_loss(val_df[:8]) # truncating validation dataframe\n",
    "            print('Validation loss: %.3f' % val_loss)\n",
    "            \n",
    "            writer.add_scalar('valid_loss', val_loss, iteration)\n",
    "            writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(encoder.state_dict(), 'encoder')\n",
    "torch.save(decoder.state_dict(), 'decoder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
